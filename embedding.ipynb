{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321e5898",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05405640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86688eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65d340c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paperId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162e2e9ac70702c146c0aa8432e4a6806bb8c42e</th>\n",
       "      <td>Coupling Large Language Models with Logic Prog...</td>\n",
       "      <td>Zhun Yang</td>\n",
       "      <td>Annual Meeting of the Association for Computat...</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>https://arxiv.org/abs/2307.07696</td>\n",
       "      <td>While large language models (LLMs), such as GP...</td>\n",
       "      <td>Title: Coupling Large Language Models with Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38fe8f324d2162e63a967a9ac6648974fc4c66f3</th>\n",
       "      <td>PaLM-E: An Embodied Multimodal Language Model</td>\n",
       "      <td>Danny Driess</td>\n",
       "      <td>International Conference on Machine Learning</td>\n",
       "      <td>2023</td>\n",
       "      <td>274</td>\n",
       "      <td>https://arxiv.org/abs/2303.03378</td>\n",
       "      <td>Large language models excel at a wide range of...</td>\n",
       "      <td>Title: PaLM-E: An Embodied Multimodal Language...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      title  \\\n",
       "paperId                                                                                       \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  Coupling Large Language Models with Logic Prog...   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3      PaLM-E: An Embodied Multimodal Language Model   \n",
       "\n",
       "                                                author  \\\n",
       "paperId                                                  \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e     Zhun Yang   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3  Danny Driess   \n",
       "\n",
       "                                                                                      venue  \\\n",
       "paperId                                                                                       \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  Annual Meeting of the Association for Computat...   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3       International Conference on Machine Learning   \n",
       "\n",
       "                                          year  citationCount  \\\n",
       "paperId                                                         \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  2023              1   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3  2023            274   \n",
       "\n",
       "                                                                       url  \\\n",
       "paperId                                                                      \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  https://arxiv.org/abs/2307.07696   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3  https://arxiv.org/abs/2303.03378   \n",
       "\n",
       "                                                                                   abstract  \\\n",
       "paperId                                                                                       \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  While large language models (LLMs), such as GP...   \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3  Large language models excel at a wide range of...   \n",
       "\n",
       "                                                                                   combined  \n",
       "paperId                                                                                      \n",
       "162e2e9ac70702c146c0aa8432e4a6806bb8c42e  Title: Coupling Large Language Models with Log...  \n",
       "38fe8f324d2162e63a967a9ac6648974fc4c66f3  Title: PaLM-E: An Embodied Multimodal Language...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load & inspect dataset\n",
    "input_data = \"papers.csv\"\n",
    "df = pd.read_csv(input_data, index_col=0)\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.title.str.strip() + \"; Abstract: \" + df.abstract.str.strip()\n",
    ")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cfbcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# This may take a few minutes\n",
    "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model))\n",
    "df.to_csv(\"papers_embedded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd46fb",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5889d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['o', 's', '^', 'v', '<', '>', 'p', '*', 'D', 'H', '+', 'x', '|', '_', '1', '2']\n",
    "colors = [\n",
    "    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', \n",
    "    '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#40004b', '#762a83', \n",
    "    '#9970ab', '#c2a5cf', '#e7d4e8', '#f7f7f7'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa29a5",
   "metadata": {},
   "source": [
    "## OpenAI Embedding + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ce4877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c518a98dd3e341828358b727053fe92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='n_clusters', max=16, min=2), Output()), _dom_classes=('w…"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# load data\n",
    "datafile_path = \"papers_embedded.csv\"\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)  # convert string to numpy array\n",
    "matrix = np.vstack(df.embedding.values)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "# 2D 데이터 포인트\n",
    "x = [x for x, y in vis_dims2]\n",
    "y = [y for x, y in vis_dims2]\n",
    "\n",
    "# 중앙점 계산\n",
    "avg_x = np.mean(x)\n",
    "avg_y = np.mean(y)\n",
    "\n",
    "# 각 데이터 포인트와 중앙점 사이의 거리 계산\n",
    "distances = np.sqrt((x - avg_x)**2 + (y - avg_y)**2)\n",
    "\n",
    "# 임계값 설정 (예: 평균 거리의 2배)\n",
    "threshold = 2 * np.mean(distances)\n",
    "\n",
    "# 아웃라이어 인덱스 찾기\n",
    "outliers = np.where(distances > threshold)[0]\n",
    "\n",
    "# 아웃라이어를 제외한 데이터프레임 설정\n",
    "filtered_df = df.drop(outliers)\n",
    "filtered_matrix = np.vstack(filtered_df.embedding.values)\n",
    "\n",
    "vis_dims2 = tsne.fit_transform(filtered_matrix)\n",
    "filtered_x = [x for x, y in vis_dims2]\n",
    "filtered_y = [y for x, y in vis_dims2]\n",
    "\n",
    "def visualize_clusters(n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    filtered_df['Cluster'] = kmeans.fit_predict(filtered_matrix)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(filtered_matrix, filtered_df['Cluster'])\n",
    "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    cluster_counts = filtered_df['Cluster'].value_counts()\n",
    "    print(\"\\nNumber of data points in each cluster:\")\n",
    "    print(cluster_counts)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for category, (marker, color) in enumerate(zip(markers, colors)):\n",
    "        xs = np.array(filtered_x)[filtered_df.Cluster == category]\n",
    "        ys = np.array(filtered_y)[filtered_df.Cluster == category]\n",
    "        \n",
    "        if len(xs) > 0:\n",
    "            plt.scatter(xs, ys, color=color, alpha=0.3, marker=marker, label=f'Cluster {category}')\n",
    "            if len(xs) > 2:\n",
    "                hull = ConvexHull(np.column_stack((xs, ys)))\n",
    "                plt.fill(xs[hull.vertices], ys[hull.vertices], color=color, alpha=0.1)\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    plt.title(f\"Clusters identified with {n_clusters} clusters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "widgets.interactive(visualize_clusters, n_clusters=widgets.IntSlider(min=2, max=16, step=1, value=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666457aa",
   "metadata": {},
   "source": [
    "## Semantic Scholar Embedding + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f624bea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 768)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# load data\n",
    "datafile_path = \"papers.csv\"\n",
    "\n",
    "df = pd.read_csv(datafile_path)\n",
    "df[\"embedding\"] = df.embedding.apply(literal_eval).apply(np.array)  # convert string to numpy array\n",
    "matrix = np.vstack(df.embedding.values)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2cc03bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paperId', 'title', 'author', 'venue', 'year', 'citationCount', 'url',\n",
       "       'abstract', 'embedding', 'Cluster', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f45efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb18456931b94118aa7ebb312abc9f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='n_clusters', max=16, min=2), Output()), _dom_classes=('w…"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def visualize_clusters(n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    df['Cluster'] = kmeans.fit_predict(matrix)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "    # 2D 데이터 포인트\n",
    "    x = [x for x, y in vis_dims2]\n",
    "    y = [y for x, y in vis_dims2]    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for category, (marker, color) in enumerate(zip(markers, colors)):        \n",
    "        xs = np.array(x)[df.Cluster == category]\n",
    "        ys = np.array(y)[df.Cluster == category]\n",
    "        \n",
    "        # 데이터 포인트가 있는 경우에만 시각화 및 평균 계산\n",
    "        if len(xs) > 0:\n",
    "            plt.scatter(xs, ys, color=color, alpha=0.3, marker=marker, label=f'Cluster {category}')\n",
    "            if len(xs) > 2:\n",
    "                hull = ConvexHull(np.column_stack((xs, ys)))\n",
    "                plt.fill(xs[hull.vertices], ys[hull.vertices], color=color, alpha=0.1)\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    plt.title(f\"Clusters identified with {n_clusters} clusters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 각 클러스터의 데이터 개수 출력\n",
    "    print(\"Number of data points in each cluster:\")\n",
    "    print(df['Cluster'].value_counts())\n",
    "    \n",
    "    # 실루엣 점수 출력\n",
    "    silhouette_avg = silhouette_score(matrix, df['Cluster'])\n",
    "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg:.4f}\")\n",
    "\n",
    "widgets.interactive(visualize_clusters, n_clusters=widgets.IntSlider(min=2, max=16, step=1, value=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "189db40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Theme:  Human-Robot Interaction\n",
      "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning\n",
      "\n",
      "Correcting Robot Plans with Natural Language Feedback\n",
      "\n",
      "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text\n",
      "\n",
      "Language Instructed Reinforcement Learning for Human-AI Coordination\n",
      "\n",
      "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning\n",
      "\n",
      "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers\n",
      "\n",
      "Generalized Planning in PDDL Domains with Pretrained Large Language Models\n",
      "\n",
      "Chat with the Environment: Interactive Multimodal Perception using Large Language Models\n",
      "\n",
      "RMM: A Recursive Mental Model for Dialog Navigation\n",
      "\n",
      "Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cluster 1 Theme:  Learning robotic manipulation policies from visual observations\n",
      "A Framework for Efficient Robotic Manipulation\n",
      "\n",
      "A System for General In-Hand Object Re-Orientation\n",
      "\n",
      "Grounding Language with Visual Affordances over Unstructured Data\n",
      "\n",
      "Human-to-Robot Imitation in the Wild\n",
      "\n",
      "Chain-of-Thought Predictive Control\n",
      "\n",
      "Auto-Lambda: Disentangling Dynamic Task Relationships\n",
      "\n",
      "Transformer Adapters for Robot Learning\n",
      "\n",
      "RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation\n",
      "\n",
      "Hybrid Random Features\n",
      "\n",
      "Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cluster 2 Theme:  Robotics Task and Motion Planning (TAMP)\n",
      "Learning Neuro-Symbolic Skills for Bilevel Planning\n",
      "\n",
      "Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo\n",
      "\n",
      "Discovering State and Action Abstractions for Generalized Task and Motion Planning\n",
      "\n",
      "STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation\n",
      "\n",
      "Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization\n",
      "\n",
      "Learning Symbolic Operators for Task and Motion Planning\n",
      "\n",
      "Beyond Pick-and-Place: Tackling Robotic Stacking of Diverse Shapes\n",
      "\n",
      "Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation\n",
      "\n",
      "Integrated Task and Motion Planning\n",
      "\n",
      "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cluster 3 Theme:  Vision-Language Robotics\n",
      "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform\n",
      "\n",
      "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation\n",
      "\n",
      "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\n",
      "\n",
      "OCID-Ref: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding\n",
      "\n",
      "INGRESS: Interactive visual grounding of referring expressions\n",
      "\n",
      "VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator\n",
      "\n",
      "Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement\n",
      "\n",
      "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action\n",
      "\n",
      "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\n",
      "\n",
      "Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(matrix)\n",
    "\n",
    "# Reading a review which belong to each group.\n",
    "abstracts_per_cluster = 10\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i} Theme:\", end=\" \")\n",
    "    \n",
    "    # Filter out NaN abstracts and then sample\n",
    "    valid_abstracts = df[(df.Cluster == i) & (df.abstract.notna())]\n",
    "    sampled_abstracts = valid_abstracts.abstract.sample(min(abstracts_per_cluster, len(valid_abstracts)), random_state=42).values\n",
    "    \n",
    "    abstracts = \"\\n\".join(sampled_abstracts)\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f'What\\'s a common topic regarding robotics in these researches?\\n\\nAbstracts:\\n\"\"\"\\n{abstracts}\\n\"\"\"\\n\\nCategory:',\n",
    "        temperature=0,\n",
    "        max_tokens=64,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n",
    "\n",
    "    sample_cluster_rows = df[df.Cluster == i].sample(abstracts_per_cluster, random_state=42)\n",
    "    for j in range(abstracts_per_cluster):\n",
    "        print(sample_cluster_rows.title.values[j], end='\\n\\n')\n",
    "\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3304e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of cluster numbers to themes\n",
    "cluster_to_theme = {\n",
    "    0: \"Human-Robot Interaction\",\n",
    "    1: \"Robot Manipulation\",\n",
    "    2: \"Robot Task and Motion Planning\",\n",
    "    3: \"Vision-Language Robotics\"\n",
    "}\n",
    "\n",
    "# Add the 'category' column to the DataFrame\n",
    "df['category'] = df['Cluster'].map(cluster_to_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef11e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dd36fb5d4242c9ab443625c2c1f8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='category', options=('Human-Robot Interaction', 'Vision-Language Ro…"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def visualize_clusters_by_category(category, n_clusters):\n",
    "    # Filter the data for the selected category\n",
    "    category_data = df[df['category'] == category].copy()\n",
    "    category_matrix = np.vstack(category_data.embedding.values)\n",
    "    \n",
    "    # Compute the 2D representation using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_dims2 = tsne.fit_transform(category_matrix)\n",
    "    \n",
    "    # Calculate distances to centroid for outlier removal\n",
    "    avg_x = np.mean([x for x, y in vis_dims2])\n",
    "    avg_y = np.mean([y for x, y in vis_dims2])\n",
    "    distances = np.sqrt((np.array([x for x, y in vis_dims2]) - avg_x)**2 + (np.array([y for x, y in vis_dims2]) - avg_y)**2)\n",
    "    threshold = 2 * np.mean(distances)\n",
    "    outliers = np.where(distances > threshold)[0]\n",
    "    \n",
    "    # Adjust the indices of outliers to match the indices in category_data\n",
    "    outlier_indices = category_data.iloc[outliers].index\n",
    "    \n",
    "    # Remove outliers\n",
    "    category_data.drop(outlier_indices, inplace=True)\n",
    "    category_matrix = np.vstack(category_data.embedding.values)\n",
    "    vis_dims2 = np.delete(vis_dims2, outliers, axis=0)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    category_data['Cluster'] = kmeans.fit_predict(category_matrix)\n",
    "    \n",
    "    # 2D data points\n",
    "    x = [x for x, y in vis_dims2]\n",
    "    y = [y for x, y in vis_dims2]    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster, (marker, color) in enumerate(zip(markers, colors)):        \n",
    "        xs = np.array(x)[category_data.Cluster == cluster]\n",
    "        ys = np.array(y)[category_data.Cluster == cluster]\n",
    "        \n",
    "        # Only visualize and compute average for data points that exist\n",
    "        if len(xs) > 0:\n",
    "            plt.scatter(xs, ys, color=color, alpha=0.3, marker=marker, label=f'Cluster {cluster}')\n",
    "            if len(xs) > 2:\n",
    "                hull = ConvexHull(np.column_stack((xs, ys)))\n",
    "                plt.fill(xs[hull.vertices], ys[hull.vertices], color=color, alpha=0.1)\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    plt.title(f\"Clusters for {category} with {n_clusters} clusters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the number of data points in each cluster\n",
    "    print(\"Number of data points in each cluster:\")\n",
    "    print(category_data['Cluster'].value_counts())\n",
    "    \n",
    "    # Print the silhouette score\n",
    "    silhouette_avg = silhouette_score(category_matrix, category_data['Cluster'])\n",
    "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Use ipywidgets to create an interactive visualization\n",
    "categories = df['category'].unique()\n",
    "widgets.interactive(visualize_clusters_by_category, category=widgets.Dropdown(options=categories), n_clusters=widgets.IntSlider(min=2, max=16, step=1, value=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6e9a78ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Human-Robot Interaction\n",
      "Subcluster 0 Theme:  Foundation Models for Decision Making\n",
      "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning\n",
      "\n",
      "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling\n",
      "\n",
      "Towards A Unified Agent with Foundation Models\n",
      "\n",
      "Keep CALM and Explore: Language Models for Action Generation in Text-based Games\n",
      "\n",
      "Foundation Models for Decision Making: Problems, Methods, and Opportunities\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 1 Theme:  Human-Robot Interaction with Large Language Models (LLMs)\n",
      "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\n",
      "\n",
      "FILM: Following Instructions in Language with Modular Methods\n",
      "\n",
      "LLM as A Robotic Brain: Unifying Egocentric Memory and Control\n",
      "\n",
      "Planning with Large Language Models via Corrective Re-prompting\n",
      "\n",
      "Generative Agents: Interactive Simulacra of Human Behavior\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 2 Theme:  Natural Language Interfaces for Human-Robot Interaction\n",
      "Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification\n",
      "\n",
      "Code as Policies: Language Model Programs for Embodied Control\n",
      "\n",
      "LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model\n",
      "\n",
      "LILA: Language-Informed Latent Actions\n",
      "\n",
      "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Category: Robot Manipulation\n",
      "Subcluster 0 Theme:  Robot Manipulation Generalization\n",
      "RT-1: Robotics Transformer for Real-World Control at Scale\n",
      "\n",
      "RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation\n",
      "\n",
      "MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale\n",
      "\n",
      "MetaMorph: Learning Universal Controllers with Transformers\n",
      "\n",
      "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 1 Theme:  Data-Efficient Robot Manipulation\n",
      "Chain-of-Thought Predictive Control\n",
      "\n",
      "Affordance Learning from Play for Sample-Efficient Policy Learning\n",
      "\n",
      "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion\n",
      "\n",
      "A Framework for Efficient Robotic Manipulation\n",
      "\n",
      "S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 2 Theme:  Language-Conditioned Robot Manipulation\n",
      "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data\n",
      "\n",
      "Grounding Language with Visual Affordances over Unstructured Data\n",
      "\n",
      "Language-Conditioned Imitation Learning for Robot Manipulation Tasks\n",
      "\n",
      "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks\n",
      "\n",
      "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 3 Theme:  Multi-Task Learning for Robot Manipulation.\n",
      "Hybrid Random Features\n",
      "\n",
      "Auto-Lambda: Disentangling Dynamic Task Relationships\n",
      "\n",
      "Gradient Surgery for Multi-Task Learning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 4 Theme:  Robot Manipulation for Assistive Tasks\n",
      "A System for General In-Hand Object Re-Orientation\n",
      "\n",
      "Behavior Transformers: Cloning k modes with one stone\n",
      "\n",
      "Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning\n",
      "\n",
      "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\n",
      "\n",
      "Shared Autonomy with Learned Latent Actions\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 5 Theme:  Real-Robot Manipulation\n",
      "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training\n",
      "\n",
      "Q-attention: Enabling Efficient Learning for Vision-based Robotic Manipulation\n",
      "\n",
      "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?\n",
      "\n",
      "Masked Visual Pre-training for Motor Control\n",
      "\n",
      "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Category: Robot Task and Motion Planning\n",
      "Subcluster 0 Theme:  Simulation Environments for Robot Task and Motion Planning\n",
      "SAPIEN: A SimulAted Part-Based Interactive ENvironment\n",
      "\n",
      "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes\n",
      "\n",
      "Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks\n",
      "\n",
      "Habitat 2.0: Training Home Assistants to Rearrange their Habitat\n",
      "\n",
      "Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 1 Theme:  Bilevel Planning with Abstractions\n",
      "Integrated Task and Motion Planning\n",
      "\n",
      "Text2Motion: From Natural Language Instructions to Feasible Plans\n",
      "\n",
      "Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning\n",
      "\n",
      "Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization\n",
      "\n",
      "Inventing Relational State and Action Abstractions for Effective and Efficient Bilevel Planning\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 2 Theme:  Robot Motion Planning\n",
      "Chain of Thought Imitation with Procedure Cloning\n",
      "\n",
      "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning\n",
      "\n",
      "Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds\n",
      "\n",
      "Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo\n",
      "\n",
      "STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 3 Theme:  Dexterous Robotic Manipulation\n",
      "A Global Quasi-Dynamic Model for Contact-Trajectory Optimization in Manipulation\n",
      "\n",
      "A Purely-Reactive Manipulability-Maximising Motion Controller.\n",
      "\n",
      "Tactile Dexterity: Manipulation Primitives with Tactile Feedback\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Category: Vision-Language Robotics\n",
      "Subcluster 0 Theme:  Vision-Language Robotics for Object Manipulation and Navigation\n",
      "Open-World Object Manipulation using Pre-trained Vision-Language Models\n",
      "\n",
      "Physically Grounded Vision-Language Models for Robotic Manipulation\n",
      "\n",
      "Open-vocabulary Queryable Scene Representations for Real World Planning\n",
      "\n",
      "Grounding Language to Landmarks in Arbitrary Outdoor Environments\n",
      "\n",
      "LATTE: LAnguage Trajectory TransformEr\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subcluster 1 Theme:  Embodied Language Models for Vision-Language Robotics\n",
      "PaLM-E: An Embodied Multimodal Language Model\n",
      "\n",
      "Object Goal Navigation using Goal-Oriented Semantic Exploration\n",
      "\n",
      "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation\n",
      "\n",
      "Audio Visual Language Maps for Robot Navigation\n",
      "\n",
      "CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define the number of clusters for each category\n",
    "category_cluster_map = {\n",
    "    \"Human-Robot Interaction\": 3,\n",
    "    \"Robot Manipulation\": 6,\n",
    "    \"Robot Task and Motion Planning\": 4,\n",
    "    \"Vision-Language Robotics\": 2\n",
    "}\n",
    "\n",
    "# Function to determine subcategories within each main category\n",
    "def determine_subcategories(category, n_clusters):\n",
    "    # Filter the data for the selected category\n",
    "    category_data = df[df['category'] == category].copy()\n",
    "    category_matrix = np.vstack(category_data.embedding.values)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    category_data['Subcluster'] = kmeans.fit_predict(category_matrix)\n",
    "    \n",
    "    # Determine the theme for each subcluster\n",
    "    abstracts_per_cluster = 5\n",
    "    for i in range(n_clusters):\n",
    "        print(f\"Subcluster {i} Theme:\", end=\" \")\n",
    "        \n",
    "        # Filter out NaN abstracts and then sample\n",
    "        valid_abstracts = category_data[(category_data.Subcluster == i) & (category_data.abstract.notna())]\n",
    "        sampled_abstracts = valid_abstracts.abstract.sample(min(abstracts_per_cluster, len(valid_abstracts)), random_state=42).values\n",
    "        \n",
    "        abstracts = \"\\n\".join(sampled_abstracts)\n",
    "        \n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=f'What\\'s a single common subtopic of {category} that encompasses these researches?\\n\\nAbstracts:\\n\"\"\"\\n{abstracts}\\n\"\"\"\\n\\nSubTopic:',\n",
    "            temperature=0,\n",
    "            max_tokens=64,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "        print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n",
    "\n",
    "        sample_subcluster_rows = category_data[category_data.Subcluster == i].sample(min(abstracts_per_cluster, len(valid_abstracts)), random_state=42)\n",
    "        for j in range(min(abstracts_per_cluster, len(valid_abstracts))):\n",
    "            print(sample_subcluster_rows.title.values[j], end='\\n\\n')\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "    # Update the original df with the Subcluster values for the current category\n",
    "    df.loc[category_data.index, 'Subcluster'] = category_data['Subcluster']\n",
    "\n",
    "# Iterate over each main category and determine its subcategories\n",
    "for category, n_clusters in category_cluster_map.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    determine_subcategories(category, n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6cad933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping of clusters to subcategories\n",
    "subcategory_mapping = {\n",
    "    'Human-Robot Interaction': {\n",
    "        0: 'Foundation Models for Decision Making',\n",
    "        1: 'LLM-based Interaction',\n",
    "        2: 'Natural Language Interfaces'\n",
    "    },\n",
    "    'Robot Manipulation': {\n",
    "        0: 'Generalization',\n",
    "        1: 'Data Augmentation',\n",
    "        2: 'Language Conditioned Robot Manipulation',\n",
    "        3: 'Multi-Task Learning',\n",
    "        4: 'Model-Free Learning',\n",
    "        5: 'Real-Robot Manipulation'\n",
    "    },\n",
    "    'Robot Task and Motion Planning': {\n",
    "        0: 'Simulation Environments',\n",
    "        1: 'Bilevel Planning with Abstractions',\n",
    "        2: 'Robot Motion Planning',\n",
    "        3: 'Dexterous Robotic Manipulation'\n",
    "    },\n",
    "    'Vision-Language Robotics': {\n",
    "        0: 'Object Manipulation and Navigation',\n",
    "        1: 'Embodied Language Models'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Assign subcategories to the dataframe based on the category and cluster\n",
    "df['subcategory'] = df.apply(lambda row: subcategory_mapping[row['category']][row['Subcluster']], axis=1)\n",
    "\n",
    "# Reorder columns to have Category and SubCategory at the beginning\n",
    "df = df[['category', 'subcategory', 'paperId', 'title', 'author', 'venue', 'year', 'citationCount', 'url',\n",
    "       'abstract', 'embedding']]\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('paper_categorized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780e979",
   "metadata": {},
   "source": [
    "## Semantic Scholar Embedding + 계층적 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a0737d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ef37b0cc042ccb831e72a20326e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='n_clusters', max=16, min=2), Output()), _dom_classes=('w…"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def visualize_hierarchical_clusters(n_clusters):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    df['Cluster'] = agglomerative.fit_predict(matrix)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "    # 2D 데이터 포인트\n",
    "    x = [x for x, y in vis_dims2]\n",
    "    y = [y for x, y in vis_dims2]    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for category, (marker, color) in enumerate(zip(markers, colors)):        \n",
    "        xs = np.array(x)[df.Cluster == category]\n",
    "        ys = np.array(y)[df.Cluster == category]\n",
    "        \n",
    "        # 데이터 포인트가 있는 경우에만 시각화 및 평균 계산\n",
    "        if len(xs) > 0:\n",
    "            plt.scatter(xs, ys, color=color, alpha=0.3, marker=marker, label=f'Cluster {category}')\n",
    "            if len(xs) > 2:\n",
    "                hull = ConvexHull(np.column_stack((xs, ys)))\n",
    "                plt.fill(xs[hull.vertices], ys[hull.vertices], color=color, alpha=0.1)\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    plt.title(f\"Clusters identified with {n_clusters} clusters (Hierarchical)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 각 클러스터의 데이터 개수 출력\n",
    "    print(\"Number of data points in each cluster:\")\n",
    "    print(df['Cluster'].value_counts())\n",
    "    \n",
    "    # 실루엣 점수 출력\n",
    "    silhouette_avg = silhouette_score(matrix, df['Cluster'])\n",
    "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg:.4f}\")\n",
    "\n",
    "widgets.interactive(visualize_hierarchical_clusters, n_clusters=widgets.IntSlider(min=2, max=16, step=1, value=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee233170",
   "metadata": {},
   "source": [
    "## Semantic Scholar API + DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6e32a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ba8c8ebe8046128ad39a6d1669f1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='eps', max=20.0, min=0.1), IntSlider(value=5, descrip…"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def visualize_dbscan(eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df['Cluster'] = dbscan.fit_predict(matrix)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200)\n",
    "    vis_dims2 = tsne.fit_transform(matrix)\n",
    "\n",
    "    # 2D 데이터 포인트\n",
    "    x = [x for x, y in vis_dims2]\n",
    "    y = [y for x, y in vis_dims2]    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    n_clusters = len(set(df['Cluster'])) - (1 if -1 in df['Cluster'] else 0)  # -1은 노이즈를 나타냅니다.\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "    for category, color in zip(set(df['Cluster']), colors):        \n",
    "        xs = np.array(x)[df.Cluster == category]\n",
    "        ys = np.array(y)[df.Cluster == category]\n",
    "        \n",
    "        # 데이터 포인트가 있는 경우에만 시각화\n",
    "        if len(xs) > 0:\n",
    "            plt.scatter(xs, ys, color=color, alpha=0.3, label=f'Cluster {category}' if category != -1 else 'Noise')\n",
    "            avg_x = xs.mean()\n",
    "            avg_y = ys.mean()\n",
    "            plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    if n_clusters > 0:\n",
    "        plt.legend()\n",
    "    plt.title(f\"Clusters identified with DBSCAN (eps={eps}, min_samples={min_samples})\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 각 클러스터의 데이터 개수 출력\n",
    "    print(\"Number of data points in each cluster:\")\n",
    "    print(df['Cluster'].value_counts())\n",
    "\n",
    "widgets.interactive(visualize_dbscan, eps=widgets.FloatSlider(min=0.1, max=20, step=0.1, value=0.5), min_samples=widgets.IntSlider(min=2, max=20, step=1, value=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202537da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
