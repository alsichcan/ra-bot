{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f350d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b714ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7,ar;q=0.6\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "    \"X-S2-Client\": \"webapp-browser\",\n",
    "    \"X-S2-Ui-Version\": \"452cc270c0d8e73927dd44144435f75b41e2c43a\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1b9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_paper(title):\n",
    "    # Define the API endpoint\n",
    "    url = \"https://www.semanticscholar.org/api/1/search\"\n",
    "\n",
    "    title = title.replace('\\xa0', ' ')\n",
    "    \n",
    "    # Define the data to be sent in the POST request\n",
    "    data = {\n",
    "        \"queryString\": title,\n",
    "        \"page\": 1,\n",
    "        \"authors\": [],\n",
    "        \"coAuthors\": [],\n",
    "        \"cues\": [\"CitedByLibraryPaperCue\"],\n",
    "        \"fieldsOfStudy\": [],\n",
    "        \"getQuerySuggestions\": False,\n",
    "        \"hydrateWithDdb\": True,\n",
    "        \"includeBadges\": True,\n",
    "        \"includePdfVisibility\": False,\n",
    "        \"includeTldrs\": True,\n",
    "        \"pageSize\": 10,\n",
    "        \"performTitleMatch\": True,\n",
    "        \"requireViewablePdf\": False,\n",
    "        \"sort\": \"relevance\",\n",
    "        \"useFallbackRankerService\": False,\n",
    "        \"useFallbackSearchCluster\": False,\n",
    "        \"venues\": [],\n",
    "        \"yearFilter\": None\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "    response_data = response.json()\n",
    "    \n",
    "   # Filter results by title match and find the result with the highest numReferences\n",
    "    max_references = -1\n",
    "    paper_id_with_max_references = None\n",
    "\n",
    "    for result in response_data['results']:        \n",
    "        if title.lower() in result['title']['text'].lower():\n",
    "            current_references = result['citationStats']['numReferences'] if 'citationStats' in result else 0\n",
    "            if current_references > max_references:\n",
    "                max_references = current_references\n",
    "                paper_id_with_max_references = result['id']\n",
    "\n",
    "    if paper_id_with_max_references is None:\n",
    "        raise Exception(\"No matching papers found with the specified title.\")\n",
    "\n",
    "    return paper_id_with_max_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a2cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail(seed_id):\n",
    "    # Define the API endpoint\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{seed_id}\"\n",
    "\n",
    "    # Specify the fields we want in the response\n",
    "    params = {\n",
    "        \"fields\": \"title,venue,year,authors,abstract,citationCount,externalIds,url,embedding.specter_v2\"\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "   \n",
    "    data = response.json()\n",
    "\n",
    "    # Extract required information\n",
    "    result = {\n",
    "        \"paperId\": data[\"paperId\"],\n",
    "        \"title\": data[\"title\"],\n",
    "        \"author\": data[\"authors\"][0][\"name\"] if data[\"authors\"] else None,\n",
    "        \"venue\": data[\"venue\"],\n",
    "        \"year\": data[\"year\"],\n",
    "        \"citationCount\": data[\"citationCount\"],\n",
    "        \"url\": f'https://arxiv.org/abs/{data[\"externalIds\"][\"ArXiv\"]}' if \"ArXiv\" in data[\"externalIds\"] else data[\"url\"],\n",
    "        \"abstract\": data[\"abstract\"],\n",
    "        \"embedding\": data['embedding']['vector']\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0e3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def write_node_data(data, file_name, fields):\n",
    "    with open(file_name, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "        with open(file_name, 'r', newline='', encoding='utf-8') as readfile:\n",
    "            reader = csv.DictReader(readfile)\n",
    "            if any(row['paperId'] == data['paperId'] for row in reader):\n",
    "                return\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca23411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(seed_id):\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{seed_id}/references\"\n",
    "    params = {\n",
    "        \"fields\": \"contexts,intents,isInfluential,paperId,year,abstract\",\n",
    "        \"limit\": \"1000\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    data = response.json()['data']\n",
    "    \n",
    "    # year가 2020 이후이고, abstract에 'robot'이 포함되며, isInfluential이 True인 것만 필터링\n",
    "    filtered_data = [entry for entry in data \n",
    "                     if entry['citedPaper']['year'] is not None \n",
    "                     and entry['citedPaper']['year'] >= 2020\n",
    "                     and entry['citedPaper'].get('abstract')\n",
    "                     and 'robot' in entry['citedPaper']['abstract'].lower()\n",
    "                     and entry['isInfluential']]\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11262d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "\n",
    "def write_edge_data(data, file_name, fields):                                              \n",
    "    with open(file_name, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff8e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# CSV 파일에서 'title' 컬럼의 데이터를 읽어와서 seed_papers 초기화\n",
    "def get_seeds(file_name):\n",
    "    titles = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            titles.append(row['Title'])\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3229ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text\n",
      "PaLM-E: An Embodied Multimodal Language Model\n",
      "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models\n",
      "Open-vocabulary Queryable Scene Representations for Real World Planning\n",
      "Visual Language Maps for Robot Navigation\n",
      "Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification\n",
      "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\n",
      "Code as Policies: Language Model Programs for Embodied Control\n",
      "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning\n",
      "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling\n",
      "Grounding Language with Visual Affordances over Unstructured Data\n",
      "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models\n",
      "Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement\n",
      "Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement\n",
      "Towards Open-World Interactive Disambiguation for Robotic Grasping\n",
      "Guiding Pretraining in Reinforcement Learning with Large Language Models\n",
      "VQA-based Robotic State Recognition Optimized with Genetic Algorithm\n",
      "Programmatically Grounded, Compositionally Generalizable Robotic Manipulation\n",
      "Neuro-Symbolic Procedural Planning with Commonsense Prompting\n",
      "Demonstrating Large Language Models on Robots\n",
      "Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization\n",
      "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\n",
      "Mapping Multi-modality Instructions to Robotic Actions with Large Language Model\n",
      "Personalized Robot Assistance with Large Language Models\n",
      "RT-1: Robotics Transformer for Real-World Control at Scale\n",
      "Generating Situated Robot Task Plans using Large Language Models\n",
      "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances\n",
      "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language\n",
      "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World\n",
      "Chat with the Environment: Interactive Multimodal Perception using Large Language Models\n",
      "Generative Agents: Interactive Simulacra of Human Behavior\n",
      "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction\n",
      "Translating Natural Language to Planning Goals with Large-Language Models\n",
      "Generalized Planning in PDDL Domains with Pretrained Large Language Models\n",
      "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?\n",
      "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\n",
      "Foundation Models for Decision Making: Problems, Methods, and Opportunities\n",
      "ChatGPT for Robotics: Design Principles and Model Abilities\n",
      "Text2Motion: From Natural Language Instructions to Feasible Plans\n",
      "ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application\n",
      "Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action\n",
      "Inner Monologue: Embodied Reasoning through Planning with Language Models\n",
      "Housekeep: Tidying Virtual Households using Commonsense Reasoning\n",
      "Pre-Trained Language Models for Interactive Decision-Making\n",
      "FILM: Following Instructions in Language with Modular Methods\n",
      "Don’t Copy the Teacher: Data and Model Challenges in Embodied Dialogue\n",
      "ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model\n",
      "Open-World Object Manipulation using Pre-trained Vision-Language Models\n",
      "Keep CALM and Explore: Language Models for Action Generation in Text-based Games\n",
      "Planning with Large Language Models via Corrective Re-prompting\n",
      "Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions\n",
      "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models\n",
      "Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control\n",
      "Robot Task Planning and Situation Handling in Open Worlds\n",
      "Reward Design with Language Models\n",
      "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning\n",
      "Collaborating with language models for embodied reasoning\n",
      "LLM as A Robotic Brain: Unifying Egocentric Memory and Control\n",
      "Building Cooperative Embodied Agents Modularly with Large Language Models\n",
      "Language to Rewards for Robotic Skill Synthesis\n",
      "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\n",
      "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models\n",
      "Chain-of-Thought Predictive Control\n",
      "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models\n",
      "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory\n",
      "VIMA: General Robot Manipulation with Multimodal Prompts\n",
      "A Multi-Task Transformer for Robotic Manipulation\n",
      "LATTE: LAnguage Trajectory TransformEr\n",
      "Robots Enact Malignant Stereotypes\n",
      "Leveraging Language for Accelerated Learning of Tool Manipulation\n",
      "Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?\n",
      "Semantic Exploration from Language Abstractions and Pretrained Representations\n",
      "Simple but Effective: CLIP Embeddings for Embodied AI\n",
      "CLIPort: What and Where Pathways for Robotic Manipulation\n",
      "Multimodal Procedural Planning via Dual Text-Image Prompting\n",
      "Pretrained Language Models as Visual Planners for Human Assistance\n",
      "R3M: A Universal Visual Representation for Robot Manipulation\n",
      "LIV: Language-Image Representations and Rewards for Robotic Control\n",
      "No, to the Right: Online Language Corrections for Robotic Manipulation via Shared Autonomy\n",
      "Task and Motion Planning with Large Language Models for Object Rearrangement\n",
      "Towards a Unified Agent with Foundation Models\n",
      "Language Instructed Reinforcement Learning for Human-AI Coordination\n",
      "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\n",
      "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks\n",
      "VOYAGER: An Open-Ended Embodied Agent with Large Language Models\n",
      "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition\n",
      "A Generalist Agent\n",
      "RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation\n",
      "Physically Grounded Vision-Language Models for Robotic Manipulation\n",
      "METAMORPH: LEARNING UNIVERSAL CONTROLLERS WITH TRANSFORMERS\n",
      "ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts\n",
      "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control\n",
      "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration\n",
      "A Recurrent Vision-and-Language BERT for Navigation\n",
      "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\n",
      "Interactive Language: Talking to Robots in Real Time\n",
      "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge\n",
      "Habitat 2.0: Training Home Assistants to Rearrange their Habitat\n",
      "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments\n",
      "iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes\n",
      "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks\n",
      "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 101\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 101\n",
      "Remaining seeds: 100\n",
      "Remaining seeds: 100\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 105\n",
      "Remaining seeds: 104\n",
      "Remaining seeds: 104\n",
      "Remaining seeds: 105\n",
      "Remaining seeds: 106\n",
      "Remaining seeds: 105\n",
      "Remaining seeds: 104\n",
      "Remaining seeds: 103\n",
      "Remaining seeds: 102\n",
      "Remaining seeds: 101\n",
      "Remaining seeds: 100\n",
      "Remaining seeds: 101\n",
      "Remaining seeds: 100\n",
      "Remaining seeds: 99\n",
      "Remaining seeds: 98\n",
      "Remaining seeds: 97\n",
      "Remaining seeds: 99\n",
      "Remaining seeds: 99\n",
      "Remaining seeds: 98\n",
      "Remaining seeds: 97\n",
      "Remaining seeds: 96\n",
      "Remaining seeds: 96\n",
      "Remaining seeds: 95\n",
      "Remaining seeds: 94\n",
      "Remaining seeds: 96\n",
      "Remaining seeds: 95\n",
      "Remaining seeds: 97\n",
      "Remaining seeds: 96\n",
      "Remaining seeds: 100\n",
      "Remaining seeds: 99\n",
      "Remaining seeds: 98\n",
      "Remaining seeds: 97\n",
      "Remaining seeds: 96\n",
      "Remaining seeds: 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining seeds: 95\n",
      "Remaining seeds: 95\n",
      "Remaining seeds: 94\n",
      "Remaining seeds: 93\n",
      "Remaining seeds: 93\n",
      "Remaining seeds: 92\n",
      "Remaining seeds: 92\n",
      "Remaining seeds: 91\n",
      "Remaining seeds: 91\n",
      "Remaining seeds: 90\n",
      "Remaining seeds: 89\n",
      "Remaining seeds: 88\n",
      "Remaining seeds: 87\n",
      "Remaining seeds: 86\n",
      "Remaining seeds: 85\n",
      "Remaining seeds: 84\n",
      "Remaining seeds: 85\n",
      "Remaining seeds: 84\n",
      "Remaining seeds: 83\n",
      "Remaining seeds: 85\n",
      "Remaining seeds: 84\n",
      "Remaining seeds: 83\n",
      "Remaining seeds: 85\n",
      "Remaining seeds: 86\n",
      "Remaining seeds: 86\n",
      "Remaining seeds: 86\n",
      "Remaining seeds: 85\n",
      "Remaining seeds: 84\n",
      "Remaining seeds: 84\n",
      "Remaining seeds: 83\n",
      "Remaining seeds: 83\n",
      "Remaining seeds: 82\n",
      "Remaining seeds: 81\n",
      "Remaining seeds: 80\n",
      "Remaining seeds: 80\n",
      "Remaining seeds: 80\n",
      "Remaining seeds: 79\n",
      "Remaining seeds: 78\n",
      "Remaining seeds: 77\n",
      "Remaining seeds: 76\n",
      "Remaining seeds: 75\n",
      "Remaining seeds: 74\n",
      "Remaining seeds: 74\n",
      "Remaining seeds: 74\n",
      "Remaining seeds: 73\n",
      "Remaining seeds: 72\n",
      "Remaining seeds: 71\n",
      "Remaining seeds: 70\n",
      "Remaining seeds: 69\n",
      "Remaining seeds: 69\n",
      "Remaining seeds: 68\n",
      "Remaining seeds: 67\n",
      "Remaining seeds: 66\n",
      "Remaining seeds: 65\n",
      "Remaining seeds: 65\n",
      "Remaining seeds: 64\n",
      "Remaining seeds: 64\n",
      "Remaining seeds: 63\n",
      "Remaining seeds: 62\n",
      "Remaining seeds: 61\n",
      "Remaining seeds: 60\n",
      "Remaining seeds: 59\n",
      "Remaining seeds: 59\n",
      "Remaining seeds: 58\n",
      "Remaining seeds: 57\n",
      "Remaining seeds: 56\n",
      "Remaining seeds: 57\n",
      "Remaining seeds: 56\n",
      "Remaining seeds: 57\n",
      "Remaining seeds: 56\n",
      "Remaining seeds: 55\n",
      "Remaining seeds: 54\n",
      "Remaining seeds: 53\n",
      "Remaining seeds: 53\n",
      "Remaining seeds: 52\n",
      "Remaining seeds: 51\n",
      "Remaining seeds: 50\n",
      "Remaining seeds: 52\n",
      "Remaining seeds: 53\n",
      "Remaining seeds: 52\n",
      "Remaining seeds: 51\n",
      "Remaining seeds: 50\n",
      "Remaining seeds: 50\n",
      "Remaining seeds: 49\n",
      "Remaining seeds: 49\n",
      "Remaining seeds: 49\n",
      "Remaining seeds: 50\n",
      "Remaining seeds: 49\n",
      "Remaining seeds: 48\n",
      "Remaining seeds: 47\n",
      "Remaining seeds: 46\n",
      "Remaining seeds: 46\n",
      "Remaining seeds: 45\n",
      "Remaining seeds: 44\n",
      "Remaining seeds: 43\n",
      "Remaining seeds: 42\n",
      "Remaining seeds: 41\n",
      "Remaining seeds: 40\n",
      "Remaining seeds: 40\n",
      "Remaining seeds: 42\n",
      "Remaining seeds: 41\n",
      "Remaining seeds: 41\n",
      "Remaining seeds: 40\n",
      "Remaining seeds: 40\n",
      "Remaining seeds: 39\n",
      "Remaining seeds: 39\n",
      "Remaining seeds: 38\n",
      "Remaining seeds: 37\n",
      "Remaining seeds: 37\n",
      "Remaining seeds: 36\n",
      "Remaining seeds: 35\n",
      "Remaining seeds: 35\n",
      "Remaining seeds: 34\n",
      "Remaining seeds: 33\n",
      "Remaining seeds: 32\n",
      "Remaining seeds: 33\n",
      "Remaining seeds: 32\n",
      "Remaining seeds: 31\n",
      "Remaining seeds: 30\n",
      "Remaining seeds: 29\n",
      "Remaining seeds: 28\n",
      "Remaining seeds: 27\n",
      "Remaining seeds: 26\n",
      "Remaining seeds: 25\n",
      "Remaining seeds: 24\n",
      "Remaining seeds: 23\n",
      "Remaining seeds: 22\n",
      "Remaining seeds: 21\n",
      "Remaining seeds: 20\n",
      "Remaining seeds: 19\n",
      "Remaining seeds: 18\n",
      "Remaining seeds: 17\n",
      "Remaining seeds: 17\n",
      "Remaining seeds: 16\n",
      "Remaining seeds: 18\n",
      "Remaining seeds: 20\n",
      "Remaining seeds: 20\n",
      "Remaining seeds: 19\n",
      "Remaining seeds: 18\n",
      "Remaining seeds: 17\n",
      "Remaining seeds: 16\n",
      "Remaining seeds: 15\n",
      "Remaining seeds: 15\n",
      "Remaining seeds: 14\n",
      "Remaining seeds: 14\n",
      "Remaining seeds: 13\n",
      "Remaining seeds: 12\n",
      "Remaining seeds: 11\n",
      "Remaining seeds: 10\n",
      "Remaining seeds: 9\n",
      "Remaining seeds: 8\n",
      "Remaining seeds: 7\n",
      "Remaining seeds: 6\n",
      "Remaining seeds: 6\n",
      "Remaining seeds: 5\n",
      "Remaining seeds: 4\n",
      "Remaining seeds: 3\n",
      "Remaining seeds: 2\n",
      "Remaining seeds: 1\n",
      "Remaining seeds: 0\n"
     ]
    }
   ],
   "source": [
    "SEED_FILE_NAME = 'seeds.csv'\n",
    "\n",
    "NODE_FILE_NAME = 'papers.csv'\n",
    "NODE_FILE_FIELDS = ['paperId', 'title', 'author', 'venue', 'year', 'citationCount',  'url', 'abstract', 'embedding']\n",
    "\n",
    "EDGE_FILE_NAME = 'references.csv'\n",
    "EDGE_FILE_FIELDS=['from_id', 'to_id', 'isInfluential', 'intents', 'contexts']\n",
    "\n",
    "\n",
    "with open(NODE_FILE_NAME, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=NODE_FILE_FIELDS)\n",
    "    writer.writeheader()\n",
    "\n",
    "with open(EDGE_FILE_NAME, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=EDGE_FILE_FIELDS)\n",
    "    writer.writeheader() \n",
    "\n",
    "seed_ids = []\n",
    "processed_ids = set()  # 이미 처리된 id를 저장할 집합\n",
    "\n",
    "for seed_paper in get_seeds(SEED_FILE_NAME):\n",
    "    print(seed_paper)\n",
    "    seed_id = search_paper(seed_paper)\n",
    "    seed_ids.append(seed_id)\n",
    "    processed_ids.add(seed_id)\n",
    "\n",
    "while seed_ids:    \n",
    "    seed_id = seed_ids.pop(0)\n",
    "    detail = get_detail(seed_id)                                    \n",
    "    write_node_data(detail, NODE_FILE_NAME, NODE_FILE_FIELDS) \n",
    "    \n",
    "    refs = get_references(seed_id)\n",
    "    for ref in refs:\n",
    "        ref_id = ref['citedPaper']['paperId']\n",
    "        # ref의 paperId가 seed_papers에 없고, 처리되지 않은 id인 경우 추가\n",
    "        if ref_id not in seed_ids and ref_id not in processed_ids:\n",
    "            seed_ids.append(ref_id)\n",
    "            processed_ids.add(ref_id)\n",
    "\n",
    "        edge_data = {\n",
    "            'from_id': seed_id,\n",
    "            'to_id': ref_id,\n",
    "            'isInfluential': ref['isInfluential'],\n",
    "            'intents': ','.join(ref['intents']),\n",
    "            'contexts': ','.join(ref['contexts'])\n",
    "        }\n",
    "\n",
    "        write_edge_data(edge_data, EDGE_FILE_NAME, EDGE_FILE_FIELDS)\n",
    "        \n",
    "    print(f\"Remaining seeds: {len(seed_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be339e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_paper(\"RoboCat: A self-improving robotic agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dca0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
