from_id,to_id,isInfluential,intents,contexts
162e2e9ac70702c146c0aa8432e4a6806bb8c42e,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,background,"Recently, there has been increasing interest in using LLMs to find a sequence of executable actions for robots, aiming to achieve high-level goals expressed in natural language, such as SayCan (Ahn et al., 2022) and Innermonologue (Huang et al.,The baseline for this dataset follows the method in SayCan’s open-source virtual tabletop environment, where GPT-3 is used as the large language model to directly find the sequence of actions from Si to Sg.,We based our work on SayCan’s open-source virtual tabletop environment6, where a robot is tasked with achieving a goal, such as ""stack the blocks,"" on a table with colored blocks and bowls.,Recently, there has been increasing interest in using LLMs to find a sequence of executable actions for robots, aiming to achieve high-level goals expressed in natural language, such as SayCan (Ahn et al., 2022) and Innermonologue (Huang et al.,
2022).,Similarly, LLMs gained attention for plan generation for robots due to the rich semantic knowledge they acquired about the world (Ahn et al., 2022; Huang et al., 2022; Zeng et al., 2022).,We noticed that the successful plans demonstrated by SayCan are restricted to simple one-step look-ahead plans that do not take into account intermediate state changes."
38fe8f324d2162e63a967a9ac6648974fc4c66f3,0e34addae55a571d7efd3a5e2543e86dd7d41a83,True,"background,methodology","Prior works focus on combining vision and language inputs in an embodied setting with the goal of direct action prediction (Guhur et al., 2022; Shridhar et al., 2022b;a; Zhang & Chai, 2021; Silva et al., 2021; Jang et al., 2022; Nair et al., 2022; Lynch et al., 2022; Brohan et al., 2022).,2 reports success rates on long-horizon tasks from the Language-Table environment (Lynch et al., 2022).,Given the observed image and a long-horizon goal, e.g. “sort the blocks by colors into corners”, PaLM-E outputs language subgoals at 1 Hz to the policies from Lynch et al. (2022), that output low-level robot actions at 5 Hz.,Interactive Language Table
We use the Language-Table real-world tabletop setup and simulated environment from Interactive Language (Lynch et al., 2022).,The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset (Lynch et al., 2022) and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics.,Prior work (Lynch et al., 2022) instead involved a human in the loop to interactively guide subgoals and corrections.,We pass the short horizon instructions to an Interactive Language policy trained using the same procedure as in Lynch et al. (2022).,While many works focus on understanding natural language goals (Lynch & Sermanet, 2020; Shridhar et al., 2022a; Nair et al., 2022; Lynch et al., 2022), fewer consider natural language as a representation for planning – the focus of this work."
38fe8f324d2162e63a967a9ac6648974fc4c66f3,c305ab1bdba79442bec72ec7f5c5ee7c49c2a566,True,"background,methodology","…LLM’s generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022;…,For a robot to do closed-loop planning, it is also important to detect failures, as is shown in (Huang
et al., 2022c).,…(Huang et al., 2022c), generating world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022; Huang et al., 2022a), visual explanations (Wang et al., 2023), program generation (Liang et al., 2022; Singh et al., 2022), or injecting information…,…of instructions directly from an LLM either by leveraging semantic similarity between an LLM’s generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models…"
38fe8f324d2162e63a967a9ac6648974fc4c66f3,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"background,methodology","…LLM’s generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022;…,For a robot to do closed-loop planning, it is also important to detect failures, as is shown in (Huang
et al., 2022c).,…(Huang et al., 2022c), generating world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022; Huang et al., 2022a), visual explanations (Wang et al., 2023), program generation (Liang et al., 2022; Singh et al., 2022), or injecting information…,…of instructions directly from an LLM either by leveraging semantic similarity between an LLM’s generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models…"
38fe8f324d2162e63a967a9ac6648974fc4c66f3,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","…semantic similarity between an LLM’s generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over…,Table Data Only SayCan Data Only Full Mixture (All robots + WebLI, VQA, COCO, etc.)
0% 25%
50%
75%
100%
PaLM-E Training Data
Su cc
es s
R at
e or
A cc
ur ac
y
LLM finetune (full mixture)
LLM finetune (single robot)
without pretraining
LLM frozen (full mixture)
LLM frozen (single robot)
20% 40% 60% 80% 100%
94.9%
48.6%
42.9%
74.3%
31.8%
Figure 4: Planning success results in the TAMP environment (1% data) for PaLM-E-12B, comparing of the effects of PaLM-E models (i) using the full training mixture, (ii) pre-training (ViT and PaLM), and (iii) freezing or finetuning the language model.,As baselines, we consider the state-of-the art visual language model PaLI (Chen et al., 2022), which has not been trained on embodiment robot data, as well as the SayCan algorithm (Ahn et al., 2022), supplied with oracle affordances.,We train the model by using the runs from (Ahn et al., 2022), which contains 2912 sequences.,The SayCan baseline (Ahn et al., 2022) utilizes oracle affordance functions and has difficulties solving this environment, since affordance functions only constrain what is possible right now, but are not informative enough for the LLM to construct long-horizon plans in TAMP environments.,We largely follow the setup in Ahn et al. (2022), where the robot needs to plan a sequence of navigation and manipulation actions based on an instruction by a human.,As with the TAMP environment, neither SayCan nor zero-shot PaLI are effective, unable to solve the easiest task tested.,, 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al.,Previous work (Ahn et al., 2022) interfaces the output of LLMs with learned robotic policies and affordance functions to make decisions, but is limited in that the LLM itself is only provided with textual input, which is insufficient for many tasks where the geometric configuration of the scene is important.,After each step is decoded, we map them to a low-level policy as defined in Ahn et al. (2022).,Previous work (Ahn et al., 2022) interfaces the output of LLMs with learned robotic policies and affordance functions to make decisions, but is limited in that the LLM itself is only provided with textual input, which is insufficient for many tasks where the geometric configuration of the scene is…,Finally, we consider a mobile manipulation domain similar to SayCan (Ahn et al., 2022), where a robot has to solve a variety of tasks in a kitchen environment, including finding objects in drawers, picking them, and bringing them to a human.,, 2022), which has not been trained on embodiment robot data, as well as the SayCan algorithm (Ahn et al., 2022), supplied with oracle affordances."
38fe8f324d2162e63a967a9ac6648974fc4c66f3,ada81a4de88a6ce474df2e2446ad11fea480616e,True,"background,methodology","…world models (Nottingham et al., 2023; Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022; Huang et al., 2022a), visual explanations (Wang et al., 2023), program generation (Liang et al., 2022; Singh et al., 2022), or injecting information into the prompt (Zeng et al., 2022).,In addition, the model can perform, zero-shot, question and answering on temporally-annotated egocentric vision, similar to what was shown in (Zeng et al., 2022) but end-to-end all in one model.,Zero-shot CoT (Kojima et al., 2022), originally a language-only concept, has been shown on multimodal data with task-specific programs (Zeng et al., 2022) but to our knowledge, not via an end-to-end model.,, 2022), originally a language-only concept, has been shown on multimodal data with task-specific programs (Zeng et al., 2022) but to our knowledge, not via an end-to-end model.,, 2022), or injecting information into the prompt (Zeng et al., 2022).,The egocentric video images are from https://youtu.be/ -UXKmqBPk1w, as in (Zeng et al., 2022), via permission from creator Cody Wanner."
e00867c7108395d56b823bc5c75d2f4591e2878d,cdf54c147434c83a4a380916b6c1279b0ca19fc2,True,"background,methodology","Different from previous work [8] that uses LLM to extract names from a sentence, our object proposal is much more demanding in four different ways as we will discuss in Sec.,Recent progress in large language models (LLMs), has shown impressive few-shot performance in language comprehension, semantic understanding, and reasoning, as well as application to robotics problems like planning [5]–[7] and instruction following [8].,LM-Nav [8] uses three pretrained model to perform visual language navigation.,1) Infer objects from implication of the instruction: In previous work [8] that use LLM to extract object names from language, all object names are nouns that are directly present in the language input."
e00867c7108395d56b823bc5c75d2f4591e2878d,2e196c3143f8ca8401d9bb6e7510b40ea4ef4cf7,True,methodology,"Recently, methods that leverage pre-trained image-text models can do zero-shot Object Goal Navigation [31], [32].,Our work can use the representation from a single exploration for many downstream planning tasks without the need to run Object Goal Navigation every time.,Our work differs from Object Goal Navigation since the eventual goal is not purely finding objects, but using object presence and location information for planning.,Object Goal Navigation.,Though a robot can avoid building a semantic representation by finding objects each time they are required, e.g., with Object Goal Navigation [1], [2], this repeated exploration can be inefficient."
e00867c7108395d56b823bc5c75d2f4591e2878d,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","Therefore, the novel object names in this experiment are either used for navigation only, or for describing objects that are visually similar to training objects in [6].,We note that manipulation policies used in this project are still limited to be with the objects that are visually similar to training objects in [6] and rely on the generalization to slightly out-of-distribution data.,In contrast, SayCan [6] showed how value functions of learned skills can provide such a grounding through selecting options scored highly by a language model and an affordance model.,Recently LLM-based planners are more flexible [6], [7], [36] and do not require handcrafting predicates, however, they do not handle the complexity of open-vocabulary object proposal and require defining a set of objects involved in planning.,Previously, SayCan [6] presents a framework that allows robots to plan and execute in the real world following human instructions.,Recent work, SayCan [6], has shown how large language models can be applied to such problems through world-grounding affordance functions, allowing LLMs to understand what a robot can do from a state.,1) Generate executable options: Vanilla SayCan [6] provides a list of skills associated with either 1) navigation policies to hard-coded locations 2) manipulation policies (pick and place) of objects, specified by object names.,NLMap +SayCan shows comparable performance as SayCan on instructions from [6] while enabling new tasks SayCan cannot do before due to its lack of contextual grounding."
e00867c7108395d56b823bc5c75d2f4591e2878d,62516303058a1322450b58e4cd778ab873b5e531,True,background,"Recently, methods that leverage pre-trained image-text models can do zero-shot Object Goal Navigation [31], [32].,Our work can use the representation from a single exploration for many downstream planning tasks without the need to run Object Goal Navigation every time.,, with Object Goal Navigation [1], [2], this repeated exploration can be inefficient.,Our work differs from Object Goal Navigation since the eventual goal is not purely finding objects, but using object presence and location information for planning.,Object Goal Navigation.,Though a robot can avoid building a semantic representation by finding objects each time they are required, e.g., with Object Goal Navigation [1], [2], this repeated exploration can be inefficient.,A few of these algorithms construct a semantic map of the current region before planning in that region [1], [22]–[24]."
93565fe6db3948c9c414af1d1edccf4aff5e2e10,5e2bceb56f116e98baf7e418208057bc0e1c1861,True,"background,methodology","Therefore, the visual encoder is better at segmenting common objects while worse at recognizing general visual concepts [5].,In contrast, ConceptFusion integrates pre-trained CLIP features into the map without fine-tuning, enabling it to recognize general concepts including regions, and thus the indexing results are improved.,Most related to our work are approaches like VLMaps [2], NLMap [41], and ConceptFusion [5], all of which combine pre-trained visual-language models with a 3D reconstruction of the scene, enabling landmark indexing with natural language and downstream language-based planning tasks.,While recent advances in training modality-specific foundations models have spurred map representations that integrate pre-trained visual-language features for improved scene understanding [2], [3], [4], [5], how to best spatially anchor additional sensing modalities, such as audio signals, in ways that enable effective data-efficient cross-modal reasoning for downstream robotics tasks, remains a relatively open question.,Concurrent work ConceptFusion [5] demonstrates that audio can be used as queries to index locations in a visual-language map."
58d80c480014c526caed7f06e86398e34070bbc9,3d8acd09296c2963c8e71aac1205f58d631307da,True,methodology,"I) with two existing baselines for natural language-to-LTL translation: CopyNet [29], and an RNN with attention mechanism1 (denoted RNN) [6].,78 CopyNet [29] 4/5 golden 1/5 golden 88.,Other work [29] [30] improves the translators’ generalization to new domains; this is complementary to our method, which improves accuracy within a given set of domains and reduces reliance on human-labeled data.,39 CopyNet [29] synthetic full golden 36."
41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"background,methodology","We evaluate CaP on a simulated table-top manipulation environment from [16], [18].,, semantic parsing [25], [27]–[32]), planning [14], [17], [18], and low-level policies (e.,, planning a sequence of steps from natural language instructions [16]–[18] without additional model finetuning.,Inner Monologue [18] expands LLM planning by incorporating outputs from success detectors or other visual language models and uses their feedback to re-plan.,Moreover, the natural-language planners [14], [16]–[18] are not applicable for tasks that require precise numerical spatialgeometric reasoning."
41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","For example, Huang et al. decompose natural language commands into sequences of executable actions by text completion and semantic translation [14], while SayCan [17] generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances [20] from value functions.,, semantic parsing [25], [27]–[32]), planning [14], [17], [18], and low-level policies (e.,decompose natural language commands into sequences of executable actions by text completion and semantic translation [14], while SayCan [17] generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances [20] from value functions.,It also illustrates the ability to follow long-horizon reactive commands with control structures as well as precise spatial reasoning, which cannot be easily accomplished by prior works [16], [17], [36].,Language serves not only as an interface for non-experts to interact with robots [24], [25], but also as a means to compositionally scale generalization to new tasks [9], [17]."
41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,ada81a4de88a6ce474df2e2446ad11fea480616e,True,"background,methodology","We evaluate CaP on a simulated table-top manipulation environment from [16], [18].,Place coke can
Socratic Models Plan [16] objects = [coke can] 1. robot.grasp(coke can) open vocab 2. robot.place_a_bit_right()
plans generated by prior works assume there exists a skill that allows the robot to move an object a bit right.,, planning a sequence of steps from natural language instructions [16]–[18] without additional model finetuning.,Socratic Models [16] uses visual language models to substitute perceptual information (in teal) into the language prompts that generate plans, and it uses language-conditioned policies e.,Meanwhile, recent progress in natural language processing shows that large language models (LLMs) pretrained on Internetscale data [11]–[13] exhibit out-of-the-box capabilities [14]–[16] that can be applied to language-using robots e.,Socratic Models [16] uses visual language models to substitute perceptual information (in teal) into the language prompts that generate plans, and it uses language-conditioned policies e.g., for grasping [36].,Moreover, the natural-language planners [14], [16]–[18] are not applicable for tasks that require precise numerical spatialgeometric reasoning.,It also illustrates the ability to follow long-horizon reactive commands with control structures as well as precise spatial reasoning, which cannot be easily accomplished by prior works [16], [17], [36].,Place coke can Socratic Models Plan [16]"
41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","On unseen tasks and attributes, endto-end systems like CLIPort struggle to generalize, and CaP outperforms LLM reasoning directly with language (also observed in [20]).,We consider two baselines: (i) language-conditioned multi-task CLIPort [36] policies trained via imitation learning on 30k demonstrations, and (ii) few-shot prompted LLM planner using natural language instead of code.,With unseen task attributes, CLIPort’s performance degrades significantly, while LLM-based methods retain similar performance.,, model-based [33]–[35], imitation learning [8], [9], [36], [37], or reinforcement learning [38]–[42]).,Train/Test Task Family CLIPort [36] NL Planner CaP (ours),CaP compares competitively to the supervised CLIPort baseline on tasks with seen attributes and instructions, despite only few-shot prompted with one example rollout for each task.,It also illustrates the ability to follow long-horizon reactive commands with control structures as well as precise spatial reasoning, which cannot be easily accomplished by prior works [16], [17], [36].,, encoding spatial relationships by leaning on familiarity of third party libraries) without additional training needed in prior works [35], [36], [52]–[56], and (ii) hierarchical code-writing (inspired by recursive summarization [57]) improves state-of-the-art code generation.,LMPs can be generated hierarchically by composing known functions (e.g., get_obj_names() using perception modules) or invoking other LMPs to define undefined functions:
# define function stack_objs_in_order(obj_names). def stack_objs_in_order(obj_names):
for i in range(len(obj_names) - 1): put_first_on_second(obj_names[i + 1], obj_names[i])
where put_first_on_second is an existing open vocabulary pick and place primitive (e.g., CLIPort [36])."
0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3,1ccd73d95b5e23279929866bc15ec2983f9fa700,True,"background,methodology",", 2022] and Inner Monologue [Huang et al., 2022b] used LLMs as high-level planners in robotics setups. Because their LLM is not directly used as agent policy for low-level actions and is not grounded using its interactions with the environment, Ahn et al. [2022] had to use an external affordance function to re-rank the actions proposed by the LLM. Similarly, Yao et al. [2022] also featured a closed-loop feedback between an LLM that is the planner and an agent that is the actor but this time in a textual environment.,However, our work studies how LLMs can not only encode this instruction [Hill et al., 2020] but also be directly used as agent policies choosing actions given the observation.,, 2022] and Inner Monologue [Huang et al., 2022b] used LLMs as high-level planners in robotics setups. Because their LLM is not directly used as agent policy for low-level actions and is not grounded using its interactions with the environment, Ahn et al. [2022] had to use an external affordance function to re-rank the actions proposed by the LLM."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,c03fa01fbb9c77fe3d10609ba5f1dee33a723867,True,"methodology,background",", 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a).,Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b).,Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,As with previous work (Singh et al., 2022; Liang et al., 2022b), we find that structured code output works well for extracting knowledge from LLMs."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"methodology,background","Adapters are especially well suited for modular finetuning due to their lightweight architecture (Liang et al., 2022a).,We implement modularity for Minecraft by finetuning a pretrained transformer policy with adapters, a technique recently implemented for RL by Liang et al. (2022a) for multi-task robotic policies.,Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a).,Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b).,Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,As with previous work (Singh et al., 2022; Liang et al., 2022b), we find that structured code output works well for extracting knowledge from LLMs."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"methodology,background","Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a).,Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b).,However, task specific textual knowledge is expensive to obtain, prompting the use of web queries (Nottingham et al., 2022) or models pretrained on general world knowledge (Dambekodi et al., 2020; Suglia et al., 2021; Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b)."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,65fc1f1c567801fee3788974e753cdbf934f07e9,True,"methodology,background","We use the video pretrained (VPT) Minecraft agent (Baker et al., 2022) as a starting point for exploration and finetuning, and we use the Minedojo implementation of the Minecraft Environment (Fan et al., 2022).,Following Baker et al. (2022), we replace the traditional entropy loss in the PPO algorithm with a KL loss between the current policy and the non-finetuned VPT policy.,We use the Video-Pretrained (VPT) Minecraft model as our starting policy (Baker et al., 2022).,Reaching a goal item is difficult without expert knowledge of Minecraft via dense rewards (Baker et al., 2022; Hafner et al., 2023) or expert demonstrations (Skrynnik et al., 2021; Patil et al., 2020), making item crafting in Minecraft a longstanding AI challenge (Guss et al., 2019; Fan et al.,…,VPT Finetuning
We finetune VPT (3x w/ behavior cloning on house contractor data) (Baker et al., 2022) with reinforcement learning (RL) using transformer adapters as described by Houlsby et al. (2019).,As with VPT (Baker et al., 2022), our subgoal policies use a pixel only observation space and a large multi-discrete action space, while our overall policy transitions between subgoals based on the agent’s current inventory.,We followed the VPT (Baker et al., 2022) observation and action spaces—128x128x3 pixel observation space and 121x8461 multi-discrete action space—with the modification of replacing the “open inventory” action with 254 discrete crafting actions."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"methodology,background","Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a).,Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b).,However, task specific textual knowledge is expensive to obtain, prompting the use of web queries (Nottingham et al., 2022) or models pretrained on general world knowledge (Dambekodi et al., 2020; Suglia et al., 2021; Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b)."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,0af51c6a6041a439796a334f65327a992c8a3e63,True,background,", 2020) prompting many improvements to decision making conditioned on language instructions (Yu et al., 2018; Lynch & Sermanet, 2020; Nottingham et al., 2021; Suglia et al., 2021; Kuo et al., 2021; Zellers et al., 2021; Song et al., 2022a; Blukis et al., 2022).,…2018; Ku et al., 2020; Shridhar et al., 2020) prompting many improvements to decision making conditioned on language instructions (Yu et al., 2018; Lynch & Sermanet, 2020; Nottingham et al., 2021; Suglia et al., 2021; Kuo et al., 2021; Zellers et al., 2021; Song et al., 2022a; Blukis et al., 2022).,However, task specific textual knowledge is expensive to obtain, prompting the use of web queries (Nottingham et al., 2022) or models pretrained on general world knowledge (Dambekodi et al., 2020; Suglia et al., 2021; Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b).,, 2022) or models pretrained on general world knowledge (Dambekodi et al., 2020; Suglia et al., 2021; Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b)."
fc918d6f8e2523696c34fa1be5aabdb42e9648d2,2739d1e71c72f153d73fe5f62775c87b0afe0a59,True,"methodology,background","Adapters are especially well suited for modular finetuning due to their lightweight architecture (Liang et al., 2022a).,We implement modularity for Minecraft by finetuning a pretrained transformer policy with adapters, a technique recently implemented for RL by Liang et al. (2022a) for multi-task robotic policies.,Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a).,Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b).,Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b).,As with previous work (Singh et al., 2022; Liang et al., 2022b), we find that structured code output works well for extracting knowledge from LLMs."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,15ac70d077bb735eed4a8502ce49aa7782c803fd,True,"methodology,background","To study the effectiveness of our hierarchical architecture, we benchmark against two languageconditioned baselines: HULC [10] and BC-Z [4].,More recently, end-to-end learning has been used to study the challenging problem of fusing perception, language and control [4], [27], [28], [1], [10], [9], [15], [5].,0 (HULC++), a hierarchical language-conditioned agent that integrates the task-agnostic control of HULC [10] with the object-centric semantic,to templated pick-and-place operations [6], [7] or deploying the policies in simpler simulated environments [8], [9], [10].,We model the low-level policy with a general-purpose goalreaching policy based on HULC [10] and trained with multicontext imitation learning [9].,that grounds 3D spatial interaction knowledge against HULC [10], a state-of-the-art end-to-end model that learns general skills grounded on language from play data."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,True,methodology,"image representations [29], [6], [30] to bootstrap downstream task learning, which we also leverage in this work.,Moreover, when initializing our affordance model with pretrained weights of R3M [29], a work that aims to learn reusable representations for learning robotic skills, HULC++ sets a new state of the art with an average sequence length of 3.,Moreover, when initializing our affordance model with pretrained weights of R3M [29], a work that aims to learn reusable representations for learning robotic skills, HULC++ sets a new state of the art with an average sequence length of 3.30.,For a fair comparison, all models have the same observation and action space, and have their visual encoders for the static camera initialized with pre-trained ResNet-18 R3M features [29]."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,8d3fddfe59a21b8245b375d33fe91e215237ddb1,True,"methodology,background","We show that by extending VAPO to learn language-conditioned affordances and combining it with a 7-DoF low-level policy that builds upon HULC, our method is capable of following multiple long-horizon manipulation tasks in a row, directly from images, while requiring an order of magnitude less data
979-8-3503-2365-8/23/$31.00 ©2023 IEEE 11576
20 23
IE EE
In te
rn at
io na
l C on
fe re
nc e
on R
ob ot
ic s a
nd A
ut om
at io
n (IC
RA ) |
9 79
-8 -3
50 3-
23 65
-8 /2
3/ $3
1.,Prior studies show that decomposing robot manipulation into semantic and spatial pathways [12], [13], [6], improves generalization, data-efficiency, and understanding of multimodal information.,Specifically, we present Hierarchical Universal Language Conditioned Policies 2.0 (HULC++), a hierarchical language-conditioned agent that integrates the task-agnostic control of HULC [10] with the object-centric semantic understanding of VAPO [13].,control of HULC [10] with the object-centric semantic understanding of VAPO [13].,work that decomposes robot manipulation into semantic and spatial pathways [12], [13], [6], we propose leveraging a self-supervised affordance model from unstructured data that guides the robot to the vicinity of actionable regions referred ""Move the sliding door to the right"" Projected end-effector position,VAPO extracts a self-supervised visual affordance model of unstructured data and not only accelerates learning, but was also shown to boost generalization of downstream control policies."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,1d803f07e4591bd67c358eef715bcd443e821894,True,background,"To study the effectiveness of our hierarchical architecture, we benchmark against two languageconditioned baselines: HULC [10] and BC-Z [4].,Learning such sensorimotor skills and grounding them in language typically requires either a massive large-scale data collection effort [1], [2], [4], [5] with frequent human interventions, limiting the skills to templated pick-and-place operations [6], [7] or deploying the policies in simpler simulated environments [8], [9], [10].,More recently, end-to-end learning has been used to study the challenging problem of fusing perception, language and control [4], [27], [28], [1], [10], [9], [15], [5].,Additionally, this setting contains an order of magnitude less data than related approaches [4].,The BC-Z baseline, on the other hand, is trained only on the data that contains language annotation and includes the proposed auxiliary loss that predicts the language embeddings from the visual ones for better aligning the visuo-lingual skill embeddings [4].,However, in order to jointly learn language, vision, and control, it needs a large amount of robot interaction data, similar to other end-toend agents [4], [9], [15].,In the area of robot manipulation, the two extremes of the spectrum are CLIPort [6] on the one hand, and agents like GATO [5] and BC-Z [4] on the other, which range from needing a few hundred expert demonstrations for pick-and-placing objects with motion planning, to several months of data collection of expert demonstrations to learn visuomotor manipulation skills for continuous control.,We observe that the BC-Z baseline has near zero performance in most tasks, due to insufficient demonstrations."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"methodology,background","Inspired by the line of work that decomposes robot manipulation into semantic and spatial pathways [12], [13], [6], we propose leveraging a self-supervised affordance model from unstructured data that guides the robot to the vicinity of actionable regions referred ""Move the sliding door to the right"" Projected end-effector position,Learning such sensorimotor skills and grounding them in language typically requires either a massive large-scale data collection effort [1], [2], [4], [5] with frequent human interventions, limiting the skills to templated pick-and-place operations [6], [7] or deploying the policies in simpler simulated environments [8], [9], [10].,Prior studies show that decomposing robot manipulation into semantic and spatial pathways [12], [13], [6], improves generalization, data-efficiency, and understanding of multimodal information.,Earlier works focused on localizing objects mentioned in referring expressions [20], [21], [22], [23], [24] and following pick-and-place instructions with predefined motion primitives [25], [6], [26].,In the area of robot manipulation, the two extremes of the spectrum are CLIPort [6] on the one hand, and agents like GATO [5] and BC-Z [4] on the other, which range from needing a few hundred expert demonstrations for pick-and-placing objects with motion planning, to several months of data collection of expert demonstrations to learn visuomotor manipulation skills for continuous control.,image representations [29], [6], [30] to bootstrap downstream task learning, which we also leverage in this work."
8f84dcbad8cd3b5b4d9229c56bc95f24be859a35,1301e9d11b728268ed1ff3f1a9adc155308d5250,True,"methodology,background","[9] showed that pairing a small number of random windows with language after-the-fact instructions, enables learning a single language-conditioned visuomotor policy that can perform a wide variety of robotic manipulation tasks.,More recently, end-to-end learning has been used to study the challenging problem of fusing perception, language and control [4], [27], [28], [1], [10], [9], [15], [5].,to templated pick-and-place operations [6], [7] or deploying the policies in simpler simulated environments [8], [9], [10].,other end-to-end agents [4], [9], [15].,We model the low-level policy with a general-purpose goalreaching policy based on HULC [10] and trained with multicontext imitation learning [9]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,"methodology,background","We refer to training on only DA as the Interactive Language (IL) [32] setting, training on only DB as the RT-1 [7] setting, and training on both DA and DB as the RT-1 + IL setting.,Using these various instruction augmented datasets, we train vision-based language-conditioned behavioral cloning policies with the RT-1 architecture [7].,Table II demonstrates that DIAL is able to solve over 40% more challenging novel tasks across the three evaluation categories compared to either RT-1 and/or IL, which do not use the instruction augmented data DC .,One main difference from RT-1 is that instead of utilizing USE [10] as the task representation for language conditioning, we instead use the language encoder of the fine-tuned CLIP model that was used for instruction
augmentation in Section III-B; full details are described further in Appendix G. Nonetheless, we treat the behavioral cloning policy as an independent component of our method and focus on studying instruction augmentation methods; we do not explore different policy architectures or losses in this work.,DB contains 80,000 episodes with structured teleoperator commands and is representative of the RT-1 [7] setting.,The policies used in this work are trained using the RT-1 [7] architecture on a large dataset of human-provided demonstrations."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,0e34addae55a571d7efd3a5e2543e86dd7d41a83,True,"methodology,background","We refer to training on only DA as the Interactive Language (IL) [32] setting, training on only DB as the RT-1 [7] setting, and training on both DA and DB as the RT-1 + IL setting.,However the performance of such methods is critically dependent on the quantity and breadth of instruction-labeled demonstration data that is available [32], and producing expert demonstrations of robot motion often requires expertise and time [33].,Similarly, Interactive Language [32]
uses crowd-sourced hindsight labels on diverse demonstration data for table-top object rearrangements.,DA contains 5,600 episodes with crowd-sourced language instructions and is representative of the Interactive Language (IL) [32] setting.,Similarly, Interactive Language [32] uses crowd-sourced hindsight labels on diverse demonstration data for table-top object rearrangements.,This experiment is practically motivated by the setting where large amounts of unstructured trajectory data are available but hindsight labels are expensive to collect, such as robot play data [14, 31, 32]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"methodology,background","For long-horizon language instructions, LLMs are used as planners both in a simulated [22] and real-world robotics [2].,Finally, we would like to thank the large team that built [1] and [2], upon which we develop DIAL.,2), where teleoperators perform 551 unique tasks motivated by common manipulation skills and objects in a kitchen environment [2].,A popular method used to learn such policies is behavioral cloning (BC) [24, 32], which has been applied to robotic datasets [2] with diverse language instructions [31].,Pretrained VLMs and LLMs for language-conditioned control Prior works have leveraged pretrained VLMs and LLMs for language-conditioned control, as part of reward modeling [17, 35], as part of the agent architecture [36, 41], or as planners for long-horizon tasks [2, 22, 23].,demonstrations is available, collected for downstream imitation learning [2, 24].,Recent advances in deep learning with large amounts of data have led to works following natural language for robotic manipulations [2, 27, 33, 42, 43].,In robotics, they have been used as representations for perception [36, 41], as task representation for language [24, 29], or as planners [2, 22].,We implement DIAL in a challenging real-world robotic manipulation setting in a kitchen environment similar to SayCan [2].,These trajectories may be collected from human teleoperated demonstrations on a wide variety of tasks [2], or from unstructured robotic “play” data [30]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,1d803f07e4591bd67c358eef715bcd443e821894,True,"methodology,background","We focus on the practically-motivated setting where a dataset of teleoperated demonstrations is available, collected for downstream imitation learning [1, 24].,In robotics, they have been used as representations for perception [37, 42], as task representation for language [24, 30], or as planners [1, 22].,of-the-art language embeddings and pretrained encoders with imitation learning on top of large, manually collected datasets of robotic demonstrations annotated with text commands [24].,a) Language instruction following in robotics: Languageinstruction following agents have been extensively explored with engineered symbolic representations [17, 45], with reinforcement learning (RL) [5, 20, 29], and with imitation learning [3, 6, 24, 30]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,methodology,"CLIPort [42] uses a frozen CLIP vision and text encoders in combination with Transporter networks [47] for imitation learning.,In robotics, they have been used as representations for perception [37, 42], as task representation for language [24, 30], or as planners [1, 22].,Motivated by promising results of CLIP in robotics in prior works [35, 42], our instantiation of DIAL uses CLIP [39] for both instruction augmentation and task representation; nonetheless, other VLMs or captioning models could also be used to propose instruction augmentations.,b) Pretrained VLMs and LLMs for language-conditioned control: Prior works have leveraged pretrained VLMs and LLMs for language-conditioned control, as part of reward modeling [18, 36], as part of the agent architecture [37, 42], or as planners for long-horizon tasks [1, 22, 23]."
ea5ee9b6b8d323f6a9d8b3982429d6efd586c34a,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"result,background,methodology","We compare our model against LLM planners [15] and end-to-end language-to-action policies [52].,• Large language models (LLMs) map instructions to language subgoals [70, 65, 14, 15] or program policies [27] with appropriate plan-like prompts.,Lastly, for tabletop manipulation tasks in simulation, the LLMPlanner of [15] assumes access to an oracle success/failure detector.,• LLMplanner, inspired by [15], an instruction-following scene-rearrangement model that uses an LLM to predict a sequence of subgoals in language form, e.,We compare our model against state-of-the-art language-to-action policies [52] as well as Large Language Model planners [15] and show it dramatically outperforms both, especially for long complicated instructions."
ea5ee9b6b8d323f6a9d8b3982429d6efd586c34a,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"result,background,methodology","We compare our model against LLM planners [15] and end-to-end language-to-action policies [52].,We test SREM in scene rearrangement of tabletop environments on simulation benchmarks of previous works [52],,Benchmarks: Existing language-conditioned manipulation benchmarks are usually dominated by a single spatial concept like “inside” [52].,• CLIPort [52], a model that takes as input an overhead RGB-D image and an instruction and uses pre-trained CLIP language and image encoders to featurize the instruction and RGB image, respectively; then fuses these with depth features to predict pick-and-place actions,We have found that these reactive policies, despite impressively effective within the training distribution, typically do not generalize to longer instructions, new object classes and attributes or novel backgrounds [32, 52].,• End-to-end language to action policies [34, 52, 32, 54] map instructions to actions or to object locations directly.,We use the publicly available code of [52].,We further evaluate our model and baselines on four tasks from the CLIPort benchmark [52], namely put-block-inbowls, pack-google objects-seq, pack-google objects-group and assemble-kits-seq.,We compare our model against state-of-the-art language-to-action policies [52] as well as Large Language Model planners [15] and show it dramatically outperforms both, especially for long complicated instructions."
ea5ee9b6b8d323f6a9d8b3982429d6efd586c34a,3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2,True,"background,methodology","Evaluation Metrics: We use the following two evaluation metrics: (i) Task Progress (TP) [69] is the percentage of the referred objects placed in their goal location, e.,Transporter Networks take as input one or more RGB-D images, reproject them to the overhead birds-eye-view, and predict two robot gripper poses: i) a pick pose and ii) a pick-conditioned placement pose.,We modify Transporter Networks to take as input a small image RGB-D patch, instead of a complete image view.,Transporter Networks decompose a given task into a sequence of pick-and-place actions.,We train Transporter Networks from scratch on all our pick-and-place demonstration datasets jointly.,consecutive poses for the robot gripper, such as pushing, sweeping, rearranging ropes, folding, and so on – for more details please refer to [69].,Short-term Manipulation Skills Our low-level policy network is based on Transporter Networks.,using the action parametrization of Transporter Networks [69].,Short-term vision-based manipulation skills We use short-term manipulation policies built upon Transporter Net-
works [69] to move the obejcts to their predicted locations.,place policies that condition on the visual patch around the predicted pick and place locations to rearrange the objects [69].,The original implementation of Transporter Networks supports training with batch size 1 only.,Baselines: We compare SREM to the following baselines:
• CLIPort [52], a model that takes as input an overhead RGB-D image and an instruction and uses pre-trained CLIP language and image encoders to featurize the instruction and RGB image, respectively; then fuses these with depth features to predict pick-and-place actions using the action parametrization of Transporter Networks [69].,Short-term vision-based manipulation skills We use short-term manipulation policies built upon Transporter Networks [69] to move the obejcts to their predicted locations."
0c83058260890052fa77be9de4613f1cb99b4030,99d95b67e34138f006ace406f4e53a97bbd37431,True,"background,methodology","Objects are then matched across the scenes, and planning over pick-and-place actions brings about the desired object displacements [3], [8], [9], [13].,Successful matching is critical to successful interpretation of a goal image, and poor matching has emerged as the principal performance bottleneck in recent works on tabletop rearrangement [3], [13].,backbone of a CNN classifier have been applied to tackle the matching problem [3], [9].,However, progress in learning both grasp prediction networks [10], [11] and segmentation networks [12] that generalise to unseen objects, mean that it is increasingly possible to both localise and grasp objects (two important stages in successful rearrangement [3], [8]).,ResNet50-(S/G) Most recently, ResNet50 features have been used in [3], where they were used for finding object,matching methods are used, from hand crafted features such as colour histograms [14] to object-specific visual features learnt unsupervised [15], and deep feature extractors trained on large scale computer vision datasets [3], [9].,Several of these works note failures in successful matching [3], [9] due to the challenges of accurately comparing objects from only single views.,Existing works have tackled particular parts of this pipeline, with work on improved collision checking [8], high-level planning [3], [9], [18], and vision-based RL,age [3], [9], generally through the integration of pre-trained object segmentation, grasping primitives or grasp prediction networks, and subsequent high-level planning at the object,Furthermore, object rearrangement problems have attracted interest recently as a challenging, and quite general, robotic manipulation problem, with the goal image motivated as a key component [2], [3], [5]–[8]."
0c83058260890052fa77be9de4613f1cb99b4030,5f975172aa9088f24236ccc8fe4bfc01ce6fb9b9,True,background,"However, progress in learning both grasp prediction networks [10], [11] and segmentation networks [12] that generalise to unseen objects, mean that it is increasingly possible to both localise and grasp objects (two important stages in successful rearrangement [3], [8]).,Furthermore, object rearrangement problems have attracted interest recently as a challenging, and quite general, robotic manipulation problem, with the goal image motivated as a key component [2], [3], [5]–[8].,Objects are then matched across the scenes, and planning over pick-and-place actions brings about the desired object displacements [3], [8], [9], [13].,Existing works have tackled particular parts of this pipeline, with work on improved collision checking [8], high-level planning [3], [9], [18], and vision-based RL"
119d3beca449efd9096d58674cf01a99c793a9a7,efb3b4550b7308ddeb2f382d8c1439e6ec7a99ec,True,"result,background,methodology","Open Attr-POMDP also achieves commendable performance in our open-world settings.,Therefore, interactive visual grounding is then introduced into grounding tasks [15, 16, 2, 3, 4, 17].,Attr-POMDP [4] explicitly uses object attributes to guide the planning of human-robot interaction for disambiguation.,Open Attr-POMDP [4] is also a re-implemented version using openworld visual models.,Previous works [1, 2, 3, 4] have tried to resolve the first two challenges and achieved promising results."
119d3beca449efd9096d58674cf01a99c793a9a7,30432eb1deae9c35ff855e8d1422e14361fb123c,True,"background,methodology","To do so, some models [19], [26], [27], [28], [29], [2] are trained on RefCOCO [30], [31], Visual Genome [32], or other vision-language datasets to generate object descriptions fitting the pre-defined question templates.,INGRESSPOMDP [2] re-models INGRESS with a POMDP planner.,Therefore, interactive visual grounding is then introduced into grounding tasks [15], [16], [2], [3], [17], [4], [18].,Following previous works [1], [2], we make the following assumptions in our task: (1) The user is trustworthy and does not change the target during interaction; (2) There is one and only one target object in the scene.,Previous works [1], [2], [3], [4] have tried to resolve the,Particularly, INGRESS [1], [2] builds an interactive system for disambiguation in robot grasping tasks.,It is also noteworthy that both INGRESS and INGRESSPOMDP failed in most cases with a success rate of 15.6% and 13.4% respectively, even lower than ReCLIP, which is non-interactive."
da3b810e10638507be9901e1b5cb94db126c8166,60c8d0619481eaafdd1189af610d0e636271fed5,True,"background,methodology","In robotics, this has translated to grounding of robotic affordances with natural language in models such as CLIPORT (Shridhar et al., 2022a).,Recently, CLIPort (Shridhar et al., 2022a) proposed to integrate pretrained vision-language (VL) models into such frameworks.,Finally, subsequent work may also consider jointly learning and leveraging pretrained representations that are more suitable for grounding interaction modes between
robots and objects, especially for manipulating difficult articulated and deformable objects in our 3D world (Shridhar et al., 2022b).,Our evaluation metric follows Shridhar et al. (2022a) based on the same CLIP encoders, where a dense score between 0 (failure) and 100 (success) represents the fraction of the episode completed correctly.,Our dataset is based off the Ravens benchmark proposed in Zeng et al. (2020), and its adaptation in Shridhar et al. (2022a).,…architectures, recent works have used large-scale pretrained vision-language representations to improve performance for tabletop manipulation (Shridhar et al., 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill learning and planning (Ahn et al., 2022; Huang et al.,…,2a, these works generally treat the pretrained VL model as a semantic prior, for example, by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2022a).,Our dataset extends the benchmark proposed by CLIPORT (Shridhar et al., 2022a), in turn based on the Ravens benchmark from Zeng et al. (2020).,To evaluate zero-shot generalization, all of our main results in Table 1 use the “unseen” variants of the tasks, as described in Shridhar et al. (2022a).,Common tasks include visual question answering (Antol et al., 2015), VL navigation (Anderson et al., 2018), or VL manipulation (Shridhar et al., 2022a;b)."
da3b810e10638507be9901e1b5cb94db126c8166,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology",", 2022), and real-world skill learning and planning (Ahn et al., 2022; Huang et al., 2022).,…architectures, recent works have used large-scale pretrained vision-language representations to improve performance for tabletop manipulation (Shridhar et al., 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill learning and planning (Ahn et al., 2022; Huang et al., 2022).,2a, these works generally treat the pretrained VL model as a semantic prior, for example, by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2022a).,Similar paradigms have also been applied to other manipulation environments such as Khandelwal et al. (2022) and Ahn et al. (2022)."
da3b810e10638507be9901e1b5cb94db126c8166,826383e18568c9c37b5fc5dd7e2913352db22b47,True,"background,methodology",", 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill learning and planning (Ahn et al.,…architectures, recent works have used large-scale pretrained vision-language representations to improve performance for tabletop manipulation (Shridhar et al., 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill learning and planning (Ahn et al., 2022; Huang et al., 2022).,2a, these works generally treat the pretrained VL model as a semantic prior, for example, by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2022a).,Similar paradigms have also been applied to other manipulation environments such as Khandelwal et al. (2022) and Ahn et al. (2022)."
da3b810e10638507be9901e1b5cb94db126c8166,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","In robotics, this has translated to grounding of robotic affordances with natural language in models such as CLIPORT (Shridhar et al., 2022a).,Recently, CLIPort (Shridhar et al., 2022a) proposed to integrate pretrained vision-language (VL) models into such frameworks.,Finally, subsequent work may also consider jointly learning and leveraging pretrained representations that are more suitable for grounding interaction modes between
robots and objects, especially for manipulating difficult articulated and deformable objects in our 3D world (Shridhar et al., 2022b).,Our evaluation metric follows Shridhar et al. (2022a) based on the same CLIP encoders, where a dense score between 0 (failure) and 100 (success) represents the fraction of the episode completed correctly.,Our dataset is based off the Ravens benchmark proposed in Zeng et al. (2020), and its adaptation in Shridhar et al. (2022a).,…architectures, recent works have used large-scale pretrained vision-language representations to improve performance for tabletop manipulation (Shridhar et al., 2022a), visual navigation (Khandelwal et al., 2022), and real-world skill learning and planning (Ahn et al., 2022; Huang et al.,…,2a, these works generally treat the pretrained VL model as a semantic prior, for example, by partially initializing the weights of image and text encoders with a pretrained VL model (Ahn et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2022a).,Our dataset extends the benchmark proposed by CLIPORT (Shridhar et al., 2022a), in turn based on the Ravens benchmark from Zeng et al. (2020).,To evaluate zero-shot generalization, all of our main results in Table 1 use the “unseen” variants of the tasks, as described in Shridhar et al. (2022a).,Common tasks include visual question answering (Antol et al., 2015), VL navigation (Anderson et al., 2018), or VL manipulation (Shridhar et al., 2022a;b)."
da3b810e10638507be9901e1b5cb94db126c8166,3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2,True,methodology,"integrating embeddings from the CLIP VL model into the Transporter manipulation framework (Zeng et al., 2020), a robot can better generalize to manipulating new objects in a zero-shot manner.,Our dataset is based off the Ravens benchmark proposed in Zeng et al. (2020), and its adaptation in Shridhar et al. (2022a).,Our dataset extends the benchmark proposed by CLIPORT (Shridhar et al., 2022a), in turn based on the Ravens benchmark from Zeng et al. (2020).,Akin to Transporter Networks (Zeng et al., 2020), our policy π is composed of two functions, Qpick and Qplace, outputting dense pixelwise action values over RH×W , and operating in sequence.,For example, CLIPORT shows that by
integrating embeddings from the CLIP VL model into the Transporter manipulation framework (Zeng et al., 2020), a robot can better generalize to manipulating new objects in a zero-shot manner."
418085c9726669bf53f3d66e0018f2b08ffc4ce6,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","Thus previous works ( (Huang et al.; Ahn et al., 2022)) have explored using the model (which is LMT in our case) to score a step selected from a fixed set of available options, instead of directly sampling from the output distributions of the language model (which is LMG in our case).,, 2020a;b), which reasons about the next step from the options given, the task, and previous steps; 2) conditioned generation (Huang et al.; Ahn et al., 2022), which generates the plans composing of temporal-extended steps to implement the task.,Procedural Planning Learning to generate procedural plan (Zhang et al., 2020b; Chang et al., 2020; Huang et al.) is important for embodied agentTellex et al. (2011); Jansen (2020); Ahn et al. (2022) and conversational assistants (Ilievski et al., 2018; Yang et al., 2022).,Such ability is a necessity for a wide range of downstream applications, such as household robots (Ahn et al., 2022; Blukis et al., 2021; Min et al., 2022; Padmakumar et al., 2022; Shridhar et al., 2020) and conversational assistants (Budzianowski et al., 2018; Ding et al., 2022; Eric et al., 2017;…,Such ability is a necessity for a wide range of downstream applications, such as household robots (Ahn et al., 2022; Blukis et al., 2021; Min et al., 2022; Padmakumar et al., 2022; Shridhar et al., 2020) and conversational assistants (Budzianowski et al.,…(Lyu et al., 2021; Wu et al., 2022; Zhang et al., 2020a;b), which reasons about the next step from the options given, the task, and previous steps; 2) conditioned generation (Huang et al.; Ahn et al., 2022), which generates the plans composing of temporal-extended steps to implement the task."
aec546a0a4b32593c3338d9e2626c1c886105576,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"background,methodology","Our tentative plan is to show at least one system running on real hardware remotely – Inner Monologue [19] or Code as Policies [27], and solicit task instructions from the live audience.,In this work, we propose a demonstration of several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27] – providing an in-depth discussion on how they differ and build on each other.,, semantic parsing [30, 25, 44, 42, 38, 31, 45]), planning [18, 19, 1], and low-level policies (e.,This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27]."
aec546a0a4b32593c3338d9e2626c1c886105576,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","In this work, we propose a demonstration of several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27] – providing an in-depth discussion on how they differ and build on each other.,Language serves not only as an interface for non-experts to interact with robots [5, 25], but also as a means to compositionally scale generalization to new tasks [21, 1].,, semantic parsing [30, 25, 44, 42, 38, 31, 45]), planning [18, 19, 1], and low-level policies (e.,Above, an LLM planner [1, 18], CLIP-based object detector [17], and CLIPort policy [40].,This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27]."
aec546a0a4b32593c3338d9e2626c1c886105576,ada81a4de88a6ce474df2e2446ad11fea480616e,True,methodology,"In this work, we propose a demonstration of several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27] – providing an in-depth discussion on how they differ and build on each other.,Inner Monologue: LLM Planning with Feedback
SayCan and Socratic Models both do not use feedback to re-plan.,Rather than post-hoc grounding generated steps to skills, Socratic Models uses pretrained visual language models (VLMs) (e.g., open-vocabulary object detectors [23, 17]) to provide additional
context (e.g., text descriptions of images) to the LLM before generating the step-by-step plan.,Each step is formatted as a function call to a pre-existing skill (parameterized with text) that calls a language-conditioned policy such as CLIPort [40]:
# Move the coke can a bit to the right. objects = [“coke can”] from open-vocab VLMs Step 1. robot.grasp(“coke can”) Step 2. robot.place_a_bit_right()
One interpretation of Socratic Models is that it uses text as the information bottleneck between modules (perception and planning), as opposed to a Bayesian approach to multimodal probabilistic inference.,The demonstration of Socratic Models simulates a UR5 robot arm overlooking a set of objects (blocks and bowls) on a tabletop setting in PyBullet [12].,This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19], and Code as Policies [27].,Inner Monologue addresses this by expanding LLM step-by-step planning (SayCan) to include language-based feedback (Socratic Models), in the form of success detection or scene description, which can come from pretrained VLMs or human feedback:
Instruction: Move the drink a bit to the right."
aec546a0a4b32593c3338d9e2626c1c886105576,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","vised learning, though for this demonstration they are implemented in the real world via RT-1 [6] and in a simulator via CLIPort [40].,, model-based [33, 3, 39], imitation learning [21, 29, 40, 41], or reinforcement learning [22, 16, 10, 32, 2]).,Each step is formatted as a function call to a pre-existing skill (parameterized with text) that calls a language-conditioned policy such as CLIPort [40]:,Each step is formatted as a function call to a pre-existing skill (parameterized with text) that calls a language-conditioned policy such as CLIPort [40]:
# Move the coke can a bit to the right. objects = [“coke can”] from open-vocab VLMs Step 1. robot.grasp(“coke can”) Step 2. robot.place_a_bit_right()
One interpretation of Socratic Models is that it uses text as the information bottleneck between modules (perception and planning), as opposed to a Bayesian approach to multimodal probabilistic inference.,Above, an LLM planner [1, 18], CLIP-based object detector [17], and CLIPort policy [40].,It uses ViLD [17] to detect the objects in the scene, then passes that information as a list of object names to InstructGPT [34] as the LLM planner, which is few-shot prompted to generate function calls to CLIPort.,The plan is then executed by pretrained, natural-language conditioned policies that can either be trained with RL, or with supervised learning, though for this demonstration they are implemented in the real world via RT-1 [6] and in a simulator via CLIPort [40].,The function calls themselves map to a text template that matches the input training data distribution of CLIPort."
38939304bb760473141c2aca0305e44fbe04e6e8,38fe8f324d2162e63a967a9ac6648974fc4c66f3,True,"background,methodology","We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).,…visual recognition (Kirillov et al., 2023; Minderer et al., 2022; Radford et al., 2021) and can even make complex inferences about object-agent interactions in images (Alayrac et al., 2022; Chen et al., 2023a,b; Driess et al., 2023; Hao et al., 2022; Huang et al., 2023; Wang et al., 2022).,The two VLMs that we finetune in our experiments, PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023), use different tokenizations.,1The original pre-training data mixture used in PaLM-E-12B (as described in Driess et al. (2023)) includes robot images for high-level VQA planning tasks that can be similar to images encountered in generalization scenarios.,…encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023).,Such models can perform a wide range of visual interpretation and reasoning tasks, from inferring the composition of an image to answering questions about individual objects and their relations to other objects (Alayrac et al., 2022; Chen et al., 2023a; Driess et al., 2023; Huang et al., 2023).,, 2023a) and PaLM-E (Driess et al., 2023), use different tokenizations.,…trained to produce natural language tokens, we can train them on robotic trajectories by tokenizing the actions into text tokens and creating “multimodal sentences” (Driess et al., 2023) that “respond” to robotic instructions paired with camera observations by producing corresponding actions.,Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories by tokenizing the actions into text tokens and creating “multimodal sentences” (Driess et al., 2023) that “respond” to robotic instructions paired with camera observations by producing corresponding actions.,Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work.,The size of modern VLMs can reach tens or hundreds of billions of parameters (Chen et al., 2023a; Driess et al., 2023).,In this work, we adapt two previously proposed VLMs to act as VLA models: PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023).,In this work, we focus on the latter category (Alayrac et al., 2022; Chen et al., 2023a,b; Driess et al., 2023; Hao et al., 2022; Li et al., 2023, 2019; Lu et al., 2019).,, 2021) and can even make complex inferences about object-agent interactions in images (Alayrac et al., 2022; Chen et al., 2023a,b; Driess et al., 2023; Hao et al., 2022; Huang et al., 2023; Wang et al., 2022).,For training, we leverage the original web scale data from Chen et al. (2023a) and Driess et al. (2023), which consists of visual question answering, captioning, and unstructured interwoven image and text examples.,This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners (Ahn et al., 2022; Driess et al., 2023) can be combined with low-level policies in a single VLA model.,The vision-language models (Chen et al., 2023a; Driess et al., 2023) that we build on in this work take as input one or more images and produce a sequence of tokens, which conventionally represents natural language text.,Many other captioning and vision question answering datasets are included as well, and more info on the dataset mixtures can be found in Chen et al. (2023b) for RT-2-PaLI-X, and Driess et al. (2023) for RT-2-PaLM-E.,…visual state representations (Karamcheti et al., 2023), for identifying objects (Gadre et al., 2022; Stone et al., 2023), for high-level planning (Driess et al., 2023), or for providing supervision or success detection (Du et al., 2023b; Ma et al., 2023; Sumers et al., 2023; Xiao et al., 2022a;…,For all RT-2 training runs we adopt the hyperparameters from the original PaLI-X (Chen et al., 2023a) and PaLM-E (Driess et al., 2023) papers, including learning rate schedules and regularizations.,We perform co-fine-tuning on pre-trained models from the PaLI-X (Chen et al., 2023a) 5B & 55B model, PaLI (Chen et al., 2023b) 3B model and the PaLM-E (Driess et al., 2023) 12B model.,, 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023).,, 2023), for high-level planning (Driess et al., 2023), or for providing supervision or success detection (Du et al.,While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training.,The vision-language datasets are based on the dataset mixtures from Chen et al. (2023b) and Driess et al. (2023).,While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the…"
38939304bb760473141c2aca0305e44fbe04e6e8,0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,True,"background,methodology","2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen environments (Cui et al.,…(Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,
2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen environments (Cui et al., 2022; Du et al., 2023a; Hansen et al., 2020).,While CLIPort (Shridhar et al., 2021) and MOO (Stone et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability.,Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work.,To compare against other architectures for using VLMs, we use MOO (Stone et al., 2023), which uses a VLM to create an additional image channel for a semantic map, which is then fed into an RT-1 backbone.,, 2021) and MOO (Stone et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability.,These prior approaches use VLMs for visual state representations (Karamcheti et al., 2023), for identifying objects (Gadre et al., 2022; Stone et al., 2023), for high-level planning (Driess et al., 2023), or for providing supervision or success detection (Du et al., 2023b; Ma et al., 2023; Sumers…,, 2023), for identifying objects (Gadre et al., 2022; Stone et al., 2023), for high-level planning (Driess et al.,• MOO: MOO Stone et al. (2023) is an object-centric approach, where a VLM is first used to specify the object of interest in a form of a single, colored pixel in the original image."
38939304bb760473141c2aca0305e44fbe04e6e8,3396609b96dd24cac3b1542aec686ce362f32fe2,True,"background,methodology","Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work.,…ImageNet classification (Shah and Kumar, 2021), data augmentation (Kostrikov et al., 2020; Laskin et al., 2020a,b; Pari et al., 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023b; Nair et al., 2022b; Xiao et al., 2022b).,, 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023b; Nair et al., 2022b; Xiao et al., 2022b).,These prior approaches use VLMs for visual state representations (Karamcheti et al., 2023), for identifying objects (Gadre et al., 2022; Stone et al., 2023), for high-level planning (Driess et al., 2023), or for providing supervision or success detection (Du et al., 2023b; Ma et al., 2023; Sumers…,These prior approaches use VLMs for visual state representations (Karamcheti et al., 2023), for identifying objects (Gadre et al."
38939304bb760473141c2aca0305e44fbe04e6e8,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,"background,methodology","Further details can be found in Brohan et al. (2022).,Our experiments investigate models with up to 55B parameters trained on Internet data and instruction-annotated robotic trajectories from previous work (Brohan et al., 2022).,Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022;…,For the seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various…,…and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation (Anil et al., 2023; Brohan et al., 2022; OpenAI, 2023) but emergent problem-solving (Cobbe et al., 2021; Lewkowycz et al., 2022; Polu et al., 2022) and creative…,Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al.,We combine it with the robot demonstration data from Brohan et al. (2022), which was collected with 13 robots over 17 months in an office kitchen environment.,High-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation (Anil et al., 2023; Brohan et al., 2022; OpenAI, 2023) but emergent problem-solving (Cobbe et al.,, 2021) 72 ± 3 RT-1 (Brohan et al., 2022) 74 ± 13 LAVA (Lynch et al.,We instantiate VLA models by building on the protocol proposed for RT-1 (Brohan et al., 2022), using a similar dataset, but expanding the model to use a large vision-language backbone.,For the seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for picking out of and placing objects into drawers.,The robotics dataset is based on the dataset from Brohan et al. (2022).,To compare against a state-of-the-art policy, we use RT-1 (Brohan et al., 2022), a 35M parameter transformer-based model.,We base our action encoding on the discretization proposed by Brohan et al. (2022) for the RT-1 model.,• RT-1: Robotics Transformer 1 Brohan et al. (2022) is a transformer-based model that achieved state-of-the-art performance on a similar suite of tasks when it was published."
38939304bb760473141c2aca0305e44fbe04e6e8,0e34addae55a571d7efd3a5e2543e86dd7d41a83,True,"background,methodology","For the results on Language-Table in Table 1, our model is trained on the Language-Table datasets from Lynch et al. (2022).,, 2022) 74 ± 13 LAVA (Lynch et al., 2022) 77 ± 4 RT-2-PaLI-3B (ours) 90 ± 10,…model is able to correctly attend to the language instruction and move to the first correct object, it is not able to control the challenging dynamics of these objects, which are significantly different than the small set of block objects that have been seen in this environment Lynch et al. (2022).,To provide an additional point of comparison using open-source baselines and environments, we leverage the open-source Language-Table simulation environment from Lynch et al. (2022)."
38939304bb760473141c2aca0305e44fbe04e6e8,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,background,"…encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023).,This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners (Ahn et al., 2022; Driess et al., 2023) can be combined with low-level policies in a single VLA model.,, 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023).,While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training.,While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the…"
38939304bb760473141c2aca0305e44fbe04e6e8,c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,True,"background,methodology","Both VC-1 and R3M test different state-of-the-art representation learning methods as an alternative to using a VLM.,To obtain a language-conditioned policy from the R3M pretrained representation, we follow the same procedure as described above for VC-1, except we use the R3M ResNet50 model to obtain the image tokens, and unfreeze it during training.,To compare against state-of-the-art pretrained representations, we use VC-1 (Majumdar et al., 2023a) and R3M (Nair et al., 2022b), with policies implemented by training an RT-1 backbone to take their representations as input.,…ImageNet classification (Shah and Kumar, 2021), data augmentation (Kostrikov et al., 2020; Laskin et al., 2020a,b; Pari et al., 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023b; Nair et al., 2022b; Xiao et al., 2022b).,• R3M: R3M Nair et al. (2022b) is a similar method to VC-1 in that R3M uses pre-trained visual-language representations to improve policy training.,…often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023;…,…et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,
2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen…"
38939304bb760473141c2aca0305e44fbe04e6e8,1d803f07e4591bd67c358eef715bcd443e821894,True,background,"…skills (Dasari and Gupta, 2021; Finn et al., 2017; James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,
2019), to tasks with novel semantic…,Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022;…,, 2021), to tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017; James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al.,Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al.,Model Language-Table BC-Zero (Jang et al., 2021) 72 ± 3 RT-1 (Brohan et al.,…Young et al., 2021), to tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017; James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al.,…"
38939304bb760473141c2aca0305e44fbe04e6e8,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","While CLIPort (Shridhar et al., 2021) and MOO (Stone et al.,2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen environments (Cui et al.,…(Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,
2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen environments (Cui et al., 2022; Du et al., 2023a; Hansen et al., 2020).,While CLIPort (Shridhar et al., 2021) and MOO (Stone et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability.,Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work."
38939304bb760473141c2aca0305e44fbe04e6e8,20e6909ce6c5f12b61e5c9022d97134137360273,True,"background,methodology","Both VC-1 and R3M test different state-of-the-art representation learning methods as an alternative to using a VLM.,To obtain a language-conditioned policy from the R3M pretrained representation, we follow the same procedure as described above for VC-1, except we use the R3M ResNet50 model to obtain the image tokens, and unfreeze it during training.,To compare against state-of-the-art pretrained representations, we use VC-1 (Majumdar et al., 2023a) and R3M (Nair et al., 2022b), with policies implemented by training an RT-1 backbone to take their representations as input.,…ImageNet classification (Shah and Kumar, 2021), data augmentation (Kostrikov et al., 2020; Laskin et al., 2020a,b; Pari et al., 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023b; Nair et al., 2022b; Xiao et al., 2022b).,• R3M: R3M Nair et al. (2022b) is a similar method to VC-1 in that R3M uses pre-trained visual-language representations to improve policy training.,…often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022a; Shridhar et al., 2022b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023;…,…et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022a; Pong et al.,
2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and to unseen…"
2195676f111ad492c50f4d4c96abb2bd3d72f7fc,25425e299101b13ec2872417a14f961f4f8aa18e,True,"background,methodology","The results show that our method achieves comparable average performance with the state-of-the-art (SOTA) learning-based approach, VIMA [15], specifically designed for task applications in the multimodal instruction version.,For more details regarding the evaluation setting, please refer to [15].,Our framework provides an easy-to-use general-purpose robotic system and shows strong competitive performance on six representative meta-tasks from VIMABench [15].,We select several representative meta tasks from VIMABench [15] (17 tasks in total), ranging from simple object manipulation to visual reasoning to evaluate the proposed methods in the tabletop manipulation domain, as shown in Fig 5.,VIMA [15] developed a large-scale benchmarking dataset by designing a multimodal promptsconditioned framework.,Additionally, we directly used the experiment results of different baselines from [15]."
2195676f111ad492c50f4d4c96abb2bd3d72f7fc,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"background,methodology","CaP [21] directly generates policy codes with detailed comments and context-specific examples to guide LLM output.,Unlike existing methods such as CaP [21], which directly generates policy codes, we generate decision-making actions that can help reduce the error rate of executing complex tasks.,Inspired by the strong capability of synthesizing simple Python programs from docstrings of LLMs, CaP [21] directly generates the robot-centric policy code based on several incontext example language commands."
fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,5922f437512158970c417f4413bface021df5f78,True,"background,methodology","Specifically, we compare to the model architectures used by Gato [51] and BC-,Behavior Transformer [54] and Gato [51] advocate for training a single model on largescale robotic and non-robotic datasets.,Although recent years have seen several large multi-task robot policies proposed in the literature [51, 23], such models often have limited breadth of real-world tasks, as with Gato [51], or focus on training tasks rather than generalization to new tasks, as with recent instruction following methods [56, 57], or attain comparatively lower performance on new tasks [23].,Original SayCan [1]∗ 73 47 SayCan w/ Gato [51] 87 33 87 0 SayCan w/ BC-Z [23] 87 53 87 13 SayCan w/ RT-1 (ours) 87 67 87 67,Gato [51] 65 52 43 35 BC-Z [23] 72 19 47 41 BC-Z XL 56 43 23 35 RT-1 (ours) 97 76 83 59,Throughout this section we will compare to two baseline state of the art architectures, Gato [51] and BC-Z [23].,[51], we do not patchify the images into visual tokens prior to feeding them to our Transformer backbone.,We use a standard categorical cross-entropy entropy objective and causal masking that was utilized in prior Transformer-based controllers [51, 34] and a standard behavior,Gato [51] 30 63 25 0 BC-Z [23] 45 38 50 50 BC-Z XL 55 63 75 38 RT-1 (ours) 70 88 75 50"
fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","allowed us to execute very long-horizon tasks in the SayCan [1] framework, with as many as 50 steps.,skills, so the skill-breadth of RT-1 can be fully seen (for more details on the SayCan algorithm please refer to [1]).,To answer this question, we execute RT-1 and various baselines within the SayCan [1] framework in two different real kitchens.,These steps are obtained automatically from higher level instructions, such as “how would you throw away all the items on the table?” by using the SayCan system [1], as described in detail in Section IV-D and Appendix K.,This level of performance allows us to execute very longhorizon tasks in the SayCan [1] framework, with as many as 50 stages.,Original SayCan [1]∗ 73 47 SayCan w/ Gato [51] 87 33 87 0 SayCan w/ BC-Z [23] 87 53 87 13 SayCan w/ RT-1 (ours) 87 67 87 67,As in RT-1, several works use language commands processed with Transformers as a robust framework for specifying and generalizing to new tasks [72, 44, 58, 23, 1, 42].,to address robotic language understanding through pipelined approaches that combine language parsing, vision, and robotic control [40, 31, 62] and with end-to-end approaches [41, 60, 39, 1]."
fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,1d803f07e4591bd67c358eef715bcd443e821894,True,"background,methodology","Z [23], as well as a larger version of BC-Z, which we refer to as BC-Z XL.,Although recent years have seen several large multi-task robot policies proposed in the literature [51, 23], such models often have limited breadth of real-world tasks, as with Gato [51], or focus on training tasks rather than generalization to new tasks, as with recent instruction following methods [56, 57], or attain comparatively lower performance on new tasks [23].,Original SayCan [1]∗ 73 47 SayCan w/ Gato [51] 87 33 87 0 SayCan w/ BC-Z [23] 87 53 87 13 SayCan w/ RT-1 (ours) 87 67 87 67,Gato [51] 65 52 43 35 BC-Z [23] 72 19 47 41 BC-Z XL 56 43 23 35 RT-1 (ours) 97 76 83 59,Throughout this section we will compare to two baseline state of the art architectures, Gato [51] and BC-Z [23].,As in RT-1, several works use language commands processed with Transformers as a robust framework for specifying and generalizing to new tasks [72, 44, 58, 23, 1, 42].,End-to-end robotic learning, with either imitation or reinforcement, typically involves collecting task-specific data in either single-task [28, 71] or multi-task [30, 23] settings that are narrowly tailored to the tasks that the robot should perform.,Gato [51] 30 63 25 0 BC-Z [23] 45 38 50 50 BC-Z XL 55 63 75 38 RT-1 (ours) 70 88 75 50"
cb5e3f085caefd1f3d5e08637ab55d39e61234fc,1d803f07e4591bd67c358eef715bcd443e821894,True,"background,methodology","In our implementation, we train the individual skills either with image-based behavioral cloning, following the BC-Z method [13], or reinforcement learning, following MTOpt [14].,In BC NL we feed the full instruction i into the policy – this approach is representative of standard RL or BC-based instruction following methods [13, 20, 21, 22].,A large number of prior works have learned language-conditioned behavior via imitation learning [51, 22, 20, 13, 26, 37] or reinforcement learning [52, 53, 49, 54, 55, 56, 21, 41].,While prior works have studied how pre-trained language embeddings can improve generalization to new instructions [38, 22, 21] and to new low-level tasks [13], we extract much more substantial knowledge from LLMs by grounding them within the robot’s affordances.,To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z [13] and use a similar policy-network architecture (shown in Fig.,Most of these prior works focus on following low-level instructions, such as for pick-and-place tasks and other robotic manipulation primitives [20, 22, 56, 21, 13, 26], though some methods address long-horizon, compound tasks in simulated domains [57, 58, 54].,The BC models use an architecture similar to BC-Z [13] (see Fig."
cb5e3f085caefd1f3d5e08637ab55d39e61234fc,20e6909ce6c5f12b61e5c9022d97134137360273,True,"background,methodology","While prior works have studied how pre-trained language embeddings can improve generalization to new instructions (Hill et al., 2020; Lynch & Sermanet, 2020; Nair et al., 2021) and to new low-level tasks (Jang et al.,, 2021) or reinforcement learning (Misra et al., 2017; Hermann et al., 2017; Luketina et al., 2019; Jiang et al., 2019; Cideron et al., 2019; Goyal et al., 2020; Nair et al., 2021; Akakzia et al., 2020).,, 2022) and many other ways of combining language and interaction (Nair et al., 2021; Lynch & Sermanet, 2020; Hill et al., 2020; Wang et al., 2016) are exciting avenues for future research.,Note that this approach is representative of standard RL or BC-based instruction following methods (Jang et al., 2021; Stepputtis et al., 2020; Nair et al., 2021; Lynch & Sermanet, 2020), which train a policy conditioned on a natural language command.,Most of these prior works focus on following low-level instructions, such as for pick-and-place tasks and other robotic manipulation primitives (Stepputtis et al., 2020; Lynch & Sermanet, 2020; Goyal et al., 2020; Nair et al., 2021; Jang et al., 2021; Shridhar et al., 2022), though some methods address long-horizon, compound tasks in simulated domains (Oh et al."
cb5e3f085caefd1f3d5e08637ab55d39e61234fc,4f68e07c6c3173480053fd52391851d6f80d651b,True,methodology,"Our method, SayCan, extracts and leverages the knowledge within LLMs in physically-grounded tasks.,In this work, we utilize the vast semantic knowledge contained in LLMs to determine useful tasks for solving high-level instructions.,This tests the performance of SayCan’s affordance model and the LLM’s ability to reason within it.,Steps are output in the form “pick up the object and place it in location”, leveraging the ability of LLMs to output code structures.,This can lead LLMs to not only make mistakes that seem unreasonable or humorous to people, but also to interpret instructions in ways that are nonsensical or unsafe for a particular physical situation.,Recent breakthroughs initiated by neural network-based Attention architectures [2] have enabled efficient scaling of so-called Large Language Models (LLMs).,While prior works have studied how pre-trained language embeddings can improve generalization to new instructions [38, 22, 21] and to new low-level tasks [13], we extract much more substantial knowledge from LLMs by grounding them within the robot’s affordances.,Recent progress in training large language models (LLMs) has led to systems that can generate complex text based on prompts, answer questions, or even engage in dialogue on a wide range of topics.,Motivated by this example, we study the problem of how to extract the knowledge in LLMs for enabling an embodied agent, such as a robot, to follow high-level textual instructions.,SayCan leverages an LLM’s semantic knowledge about the world for interpreting instructions and understanding how to execute them.,First, we expect this method to inherit the limitations and biases of LLMs [82, 83], including the dependence on the training data.,Figure 1: LLMs have not interacted with their environment and observed the outcome of their responses, and thus are not grounded in the world.,The use of LLMs and generality of learned low-level policies enables long-horizon, abstract tasks that scale effectively to the real world, as demonstrated in our robot experiments.,Additionally, by evaluating the performance of the system with different LLMs, we show that a robot’s performance can be improved simply by enhancing the underlying language model.,But how can embodied agents extract and harness the knowledge of LLMs for physically grounded tasks?,SayCan grounds LLMs via value functions of pretrained skills, allowing them to execute real-world, abstract, long-horizon commands on robots.,LLMs are not grounded in the physical world and they do not observe the consequences of their generations on any physical process [1]."
cb5e3f085caefd1f3d5e08637ab55d39e61234fc,3e85d208b1b927fdb69ecf8336c70995818aaebd,True,methodology,"Given Qπ(s, a) with action a and state s, value v(s) = maxaQπ(s, a) is found through optimization via the cross entropy method, similar to MT-Opt.,In our implementation, we train the individual skills either with image-based behavioral cloning, following the BC-Z method [13], or reinforcement learning, following MTOpt [14].,The RL models use an architecture similar to MT-Opt [14], with slight changes to support natural language inputs (see Fig.,In order to learn a language-conditioned RL policy, we utilize MT-Opt [14] in the Everyday Robots simulator using said simulation-to-real transfer.,To learn a language-conditioned RL policy, we use MT-Opt [14] in the Everyday Robots simulator using RetinaGAN sim-to-real transfer [16].,We use a network architecture similar to MT-Opt (shown in Fig."
cb5e3f085caefd1f3d5e08637ab55d39e61234fc,8095bdd5861d1dbe43b77997bc0dbc2fd51acb93,True,"background,methodology","In BC NL we feed the full instruction i into the policy – this approach is representative of standard RL or BC-based instruction following methods [13, 20, 21, 22].,A large number of prior works have learned language-conditioned behavior via imitation learning [51, 22, 20, 13, 26, 37] or reinforcement learning [52, 53, 49, 54, 55, 56, 21, 41].,One approach to grounding language models in interaction is by learning downstream networks with pre-trained LLM representations [38, 22, 21, 39, 40, 41, 42, 43].,Ideas such as combining robot planning and language [85], using language models as a pre-training mechanism for policies [44] and many other ways of combining language and interaction [21, 22, 38, 86] are exciting avenues for future research.,While prior works have studied how pre-trained language embeddings can improve generalization to new instructions [38, 22, 21] and to new low-level tasks [13], we extract much more substantial knowledge from LLMs by grounding them within the robot’s affordances.,Most of these prior works focus on following low-level instructions, such as for pick-and-place tasks and other robotic manipulation primitives [20, 22, 56, 21, 13, 26], though some methods address long-horizon, compound tasks in simulated domains [57, 58, 54]."
ada81a4de88a6ce474df2e2446ad11fea480616e,f27e8c4731c575bd5f5db4c93ad8588f684dcbd0,True,methodology,"In the scenario of strict mem-
ory requirements, we propose to leverage recently introduced techniques on linear attention (Choromanski et al. 2021b) combined with modern continuous associative memory (MCAM) models (Ramsauer et al. 2021).,This approach relaxes the MIP-search to sampling from the linearized softmax distribution via FAVOR+ (Choromanski et al. 2021a).,A naive computation of such an energy still requires explicitly keeping all the patterns (which is exactly what we want to avoid), but this can be bypassed by applying the linearization of that energy (which effectively is just the negated sum of the softmax kernel values) with the FAVOR+ mechanism used in linear-attention Transformers, called Performers (Choromanski et al. 2021b).,…(which is exactly what we want to avoid), but this can be bypassed by applying the linearization of that energy (which effectively is just the negated sum of the softmax kernel values) with the FAVOR+ mechanism used in linear-attention Transformers, called Performers (Choromanski et al. 2021b)."
ada81a4de88a6ce474df2e2446ad11fea480616e,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"methodology,background",", models similar to CLIPort [83, 84] for open vocabulary pick-and-place).,[84] (inspired by CLIPort [83] for open vocabulary pick-and-place).,…directions for data-driven learning in robotics, where the various modules within a robot system (e.g., planning (Ahn et al. 2022; Huang et al. 2022), perception (Shridhar, Manuelli, and Fox 2022)) can be replaced with zero-shot foundation models imbued with commonsense priors across domains.,The SM robot system uses a VLM (open-vocabulary object detection with ViLD [65]) to describe the objects in the scene, feeds that description as context to a LM as a multi-step planner [7, 6], that then takes as input a natural language instruction and generates the individual steps to be passed to a pretrained language-conditioned robot policy, for which we specifically use a CLIP-conditioned [83] No-Transport baseline from Zeng et al.,, via CLIPort [83]) to enable robots to parse and generate plans from free-form human instructions (in magenta)."
ada81a4de88a6ce474df2e2446ad11fea480616e,4f68e07c6c3173480053fd52391851d6f80d651b,True,background,"Despite these limitations, large pretrained foundation models [4] are likely to serve as a backbone for many intelligent systems of the future – SMs is a systems approach (i.,, BERT [1], GPT-3 [2], CLIP [3]) have enabled impressive capabilities [4]: from zero-shot image classification [3, 5], to high-level planning [6, 7].,There are many options of large pretrained “foundation” [4] models to choose from, but our experiments in the main paper use models that are publicly available, so that our systems can be made accessible to the community.,These modules can either contain (i) large pretrained (“foundation” [4]) models, or (ii) APIs that interface with external capabilities or databases (e.,Foundation models (Bommasani et al. 2021) (e.g., BERT, GPT-3, CLIP) have enabled impressive capabilities in recent years: from zero-shot image classification (Radford et al. 2021; Li et al. 2021a), to high-level planning (Huang et al. 2022; Ahn et al. 2022)."
7f6d5c8f0d1da29f1a0f9dc3d32288bf6da8cacb,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","LLMs have been showing incredible potential in areas besides robotics [1], [7], [15], [16].,An issue of grounding LLMs on robotic scenarios is that some of the suggestions generated by LLMs are not executable for a specific robot [1], [10], which stems from the fact that LLMs are pre-trained with extremely large opendomain corpora, while the robot is constrained by its physical capability and application scenarios, e.,This allows their integration with a robot to not only plan with respect to a robot’s built-in ability [27], [1] but also respond according to environmental feedback.,They use LLMs as a planner to autoregressively select actions that are appropriate with respect to the instruction according to application-based prompts [27], the semantic similarity between mapped pairs [10], or the contextual language score grounded on realistic robot affordances [1]."
7f6d5c8f0d1da29f1a0f9dc3d32288bf6da8cacb,ada81a4de88a6ce474df2e2446ad11fea480616e,True,"background,methodology","In this work, the LLM is applied for few-shot planning [16], [27], in which all the executable commands are defined together with several task examples as the initial “chat” history.,other various fields where domain knowledge is distinct and modular frameworks can be composed via language as the intermediate representation [18], [16], [27].,This allows their integration with a robot to not only plan with respect to a robot’s built-in ability [27], [1] but also respond according to environmental feedback.,They use LLMs as a planner to autoregressively select actions that are appropriate with respect to the instruction according to application-based prompts [27], the semantic similarity between mapped pairs [10], or the contextual language score grounded on realistic robot affordances [1]."
fdb03aa9c310fa61df0be724705fb6f4ab20d37e,acf0e19813341729c1b5190333f3d88e8d4f352f,True,"background,result,methodology","• MANNERS-DB [23], a recent dataset for assessing the appropriateness of robot actions in specific social contexts.,We use two HRI-related datasets: • MANNERS-DB [23], a recent dataset for assessing
the appropriateness of robot actions in specific social contexts.,We also observe lower performance on ‘Carry big objects’ and ‘Starting a conversation’, which are actions where social appropriateness is highly correlated with interagent distances [23].,• BNN-16CL4 [23], a Bayesian continual learning model trained on the MANNERS-DB dataset to predict social appropriateness of robot actions.,For each dataset, we show an illustrative images of tasks (reproduced from [23], [25], [26], respectively) and an example prompt.,The 2-Wasserstein metric captures how similar two distributions are; it measures how much probability mass has to be moved to transform q to p.
4We ran BNN-16CL using code provided by the authors at https:// github.com/jonastjoms/MANNERS-DB.,To produce a consistent score across the different datasets , we first binarized the targets; for MANNERS-DB, if a majority of annotators rated the appropriateness level of a robot action as ≥ 3 (neutral), we labelled it as socially acceptable (otherwise, it was labelled as not socially acceptable).,We use two HRI datasets: MANNERS-DB [23] and TrustTransfer [24], [25], and included SocialIQA [26], a general social reasoning benchmark for human interactions.,The error scores are RMSE, MAE, and error rate for MANNERS-DB, Trust-Transfer and SocialIQA, respectively.,obtained for ‘Vacuum cleaning’ and ‘Mopping the floor’, which are the two most intrusive actions in the dataset [23] and require an understanding of personal space.,T5 also achieves good scores on the MANNERS-DB and SocialIQA datasets but performs poorly on the Trust-Transfer datasets; we delve into the reasons for this subpar performance in later analysis.,We compared the LLMs to specialized published models created for each of the datasets: • BNN-16CL4 [23], a Bayesian continual learning model
trained on the MANNERS-DB dataset to predict social appropriateness of robot actions.,For both MANNERS-DB and SocialIQA, the labels are provided by third-party humans who are not actually part of the environment.,The CwM scores indicate that the LLMs achieve humanlevel consistency to the majority label on the MANNERS-DB and SocialIQA datasets.,Our reported numbers are different from [23] possibly due to a difference in the test dataset split.,Note that these two distribution-related measures could only be computed for MANNERS-DB (since Trust-Transfer and SocialIQA only contain one label per instance)."
326f6a8011e43322c433751b9cc31fd56564621c,f02577b75226e60440a506ca79f40d8adfb533f3,True,"background,methodology","When and how to adapt PVRs for downstream applications remains an open research question (Kumar et al., 2022; Wijmans et al., 2022; Kirichenko et al., 2022; Lee et al., 2022; Goyal et al., 2022).,For ImageNav and the Habitat 2.0 Mobile-Pick task, we use RL for 500M environment steps with DD-PPO (Wijmans et al., 2020) and VER (Wijmans et al., 2022).,When and how to adapt PVRs for downstream applications remains an open research question (Kumar et al., 2022; Wijmans et al., 2022; Kirichenko et al.,
2022; Lee et al., 2022; Goyal et al., 2022)."
326f6a8011e43322c433751b9cc31fd56564621c,979810ca765695a481c37126103b8ba256ee2192,True,"background,result,methodology","…on large quantities of egocentric-videos and web-images can substantially improve performance and learning efficiency for navigation (Khandelwal et al., 2022; Yadav et al., 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,, 2021) was pretrained on 400M image-text pairs from the web; MVP (Radosavovic et al., 2022) on 4.,Closely related, Radosavovic et al. (2022) demonstrate that MAE pre-training on internet-scale video and image data can produce effective visual representations for robotic manipulation tasks.,sists of videos showcasing people manipulating objects and are comparable to the datasets used in MVP (Radosavovic et al., 2022).,MAE is selected for these experiments due to the strong performance on CORTEXBENCH of the MVP (Radosavovic et al., 2022) models (Table 2), which use the MAE pre-training objective.,– MVP (Radosavovic et al., 2022).,These design choices follow prior work such as Radosavovic et al. (2022); Nair et al. (2022).,, 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,Inspired by the advancements in self-supervised learning, recent work has incorporated visual representation learning into the training pipelines for EAI agents (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022; Khandelwal et al., 2022; Yadav et al., 2022b; 2023).,In EAI, Radosavovic et al. (2022) find that scaling model and data sizes improves downstream policy performances for robotic manipulation tasks.,In the context of EAI, Parisi et al. (2022) and Hansen et al. (2022b) show that naively fine-tuning PVRs with behavior cloning can reduce performance in simulation, and Radosavovic et al. (2022) observe minimal gains in real-world tasks manipulation tasks.,This subset con-
sists of videos showcasing people manipulating objects and are comparable to the datasets used in MVP (Radosavovic et al., 2022).,…significantly in the size and type of pre-training datasets: CLIP (Radford et al., 2021) was pretrained on 400M image-text pairs from the web; MVP (Radosavovic et al., 2022) on 4.5M frames from web-images and many egocentric-video datasets; R3M (Nair et al., 2022) on ∼5M frames from Ego4D –…,Nair et al. (2022); Radosavovic et al. (2022); Ma et al. (2022) introduce new methods for pretraining visual representations using egocentric video data, targeting robotic manipulation tasks."
326f6a8011e43322c433751b9cc31fd56564621c,3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3,True,"background,methodology","…on large quantities of egocentric-videos and web-images can substantially improve performance and learning efficiency for navigation (Khandelwal et al., 2022; Yadav et al., 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,, 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,Inspired by the advancements in self-supervised learning, recent work has incorporated visual representation learning into the training pipelines for EAI agents (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022; Khandelwal et al., 2022; Yadav et al., 2022b; 2023).,Nair et al. (2022); Radosavovic et al. (2022); Ma et al. (2022) introduce new methods for pretraining visual representations using egocentric video data, targeting robotic manipulation tasks.,– VIP (Ma et al., 2022)."
326f6a8011e43322c433751b9cc31fd56564621c,c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,True,"background,methodology","…on large quantities of egocentric-videos and web-images can substantially improve performance and learning efficiency for navigation (Khandelwal et al., 2022; Yadav et al., 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,Best prior results sources (row 1): Adroit and MetaWorld approximated from (Nair et al., 2022), DMControl from (Parisi et al.,“MuJoCo Tasks” On the tasks from the Adroit, MetaWorld, and DMC suites we train policies using behavior cloning on a small number of expert demonstrations (100 for Adroit and DMC and 25 for MetaWorld), which follows Parisi et al. (2022); Nair et al. (2022).,– R3M (Nair et al., 2022) Time-Contrastive video-language alignment pre-training objective; Trains on 5M images from a subset of Ego4D; ResNet-50 backbone.,5M frames from web-images and many egocentric-video datasets; R3M (Nair et al., 2022) on ∼5M frames from Ego4D – yet, each performs best on some subset of tasks in CORTEXBENCH.,These design choices follow prior work such as Radosavovic et al. (2022); Nair et al. (2022).,, 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022).,Inspired by the advancements in self-supervised learning, recent work has incorporated visual representation learning into the training pipelines for EAI agents (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022; Khandelwal et al., 2022; Yadav et al., 2022b; 2023).,We consider five tasks from MetaWorld: Assembly, Bin-Picking, Button-Press, Drawer-Open, and Hammer, which follows the evaluations performed in (Nair et al., 2022).,Nair et al. (2022); Radosavovic et al. (2022); Ma et al. (2022) introduce new methods for pretraining visual representations using egocentric video data, targeting robotic manipulation tasks.,…CLIP (Radford et al., 2021) was pretrained on 400M image-text pairs from the web; MVP (Radosavovic et al., 2022) on 4.5M frames from web-images and many egocentric-video datasets; R3M (Nair et al., 2022) on ∼5M frames from Ego4D – yet, each performs best on some subset of tasks in CORTEXBENCH."
326f6a8011e43322c433751b9cc31fd56564621c,826383e18568c9c37b5fc5dd7e2913352db22b47,True,"background,methodology","Similarly, Khandelwal et al. (2022); Yadav et al. (2022b; 2023) use pre-trained visual representations to improve performance on multiple visual navigation tasks.,Inspired by the advancements in self-supervised learning, recent work has incorporated visual representation learning into the training pipelines for EAI agents (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et al., 2022; Khandelwal et al., 2022; Yadav et al., 2022b; 2023).,1 Indeed, recent work has shown that PVRs trained on large quantities of egocentric-videos and web-images can substantially improve performance and learning efficiency for navigation (Khandelwal et al., 2022; Yadav et al., 2022b; 2023) and manipulation tasks (Parisi et al.,…PVRs trained on large quantities of egocentric-videos and web-images can substantially improve performance and learning efficiency for navigation (Khandelwal et al., 2022; Yadav et al., 2022b; 2023) and manipulation tasks (Parisi et al., 2022; Nair et al., 2022; Radosavovic et al., 2022; Ma et…"
2ebd5df74980a37370b0bcdf16deff958289c041,390c71025beb7e7613640ecd331fa9a1179ca568,True,"methodology,background","For conditional generative modeling of behavior, language goals [Du et al. 2023b], image goals [Brohan et al. 2022], returns [Lee et al. 2022], environment constraints [Ajay et al. 2022], and expert demonstrations [Reed et al. 2022] have all been explored as s conditioning factor for finetuning or…,UniPi [Du et al. 2023b] directly learns to predict robotic videos and trains a separate inverse model to infer actions from generated videos.,Finally, one can use image frames as a universal interface to represent state action spaces, and use videos to represent policies [Du et al. 2023b].,Instead of using text as states and actions, one can also use text descriptions to specify tasks (rewards) [Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Du et al. 2023b], avoiding the difficulties around reward shaping.,When agent observations consist of both images and text descriptions, vision-language captioning models can further enrich agent
14
observations with language descriptions [Tam et al. 2022; Du et al. 2023a; Driess et al. 2023].,Going beyond low dimensional state action spaces, [Du et al. 2023b] also show that diffusion models of long-term futures can also be applied to high-dimensional video data 𝜏 , using 𝑧 (𝜏) as text descriptions, effectively improving decision making with large-pretrained text-video foundation…,observations with language descriptions [Tam et al. 2022; Du et al. 2023a; Driess et al. 2023]."
2ebd5df74980a37370b0bcdf16deff958289c041,da2fe6cd385194b0274d04d04ee72e8caf3854d4,True,"methodology,background","For conditional generative modeling of behavior, language goals [Du et al. 2023b], image goals [Brohan et al. 2022], returns [Lee et al. 2022], environment constraints [Ajay et al. 2022], and expert demonstrations [Reed et al. 2022] have all been explored as s conditioning factor for finetuning or…,UniPi [Du et al. 2023b] directly learns to predict robotic videos and trains a separate inverse model to infer actions from generated videos.,long-term future (UniPi [Du et al. 2023b]) generally require data with good coverage.,Finally, one can use image frames as a universal interface to represent state action spaces, and use videos to represent policies [Du et al. 2023b].,Instead of using text as states and actions, one can also use text descriptions to specify tasks (rewards) [Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Du et al. 2023b], avoiding the difficulties around reward shaping.,When agent observations consist of both images and text descriptions, vision-language captioning models can further enrich agent
14
observations with language descriptions [Tam et al. 2022; Du et al. 2023a; Driess et al. 2023].,Going beyond low dimensional state action spaces, [Du et al. 2023b] also show that diffusion models of long-term futures can also be applied to high-dimensional video data τ , using z (τ) as text descriptions, effectively improving decision making with large-pretrained text-video foundation models.,Going beyond low dimensional state action spaces, [Du et al. 2023b] also show that diffusion models of long-term futures can also be applied to high-dimensional video data 𝜏 , using 𝑧 (𝜏) as text descriptions, effectively improving decision making with large-pretrained text-video foundation…,For conditional generative modeling of behavior, language goals [Du et al. 2023b], image goals [Brohan et al."
2ebd5df74980a37370b0bcdf16deff958289c041,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,"methodology,background","For conditional generative modeling of behavior, language goals [Du et al. 2023b], image goals [Brohan et al. 2022], returns [Lee et al. 2022], environment constraints [Ajay et al. 2022], and expert demonstrations [Reed et al. 2022] have all been explored as s conditioning factor for finetuning or…,Conversely, in the sequential decision making communities, researchers inspired by the success of large scale vision and language models have begun to curate ever-larger datasets for learning multimodel, multitask, and generalist interactive agents [Agarwal et al. 2020b; Szot et al. 2021; Fan et al. 2022; Brohan et al. 2022; Reed et al. 2022; Lee et al. 2022].,2023b], image goals [Brohan et al. 2022], returns [Lee et al.,Instead of using text as states and actions, one can also use text descriptions to specify tasks (rewards) [Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Du et al. 2023b], avoiding the difficulties around reward shaping.,Such dependence is less common in Markovian environments, but has shown empirical benefits [Brohan et al. 2022].,2022], over 700 real-world robot tasks [Brohan et al. 2022], and over 600 distinct tasks with varying modalities, observations and action specifications [Reed et al.,…of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al. 2022], over 700 real-world robot tasks [Brohan et al. 2022], and over 600 distinct tasks with varying modalities, observations and action specifications [Reed et al. 2022].,2022a], control [Brohan et al. 2022], search [Strohman et al.,• RT-1 [Brohan et al. 2022] Robotics Transformer for Real-World Control at Scale (to be released).,…inspired by the success of large scale vision and language models have begun to curate ever-larger datasets for learning multimodel, multitask, and generalist interactive agents [Agarwal et al. 2020b; Szot et al. 2021; Fan et al. 2022; Brohan et al. 2022; Reed et al. 2022; Lee et al. 2022].,As suchmodels continue to be applied tomore complex problems that involve long-term reasoning [Wei et al. 2022a], control [Brohan et al. 2022], search [Strohman et al. 2005], and planning [Huang et al. 2022b], or are deployed in applications such as dialogue, autonomous driving, healthcare, and…,An important special case of plug-and-play foundation models is to use text commands or visual inputs as task specifiers to learn more robust, general, and multi-task policies[Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Liu et al. 2022a]."
2ebd5df74980a37370b0bcdf16deff958289c041,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"methodology,background","Instead of using text as states and actions, one can also use text descriptions to specify tasks (rewards) [Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Du et al. 2023b], avoiding the difficulties around reward shaping.,…pick up the cup”), pretrained language models can be used to generate higher-level plans for longer-horizon tasks, with the hope that language based descriptions of actions generalize better than low-level motor controls [Huang et al. 2022a; Ahn et al. 2022; Wang et al. 2023; Driess et al. 2023].,Second, pretrained language models (equipped with prompting methods such as chain-of-thought) can decompose high-level tasks into lower-level instructions that are easier to execute [Ahn et al. 2022; Huang et al. 2022a; Jiang et al. 2022; Team et al. 2021].,…tasks [Lynch and Sermanet 2020; Hill et al. 2020; Hao et al. 2020; Majumdar et al. 2020; Nair et al. 2022; Jang et al. 2022a; Ahn et al. 2022; Huang et al. 2022a; Khandelwal et al. 2022; Shridhar et al. 2022; Guhur et al. 2022; Shah et al. 2022], which has been a key challenge in robotics…,Visionlanguage models such as CLIP and PaLI [Chen et al. 2022a] are further able to provide task feedback and reward information by aligning image and language modalities in the agent’s observation and goal space [Huang et al. 2022a; Mahmoudieh et al. 2022; Fan et al. 2022].,…complex problems that involve long-term reasoning [Wei et al. 2022a], control [Brohan et al. 2022], search [Strohman et al. 2005], and planning [Huang et al. 2022b], or are deployed in applications such as dialogue, autonomous driving, healthcare, and robotics, they are expected to interface…,An important special case of plug-and-play foundation models is to use text commands or visual inputs as task specifiers to learn more robust, general, and multi-task policies[Ahn et al. 2022; Huang et al. 2022a; Brohan et al. 2022; Liu et al. 2022a]."
2ebd5df74980a37370b0bcdf16deff958289c041,01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8,True,methodology,"Similarly, the autoregressive sequence modeling objective from Equation 11 can also be instantiated to model behavioral priors [Shafiullah et al. 2022], resulting in a policy that can depend on the history of interaction π (at |st , τ<t ).,Similarly, the autoregressive sequence modeling objective from Equation 11 can also be instantiated to model behavioral priors [Shafiullah et al. 2022], resulting in a policy that can depend on the history of interaction 𝜋 (𝑎𝑡 |𝑠𝑡 , 𝜏<𝑡 ).,Inspired by the scaling success of transformers, generalist agents modeling sequences of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al. 2022], over 700 real-world robot tasks [Brohan et al. 2022], and over 600 distinct tasks with…,Inspired by the scaling success of transformers, generalist agents modeling sequences of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al."
2ebd5df74980a37370b0bcdf16deff958289c041,5922f437512158970c417f4413bface021df5f78,True,background,"2022], and expert demonstrations [Reed et al. 2022] have all been explored as s conditioning factor for finetuning or prompting schemes, so that the models can be “controlled”.,Conversely, in the sequential decision making communities, researchers inspired by the success of large scale vision and language models have begun to curate ever-larger datasets for learning multimodel, multitask, and generalist interactive agents [Agarwal et al. 2020b; Szot et al. 2021; Fan et al. 2022; Brohan et al. 2022; Reed et al. 2022; Lee et al. 2022].,…of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al. 2022], over 700 real-world robot tasks [Brohan et al. 2022], and over 600 distinct tasks with varying modalities, observations and action specifications [Reed et al. 2022].,…goals [Du et al. 2023b], image goals [Brohan et al. 2022], returns [Lee et al. 2022], environment constraints [Ajay et al. 2022], and expert demonstrations [Reed et al. 2022] have all been explored as s conditioning factor for finetuning or prompting schemes, so that the models can be “controlled”.,Meanwhile, decision making datasets can be made more broad and general (DRL → D) by combining a wide range of tasks-specific datasets (e.g., Gato).,For instance, Gato [Reed et al. 2022] approaches this issue with universal tokenization, so that data with and without actions can be jointly trained using large sequence models.,…inspired by the success of large scale vision and language models have begun to curate ever-larger datasets for learning multimodel, multitask, and generalist interactive agents [Agarwal et al. 2020b; Szot et al. 2021; Fan et al. 2022; Brohan et al. 2022; Reed et al. 2022; Lee et al. 2022].,2022], and over 600 distinct tasks with varying modalities, observations and action specifications [Reed et al. 2022]."
2ebd5df74980a37370b0bcdf16deff958289c041,ada81a4de88a6ce474df2e2446ad11fea480616e,True,background,"…limitation of existing works is that they either require finetuning [Alayrac et al. 2022] or defined interfaces within which models can communicate [Zeng et al. 2022; Li et al. 2022a], which prevents novel combinations of foundation models from being easily composed at test-time in a free-form…,2022] or defined interfaces within which models can communicate [Zeng et al. 2022; Li et al. 2022a], which prevents novel combinations of foundation models from being easily composed at test-time in a free-form manner.,Alternatively, language can be used as a ubiquitous interface in which separate foundation models can communicate [Zeng et al. 2022]."
2ebd5df74980a37370b0bcdf16deff958289c041,6bae20930eaa0d9d489317f3b3b1aaaf18205ef8,True,background,"Pretrain
Interact
Feedback
Broad Datasets
Foundation Models
External Entity
ar X
iv :2
30 3.,• Something-Something V2 Dataset [Goyal et al. 2017] contains 220k short videos of people performing various tasks with everyday objects, such as putting on a hat and opening a bottle.,A I]
7 M
ar 2
02 3
2 Contents
Contents 2 1 Introduction 3 1.1 Structure of This Report 4 2 Preliminaries 4 2.1 Sequential Decision Making Preliminaries 4 2.2 Example Scenarios 7 3 Foundation Models as Conditional Generative Models 8 3.1 Generative Model Preliminaries 8 3.2 Generative Models of Behavior 9 3.3 Generative Models of the World 12 4 Foundation Models as Representation Learners 13 4.1 Plug-and-Play 13 4.2 Vision and Language as Task Specifiers 14 4.3 Learning Representations for Sequential Decision Making 14 5 Large Language Models as Agents and Environments 17 5.1 Interacting with Humans 17 5.2 Interacting with Tools 18 5.3 Language Models as Environments 18 6 Open Problems, Challenges, and Opportunities 19 6.1 How to Leverage or Collect Datasets 19 6.2 How to Structure Environments and Tasks 20 6.3 Improving Foundation Models 21 6.4 Improving Decision Making 22 7 Discussion and Perspectives 22 Acknowledgments 23 References 23
Foundation Models for Decision Making: Problems, Methods, and Opportunities 3,• Bridge Data [Ebert et al. 2021] contains 7,200 text-video demonstrations of a 6-dof WidowX250s robot arm performing 71 tasks across 10 kitchen-themed environments."
d6811adf94d85108b30da82a851ad364823fb9db,b75359b5b22024ac0aec8b942bbd86bde81f8e70,True,"methodology,background","Specifically, we consider the PolicyCEM variant of STAP, where sampling-based optimization of the skill sequence’s π1:h success probability is warm started with actions sampled from the policies a1:h ∼ π1:h.,Thus, upon executing the skill πh at timestep h and receiving environment feedback sh+1, STAP [1] is called to perform policy sequence optimization on the remaining planned skills πh+1:H .,We propose Text2Motion, a language-based planning framework that interfaces an LLM with a library of learned skill policies and a policy sequence optimizer [1] to solve geometrically complex sequential manipulation tasks (Fig.,Lastly, these methods ignore the uncertainty of skill feasibility predictions, which [1] demonstrates is important for using skills to solve geometrically complex TAMP problems.,Instead, Text2Motion constructs plans of skills and coordinates their geometric dependencies through policy sequence optimization [1].,In the experiments, we leverage STAP [1].,However, in our experiments we leverage Sequencing Task-Agnostic Policies (STAP) [1].,8) in the context of the full skill sequence π1:h by the Q-value of the last action,
p(rh = 1 | s1, π1:h) ≈ Qπh(sh, a∗h), (11)
where a∗h is determined by STAP and sh is predicted by the dynamics model.,Nonetheless, runtime complexity was not a core focus of this work, and expensive optimization subroutines [1] were frequently called."
d6811adf94d85108b30da82a851ad364823fb9db,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"methodology,background","6), comparing our plan termination method to that of SayCan and Inner Monologue’s, while keeping all else constant for our integrated planner.,Closest in spirit to our work are SayCan [2] and Inner Monologue (IM) [26] which at each timestep score the usefulness and feasibility of all possible skills and execute the one with the highest score.,We acquire INNERMONO-GS by equipping [26] with the Generator-Scorer for cost efficiency.,Related work [2, 26] immediately executes skills that are deemed useful and feasible at the current timestep.,Several recent works [2, 26, 38] have capitalized on their ability to perform planning for robot systems without needing to manually specify symbolic planning domains.,INNERMONO-GS: We implement the Object + Scene variant of Inner Monologue [26] by providing task-progress scene context in the form of the environment’s symbolic state."
d6811adf94d85108b30da82a851ad364823fb9db,0ab3f612db15a5a986d731283ca52e08058c9c44,True,background,"While learning symbolic representations has been proposed for TAMP [34, 3, 33, 49, 11, 10, 50], these approaches often require task-specific symbolic transition experience.,I. INTRODUCTION
Task and Motion Planning (TAMP) refers to a problem setting in which a robot has to solve long-horizon tasks that require both symbolic and geometric reasoning [21].,We conduct experiments to test the following hypotheses: H1 Policy sequence optimization is a necessary ingre-
dient for solving TAMP-like problems specified by natural language with LLMs and robot skills.,While different TAMP methods have relaxed the dependence on these components, many prominent works require all three [32, 36, 53, 12, 4, 20].,Text2Motion is inspired by traditional TAMP methods, but
ar X
iv :2
30 3.,We follow a modular approach similar to traditional TAMP methods but replace the commonly used symbolic task planner with an LLM.,Hence, the process of iterating between a) and b) may take on the order of minutes until the TAMP solver returns a solution to a sufficiently complex task or indicates that none exists.,While details differ across TAMP methods, three core components are common [21].,Lastly, these methods ignore the uncertainty of skill feasibility predictions, which [1] demonstrates is important for using skills to solve geometrically complex TAMP problems.,A. Planning is required to solve TAMP-like tasks (H1) Our first hypothesis is that performing policy sequence optimization on task plans output by the LLM is essential to task success.,However, these methods learn from solutions computed by classical TAMP solvers, and thus, they also depend on meticulously hand-crafted and task-specific symbolic planning domains.,Our results highlight the importance of policy sequence optimization when solving TAMP-like tasks from a natural language instruction, that integrated planning is equipped to solve a family of semantically partially observable problems, and that plan termination via inferred symbolic constraints is more reliable than prior LLM scoring techniques.,Another line of works accelerates TAMP by learning sampling distributions [54, 56], visual feasibility heuristics [14, 15, 17], low-level controllers [16, 50], or state sparsifiers [9, 48]."
d6811adf94d85108b30da82a851ad364823fb9db,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,background,"By addressing these limitations, Text2Motion can outperform SayCan and IM on geometrically complex tasks, as demonstrated in the experiments.,6), comparing our plan termination method to that of SayCan and Inner Monologue’s, while keeping all else constant for our integrated planner.,SAYCAN-GS: We implement a cost-considerate variant of SayCan with a module dubbed Generator-Scorer (GS).,Closest in spirit to our work are SayCan [2] and Inner Monologue (IM) [26] which at each timestep score the usefulness and feasibility of all possible skills and execute the one with the highest score.,At each timestep h, vanilla SayCan [2] ranks all possible actions by p(πh | i, π1:h−1)× V πh(sh), before executing the top scoring action (Scorer).,While the generality of SayCan and IM is shown over a diverse range of tasks, there are several drawbacks that impede their performance in the settings we study.,At each timestep h, vanilla SayCan [2] ranks all possible actions by p(πh | i, π1:h−1)× V h(sh), before executing the top scoring action (Scorer).,Related work [2, 26] immediately executes skills that are deemed useful and feasible at the current timestep.,Several recent works [2, 26, 38] have capitalized on their ability to perform planning for robot systems without needing to manually specify symbolic planning domains."
d6811adf94d85108b30da82a851ad364823fb9db,5185759e89a8e52ee1184cbebba5917371673790,True,background,"While learning symbolic representations has been proposed for TAMP [34, 3, 33, 49, 11, 10, 50], these approaches often require task-specific symbolic transition experience.,I. INTRODUCTION
Task and Motion Planning (TAMP) refers to a problem setting in which a robot has to solve long-horizon tasks that require both symbolic and geometric reasoning [21].,We conduct experiments to test the following hypotheses: H1 Policy sequence optimization is a necessary ingre-
dient for solving TAMP-like problems specified by natural language with LLMs and robot skills.,While different TAMP methods have relaxed the dependence on these components, many prominent works require all three [32, 36, 53, 12, 4, 20].,Text2Motion is inspired by traditional TAMP methods, but
ar X
iv :2
30 3.,We follow a modular approach similar to traditional TAMP methods but replace the commonly used symbolic task planner with an LLM.,Hence, the process of iterating between a) and b) may take on the order of minutes until the TAMP solver returns a solution to a sufficiently complex task or indicates that none exists.,While details differ across TAMP methods, three core components are common [21].,Lastly, these methods ignore the uncertainty of skill feasibility predictions, which [1] demonstrates is important for using skills to solve geometrically complex TAMP problems.,A. Planning is required to solve TAMP-like tasks (H1) Our first hypothesis is that performing policy sequence optimization on task plans output by the LLM is essential to task success.,However, these methods learn from solutions computed by classical TAMP solvers, and thus, they also depend on meticulously hand-crafted and task-specific symbolic planning domains.,Our results highlight the importance of policy sequence optimization when solving TAMP-like tasks from a natural language instruction, that integrated planning is equipped to solve a family of semantically partially observable problems, and that plan termination via inferred symbolic constraints is more reliable than prior LLM scoring techniques.,Another line of works accelerates TAMP by learning sampling distributions [54, 56], visual feasibility heuristics [14, 15, 17], low-level controllers [16, 50], or state sparsifiers [9, 48]."
d6811adf94d85108b30da82a851ad364823fb9db,01bc9970cdceafe7a81fcd9acc95b82d6666c502,True,background,"While learning symbolic representations has been proposed for TAMP [34, 3, 33, 49, 11, 10, 50], these approaches often require task-specific symbolic transition experience.,I. INTRODUCTION
Task and Motion Planning (TAMP) refers to a problem setting in which a robot has to solve long-horizon tasks that require both symbolic and geometric reasoning [21].,We conduct experiments to test the following hypotheses: H1 Policy sequence optimization is a necessary ingre-
dient for solving TAMP-like problems specified by natural language with LLMs and robot skills.,While different TAMP methods have relaxed the dependence on these components, many prominent works require all three [32, 36, 53, 12, 4, 20].,Text2Motion is inspired by traditional TAMP methods, but
ar X
iv :2
30 3.,We follow a modular approach similar to traditional TAMP methods but replace the commonly used symbolic task planner with an LLM.,Hence, the process of iterating between a) and b) may take on the order of minutes until the TAMP solver returns a solution to a sufficiently complex task or indicates that none exists.,While details differ across TAMP methods, three core components are common [21].,Lastly, these methods ignore the uncertainty of skill feasibility predictions, which [1] demonstrates is important for using skills to solve geometrically complex TAMP problems.,A. Planning is required to solve TAMP-like tasks (H1) Our first hypothesis is that performing policy sequence optimization on task plans output by the LLM is essential to task success.,However, these methods learn from solutions computed by classical TAMP solvers, and thus, they also depend on meticulously hand-crafted and task-specific symbolic planning domains.,Our results highlight the importance of policy sequence optimization when solving TAMP-like tasks from a natural language instruction, that integrated planning is equipped to solve a family of semantically partially observable problems, and that plan termination via inferred symbolic constraints is more reliable than prior LLM scoring techniques.,Another line of works accelerates TAMP by learning sampling distributions [54, 56], visual feasibility heuristics [14, 15, 17], low-level controllers [16, 50], or state sparsifiers [9, 48]."
d6811adf94d85108b30da82a851ad364823fb9db,869cd60e0fe1c0ae5e93854f218bb33eaa45c1d4,True,background,"While learning symbolic representations has been proposed for TAMP [34, 3, 33, 49, 11, 10, 50], these approaches often require task-specific symbolic transition experience.,I. INTRODUCTION
Task and Motion Planning (TAMP) refers to a problem setting in which a robot has to solve long-horizon tasks that require both symbolic and geometric reasoning [21].,We conduct experiments to test the following hypotheses: H1 Policy sequence optimization is a necessary ingre-
dient for solving TAMP-like problems specified by natural language with LLMs and robot skills.,While different TAMP methods have relaxed the dependence on these components, many prominent works require all three [32, 36, 53, 12, 4, 20].,Text2Motion is inspired by traditional TAMP methods, but
ar X
iv :2
30 3.,We follow a modular approach similar to traditional TAMP methods but replace the commonly used symbolic task planner with an LLM.,Hence, the process of iterating between a) and b) may take on the order of minutes until the TAMP solver returns a solution to a sufficiently complex task or indicates that none exists.,While details differ across TAMP methods, three core components are common [21].,Lastly, these methods ignore the uncertainty of skill feasibility predictions, which [1] demonstrates is important for using skills to solve geometrically complex TAMP problems.,A. Planning is required to solve TAMP-like tasks (H1) Our first hypothesis is that performing policy sequence optimization on task plans output by the LLM is essential to task success.,However, these methods learn from solutions computed by classical TAMP solvers, and thus, they also depend on meticulously hand-crafted and task-specific symbolic planning domains.,Our results highlight the importance of policy sequence optimization when solving TAMP-like tasks from a natural language instruction, that integrated planning is equipped to solve a family of semantically partially observable problems, and that plan termination via inferred symbolic constraints is more reliable than prior LLM scoring techniques.,Another line of works accelerates TAMP by learning sampling distributions [54, 56], visual feasibility heuristics [14, 15, 17], low-level controllers [16, 50], or state sparsifiers [9, 48]."
f3cf71c51b882fe3111d71c4bf104297d38197f8,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,result,methodology","Similar to ours are recent task planning approaches that leverage pre-trained autoregressive LLMs to decompose abstract, high-level instructions into a sequence of low-level steps executable by an agent [20, 21] in a zero-shot manner.,While prior work has investigated using language models as planners [20, 21] or incorporating multimodal-informed perception through language [19], to the best of our knowledge no work has studied the critical link of not only planning with language, but also informing embodied feedback with language, which we investigate in this work.,The “planner,” which is a pretrained LLM [20, 21], attempts to find a sequence of skills to accomplish the instruction.,In real-world kitchen mobile manipulation domain (bottom), we additionally ground the actions using pre-trained affordance functions built in [21], which do not communicate back to the language model.,Various prior works have explored using language as a space for planning [51, 52, 53, 20, 54, 21].,This instantiation of Inner Monologue uses (i) InstructGPT [9, 97] for planning [20, 21], (ii) scripted modules to provide language feedback in the form of object recognition (Object), success detection (Success), and task-progress scene description (Scene), and (iii) a pre-trained language-conditioned pick-and-place primitive (similar to CLIPort [82] and Transporter Nets [81]).,The baseline, SayCan [21], is a method that plans and acts in diverse real world scenarios by combining an LLM with value functions of control policies.,While LLMs have demonstrated exceptional planning capabilities for embodied control tasks [20], prior works have found it crucial to ground LLM predictions with external components such as affordance functions [21] in order to produce useful plans that are executable by robots.,SayCan [21] instead grounds the actions by multiplying each candidate action’s probability under FLAN [59] with the action’s value function, which serves as a proxy for affordance [34].,We implement Inner Monologue in a robotic system using the kitchen environment and task definitions described in SayCan [21]."
f3cf71c51b882fe3111d71c4bf104297d38197f8,ada81a4de88a6ce474df2e2446ad11fea480616e,True,"background,result,methodology","Finally, Socratic Models [19] proposes the combination of different foundation models (e.g., GPT-3 [9], ViLD [83]) and language-conditioned policies, using language as the common interface.,Socratic Models [19] combines several foundation models (e.,multimodal-informed perception through language [19], to the best of our knowledge no work has studied the critical link of not only planning with language, but also informing embodied feedback with language, which we investigate in this work.,While Socratic Models has been demonstrated on a tabletop object manipulation task, Inner Monologue examines additional challenges for robots operating in dynamic environments, which require closed-loop feedback to the planner.,We observe that similarly to recent work [19], natural language provides a universal and interpretable interface for such grounding of model communication and allows them to incorporate their conclusions in an overarching inner monologue driven by a language model.,Object feedback informs the LLM planner about the objects present in the scene, and the variant using only Object feedback is similar to the demonstrated example in [19] in this environment.,Various prior works have explored using language as a space for planning [51, 52, 20, 53, 21, 19].,We compare different variants of Inner Monologue with different LLM-informed closed-loop feedback, as well as an open-loop variant that only runs object recognition once at the beginning of the task (similar to the system demonstrated in [19]).,Task Planning with Language Models."
f3cf71c51b882fe3111d71c4bf104297d38197f8,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,methodology,"We additionally compare to a multi-task CLIPort directly trained on long-horizon task instructions.,Because CLIPort is a single-step policy and does not terminate spontaneously during policy rollout, we report CLIPort evaluations with oracle termination (i.e., repeat until oracle indicates task completion) and fixed-step termination (i.e., repeat for 15 steps).,This instantiation of Inner Monologue uses (i) InstructGPT [9, 97] for planning [20, 21], (ii) scripted modules to provide language feedback in the form of object recognition (Object), success detection (Success), and task-progress scene description (Scene), and (iii) a pre-trained language-conditioned pick-and-place primitive (similar to CLIPort [82] and Transporter Nets [81]).,Furthermore, this performance directly translates to unseen tasks by leveraging rich semantic knowledge of LLM. Finally, we observe that non-hierarchical and solitary systems such as CLIPort (i) struggle at generalizing to unseen long-horizon tasks under test-time disturbances, and (ii) on training tasks, an oracle is also often required to indicate task completion for good performance.,CLIP has been employed in several robotics and embodied settings in zero-shot manner [80], or combined with Transporter networks [81] as in CLIPort [82]."
b9b220b485d2add79118ffdc2aaa148b67fa53ef,1ccd73d95b5e23279929866bc15ec2983f9fa700,True,background,"Some of the most successful applications of pre-training lie at the boundary of natural language processing and other domains, as in instruction following (Hill et al., 2020) and language-guided image retrieval (Lu et al.,…and additionally require nontrivial planning and reasoning capabilities: for example, visionlanguage navigation (Majumdar et al., 2020; Fried et al., 2018; Suglia et al., 2021), instruction following (Zhang & Chai, 2021; Hill et al., 2020), and visual question answering (Tsimpoukelli et al., 2021).,, 2021), instruction following (Zhang & Chai, 2021; Hill et al., 2020), and visual question answering (Tsimpoukelli et al.,Some of the most successful applications of pre-training lie at the boundary of natural language processing and other domains, as in instruction following (Hill et al., 2020) and language-guided image retrieval (Lu et al., 2019)."
bef63d4f7656393b7bceb2ec704e86577c286166,6536f36648d39f0f9f6105562f76704fcc0b19e8,True,"background,methodology","It achieves 8.17% absolute (50.15% relative) gain in SR on Tests Unseen, and 0.66% absolute (2.63% relative) gain in SR on Tests Seen over HLSM, the previous SOTA.,Egocentric RGB is first processed into depth map and instance segmentation, with MaskRCNN (He et al., 2017) (and its implementation by Shridhar et al. (2021)) and the depth prediction method of Blukis et al. (2021); details of the training are explained in Section 5 3 .,%) with a large margin (8% absolute) from the previous SOTA (Blukis et al., 2021).,Looking down is common in existing models as well (Kim et al., 2021; Zhang & Chai, 2021; Blukis et al., 2021).,Baselines There are two kinds of baselines: those that use low-level sequential instructions (Kim et al., 2021; Zhang & Chai, 2021; Nguyen et al., 2021; Pashevich et al., 2021) and those that do not (Nottingham et al., 2021; Blukis et al., 2021).,While the above error analysis is specific to FILM, its implications regarding visual perception may generally represent the weaknesses of existing methods for EIF, since most recent methods (ABP, HLSM, HiTUT, LWIT, E.T.) use the same family of segmentation/ detection models as FILM, such as Mask-RCNN and Fast-RCNN (Wang et al., 2017).,Method % 1st Goal Found SR
HLSM (Blukis et al., 2021) N/A 11.8
FILM with Search 80.51 20.09 FILM w.o. Search 76.12 19.85 With Valid Unseen as the development set, we observed that the semantic search policy significantly helps to find small objects (Table 5); we use the percent of episodes in which the first goal object was found (%1st Goal Found) as a proxy, since it can be picked up (e.g. “Apple”, “Pen”)
and thus is usually small.,In the Semantic Mapping module, separate depth models for camera horizons of 45◦and 0◦were fine-tuned from an existing model of HLSM (Blukis et al., 2021), both with learning rate 1e-3 and the AdamW optimizer (epsilon 1e-6, weight decay 1e-2).,The C channels each represent whether a particular object of interest was observed; the two extra channels denote whether obstacle exists and whether exploration happened
3We use the publicly released code of Shridhar et al. (2021); Blukis et al. (2021).
in a particular 5cm × 5cm space.,46%) with a large margin (8% absolute) from the previous SOTA (Blukis et al., 2021).,Looking down is common in existing models as well (Kim et al., 2021; Zhang & Chai, 2021; Blukis et al., 2021).
the required interaction actions.,5Existing works(Blukis et al., 2021; Kim et al., 2021; Zhang & Chai, 2021; Pashevich et al., 2021) use subtask sequence annotations (or expert trajectories that contain the subtask annotations) as well.,Recently, Blukis et al. (2021) has proposed a more modular method with a persistent and structured spatial memory.,While Blukis et al. (2021) recently introduced a method that uses a structured spatial memory, it comes with some limitations from the lack of explicit semantic search and the reliance on expert trajectories.,Method % 1st Goal Found SR
HLSM (Blukis et al., 2021) N/A 11.8
FILM with Search 80.51 20.09 FILM w.o. Search 76.12 19.85 With Valid Unseen as the development set, we observed that the semantic search policy significantly helps to find small objects (Table 5); we use the percent of episodes in which…"
bef63d4f7656393b7bceb2ec704e86577c286166,62516303058a1322450b58e4cd778ab873b5e531,True,"background,result,methodology","While this “low-level” policy could be learned with imitation or reinforcement learning, we used a deterministic one based on the findings of earlier work that observed that the Fast Marching Method performs as well as a learned local navigation policy (Chaplot et al., 2020b).,Embodied instruction following (EIF) presents a more complex and human-like setting than VLN or Object Goal Navigation (Gupta et al., 2017; Chaplot et al., 2020b; Du et al., 2021); beyond just navigation, agents are required to execute sequences of sub-tasks that entail both navigation and…,We designed the semantic mapping module (Appendix A.2) with inspirations from prior work (Chaplot et al., 2020b).,In contrast to these works, recent methods (Chaplot et al., 2020b;a) build semantic maps with differentiable projection operations, which restrain egocentric prediction errors amplifying in the map.,Furthermore, while Chaplot et al. (2020b) employs a semantic exploration policy, our and their semantic policies serve fundamentally different purposes; while their policy guides a general sense of direction among multiple rooms in the search for large objects (e.g. fridge), ours guides the search…"
a19a9b3db8b67e91b45724e349aa2ab3f2006437,6536f36648d39f0f9f6105562f76704fcc0b19e8,True,"methodology,background",", 2020) and modularly trained components (Blukis et al., 2021; Min et al., 2021) (e.,…embodied navigation (Thomason et al., 2020; Chi et al., 2019; Roman et al., 2020) or limited interaction (Suhr et al., 2019), which are narrower domains than the larger instruction following literature (Tellex et al., 2011, 2020; Shridhar et al., 2020; Blukis et al., 2018, 2021; Min et al., 2021).,Popular methods rely on imitation learning (Pashevich et al., 2021; Singh et al., 2020) and modularly trained components (Blukis et al., 2021; Min et al., 2021) (e.g. for mapping and depth)."
a19a9b3db8b67e91b45724e349aa2ab3f2006437,bc23dfb70e109dbe666b979f995e2779b96e1073,True,background,"…embodied navigation (Thomason et al., 2020; Chi et al., 2019; Roman et al., 2020) or limited interaction (Suhr et al., 2019), which are narrower domains than the larger instruction following literature (Tellex et al., 2011, 2020; Shridhar et al., 2020; Blukis et al., 2018, 2021; Min et al., 2021)."
2d2ca2e54c54748557b8aac7d328ce32ebfe8944,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"methodology,background","Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022) and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision making.,We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022).,Inner Monologue made further improvements by adding the eponymous “inner monologue"", which is implemented as injected feedback from the environment.,However, we argue that Inner Monologue does not truly comprise of inner thoughts — this is elaborated in Section 4.,Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”.,To our knowledge, Inner Monologue is the first work that demonstrates such a closed-loop system, which ReAct builds on.,However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state.,On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors.,This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct thoughts can do these, it can also modify the model’s internal belief, reasoning styles, or anything the flexible thought space supports, for better task solving."
0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,"result,methodology,background","We start with the demonstration data used by RT1 [24] covering 16 unique objects.,representations [5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 21], while other works have leveraged pre-trained language models [9, 10, 11, 12, 22, 23, 24, 25].,Generalization in Robotic Learning A number of recent works have studied how robots can complete novel language instructions [9, 10, 11, 22, 23, 24, 26, 27, 28], typically focusing on instructions with novel combinations of words, i.,To learn a language-conditioned policy , we build on top of RT-1 [24], a recent robotics transformer-based model that achieves high levels of performance across a wide variety of manipulation tasks.,The data on the left was what was used by [24].,never seen at training time? To answer this question, we compare our method, MOO, to two other baselines: RT-1 [24] that uses an ImageNet-pretrained E cientNet tokenizer but does not use an additional VLM to detect objects and VIMA-like [25] that also utilizes object-centric representations but not in the form of object pixels.,Once we obtain the mask, we append it channelwise to the current image together with the recent image history, which is passed into the RT-1 policy architecture [24].,RT-1 (our data) [24] 54 25 50 50 RT-1 (original data) 311 38 171 13 VIMA-like [25] 62 50 50 25 MOO (ours) 92 75 83 75,Inspired by RT-1 [24], in this work, we focus on ve di erent types of skills: “pick X ,” “move X near Y ,” “knock X over,“ “place X upright,“ and “place X into Y ,” where X and Y are object descriptions, such as “red cup” or “pink stu ed elephant,” describing objects in the scene.,We compare MOO to two prior methods: RT-1 [24] and a modi ed version of VIMA [25], which we refer to as “VIMA-like”."
0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,25425e299101b13ec2872417a14f961f4f8aa18e,True,"result,methodology","To answer this question, we compare our method, MOO, to two other baselines: RT-1 [24] that uses an ImageNet-pretrained E cientNet tokenizer but does not use an additional VLM to detect objects and VIMA-like [25] that also utilizes object-centric representations but not in the form of object pixels.,These modi cations are necessary because the original VIMA implementation is tied to the action space of a UR5 robot and is not applicable to our data and robot, i.e., our robot arm moves in 6D and it has a gripper that can open and close continuously.,representations [5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 21], while other works have leveraged pre-trained language models [9, 10, 11, 12, 22, 23, 24, 25].,VIMA-like preserves the cross-attention mechanism, but uses the mask image as the prompt token and the current image as state token.,never seen at training time? To answer this question, we compare our method, MOO, to two other baselines: RT-1 [24] that uses an ImageNet-pretrained E cientNet tokenizer but does not use an additional VLM to detect objects and VIMA-like [25] that also utilizes object-centric representations but not in the form of object pixels.,RT-1 (our data) [24] 17 7 29 VIMA-like [25] 50 7 7 MOO (ours) 67 50 43,This can be explained by the fact that MOO is able to correctly utilize an underlying VLM to nd novel objects that the robot has not interacted with and it can incorporate that information more e ectively than the VIMA-like baseline.,These two baselines correspond to common alternatives where the computer vision data is used as a pre-training mechanism (as in the case of RT-1) or object-centric information is fed to the network in a di erent way (as in the case of VIMA-like).,RT-1 (our data) [24] 54 25 50 50 RT-1 (original data) 311 38 171 13 VIMA-like [25] 62 50 50 25 MOO (ours) 92 75 83 75,We compare MOO to two prior methods: RT-1 [24] and a modi ed version of VIMA [25], which we refer to as “VIMA-like”.,Across all of these challenging evaluation scenes, we nd that MOO is signi cantly more robust compared to a VIMA-like baseline [25] and an RT-1 baseline [24]."
0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,742b195fb4c2868a4e60012c8e0bf7db43bb5650,True,"methodology,background","Figure 10: We present CoW-MOO, a system that combines an open-vocabulary object navigation by CoW [54] with open-world manipulation by MOO.,We implement a variant of CoW for pre-explored setting and combine it with MOO, which we refer to as CoW-MOO.,Finally, our experiments further show that our method can be integrated with an open-vocabulary object navigation model called Clip-on-Wheels (CoW), to complete mobile manipulation tasks involving novel object categories.,For the last experimental question, we consider how such a system can be integrated with open-vocabulary object-based navigation such as NLMap [55] or CLIP on Wheels (CoW) [54].,Throughout this paper, we refer to our approach as Manipulation of Open-vocabulary Objects (MOO) and the integrated mobile manipulation system as CoW-MOO.,CoW handles open-vocabulary navigation to an object of interest, upon which MOO continues with manipulating the target object."
0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,1d803f07e4591bd67c358eef715bcd443e821894,True,"result,background","On the other hand, several prior works have trained neural network policies with pre-trained image representations [5, 6, 7, 8] and pre-trained language instruction embeddings [9, 10, 11, 12].,Learning Using o -the-shelf vision, speech, or language models is a long-standing approach in robotics [10, 13, 14].,representations [5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 21], while other works have leveraged pre-trained language models [9, 10, 11, 12, 22, 23, 24, 25].,Generalization in Robotic Learning A number of recent works have studied how robots can complete novel language instructions [9, 10, 11, 22, 23, 24, 26, 27, 28], typically focusing on instructions with novel combinations of words, i."
f318ab67ac22cb758e38a16dafdc8e486b7b9756,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"methodology,background",", re-prompting with inference can be integrated into visual-language model approaches like SayCan (Ahn et al., 2022), whereas re-prompting with inference can be integrated with task and motion planning approaches (Garrett et al.,…prompt depends on the modeling capabilities of the agent (i.e., re-prompting with inference can be integrated into visual-language model approaches like SayCan (Ahn et al., 2022), whereas re-prompting with inference can be integrated with task and motion planning approaches (Garrett et al., 2021)).,Many related works have investigated identifying robotic affordances in the context of robotic grasping (Yamanobe et al., 2017), path planning (Ardón et al., 2020), and generic skill execution (Ahn et al., 2022).,Therefore, deciding which pre-condition error information in the corrective prompt depends on the modeling capabilities of the agent (i.e., re-prompting with inference can be integrated into visual-language model approaches like SayCan (Ahn et al., 2022), whereas re-prompting with inference can be integrated with task and motion planning approaches (Garrett et al., 2021)).,, 2020), and generic skill execution (Ahn et al., 2022).,…contain sufficient world knowledge to make goal-driven plans and particular language prompts improve an LLM’s ability to outline logical reasoning (Ahn et al., 2022; Kojima et al., 2022), so we see language-based re-prompting as a potentially useful method to integrate environment error…,Similarly, Ahn et al. (2022) introduced SayCan, an LLM-integrated pipeline that is capable of determining a sequence of actions to achieve specific goals grounded to “affordances” (pre-defined set of skills that the robot can perform, all manually demonstrated by expert).,Previous works have shown that pretrained LLMs contain sufficient world knowledge to make goal-driven plans and particular language prompts improve an LLM’s ability to outline logical reasoning (Ahn et al., 2022; Kojima et al., 2022), so we see language-based re-prompting as a potentially useful method to integrate environment error information to LLM planners.,Most closely related to our paper are Ahn et al. (2022) and Huang et al. (2022), which both aim to integrate LLMs into an open-loop planning pipeline for task execution.,To provide context about the scene, other work has used visual-language models to determine what natural language actions are feasible based on sensor data (Ahn et al., 2022) or processed scene descriptions (e.,…context about the scene, other work has used visual-language models to determine what natural language actions are feasible based on sensor data (Ahn et al., 2022) or processed scene descriptions (e.g., common-sense reasoning for embodied agents in text-based video games (Yao et al., 2020;…"
f318ab67ac22cb758e38a16dafdc8e486b7b9756,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,background,"…prompt depends on the modeling capabilities of the agent (i.e., re-prompting with inference can be integrated into visual-language model approaches like SayCan (Ahn et al., 2022), whereas re-prompting with inference can be integrated with task and motion planning approaches (Garrett et al., 2021)).,Task and motion planning (TAMP) aims to decompose robot planning and execution processes in a hierarchical manner (Kaelbling & Lozano-Pérez, 2010; Garrett et al., 2021).,Task and motion planning (TAMP) aims to decompose robot planning and execution processes in a hierarchical manner (Kaelbling & Lozano-Pérez, 2010; Garrett et al., 2021).,, 2022), whereas re-prompting with inference can be integrated with task and motion planning approaches (Garrett et al., 2021)).,Learning and modeling preconditions have been largely studied in model-based approaches that leverage symbolic planning (Garrett et al., 2021; Konidaris et al., 2018), but in this work we investigate how precondition information can be leveraged to improve planning with LLMs."
8ee45aeb7c97e3346cc62f216f673b91277ac718,4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,True,background,Recent years have seen surging interests in such embodied instruction following agents [9] based on simulated environments such as Matterport3D [2] and AI2-Thor [15].
8ee45aeb7c97e3346cc62f216f673b91277ac718,6536f36648d39f0f9f6105562f76704fcc0b19e8,True,"result,background,methodology","We compare with two main baseline models, HLSM [3] and FILM [24].,For evaluating the end-to-end task completion performance of LLM-Planner, we integrate it with a strong baseline method, HLSM [3], which satisfies such an interface.,We consider hierarchical planning models [36] for VLN, which is explored to various extent in several recent studies [3, 24, 31, 34], but none of them considers the few-shot setting or LLMs for planning.,We evaluate LLM-Planner on the standard ALFRED [32] household dataset by integrating it with the perception module and low-level planner from a strong baseline model, HLSM [3].,HLSM [3] consists of three components: a semantic voxel map, a high-level planner, and a low-level planner.,State-of-the-art VLN methods [3,24] use a map-based low-level planner and a simple path-finding algorithm to find the target object in the current subgoal from the map.,However, in more complex VLN, or embodied instruction following, datasets such as ALFRED [32], hierarchical planning models [3, 17, 24] that separate the high-level and low-level planning have proven to be most effective."
a9e6380ec7e0a9c336606e94e07705dee9391fef,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,background,"While it is an intuitive idea of extracting common sense from LLMs for task planning [10]–[12], [21], there is a fundamental challenge for robots to “ground” domain-independent commonsense knowledge [22] to specific domains that are featured with many domain-dependent constraints.,, “make breakfast”) by sequencing actions [11], [12], [34].,Recent research has demonstrated that those LLMs contain a wealth of commonsense knowledge [12], [18]–[20].,To this end, researchers have developed openworld planning methods for robust task completions in realworld scenarios [7]–[12]."
9f281c830a3f65fd03ed105d0712207f5428d3b9,f3cf71c51b882fe3111d71c4bf104297d38197f8,True,"methodology,background","It has been used as a few-shot policy for language-conditioned task planning [5, 6, 7].,[7] proposed translating sensor observations into language feedback to provide to the pre-trained LLMs so that it is able to provide the next instruction conditioned on the outcomes of the previous instructions.,Most recently, the pre-trained LLMs have been exhibiting impressive ability for format following and content generation, allowing them to be applied as policies for task planning [5, 6, 7]."
94bcf0390d5acb1b92323bd15cc1dc311314122c,960236c4380656fa254f7e367ceb4b14cbeda45c,True,methodology,"Following the observation by Howell et al [21], second-order planners such as iLQG produces smoother and more accurate actions while zeroth-order planners such as Predictive Sampling is better at exploring non-smooth optimization landscape, we use iLQG for legged locomotion tasks, while use Predictive Sampling for manipulation tasks in this work.,In addition, as seen in the supplementary video, MJPC can discover highly dexterous and dyanmic maneuvers to accomplish the desired task.,Specifically, we use an open-source implementation based on the MuJoCo simulator [49], MJPC [21].,For each generated reward code, we evaluate on MJPC for 50 times and report the success rate.,We design experiments to answer the following questions:
1) Is our proposed method, by combining LLMs and MJPC, able to generate diverse and complex robot motions through natural language interface?,A.6.3 Sim-to-Real residual term
As seen in the supplementary video, MuJoCo MPC can discover highly dynamic and dexterous manipulation skills that exceeds the capabilities of existing hardwares.,Concretely, we explore the code-writing capabilities of LLMs to translate task semantics to reward functions, and use MuJoCo MPC, a real-time optimization tool to synthesize robot behavior in real-time [21].,Both robots are modeled and simulated in MuJoCo MPC [21].,One benefit of using a real time optimization tool like MJPC is that humans can observe the motion being synthesized in real time and provide feedback.,In this work, we investigate a new paradigm for interfacing an LLM with a robot through reward functions, powered by a low-level model predictive control tool, MuJoCo MPC.,For each task and method considered, we generate 10 responses from Reward Translator, each evaluated in MJPC for 50 times, thus we measure the end-to-end stability of the full pipeline.,MJPC has demonstrated the interactive creation of diverse behaviors such as legged locomotion, grasping, and finger-gaiting while supporting multiple planning algorithms, such as iLQG and Predictive Sampling.,In this work, we assume the reward takes a particular form, suitable for use with MJPC (see below).,1 right): i) a Reward Translator, built upon pre-trained Large Language Models (LLMs) [10], that interacts with and understands user intents and modulates all reward parameters ψ and weights w, and ii) a Motion Controller, based on MuJoCo MPC [21], that takes the generated reward and interactively optimize the optimal action sequence a1:H .,The robot applies the action corresponding to its current timestamp, advances to the next step, and sends the updated robot states to the MJPC planner to initiate the next planning cycle."
94bcf0390d5acb1b92323bd15cc1dc311314122c,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"background,methodology","For the Code-as-Policies (CaP) baseline, we design the primitive skills based on common commands available to the robot.,You do not need to make the robot do extra things not mentioned in the plan such as stopping the robot.
vi) Baseline: Code-as-Policies Prompt for Dexterous Manipulator
We have a manipulator and we want you to help plan how it should move to perform tasks using the following APIs:
def end_effector_to(position_obj)
position_obj is a list of 3 float numbers [x,y,z]
def end_effector_open()
Open the end effector.
def end_effector_close()
Close the end effector.
def get_object_position(obj_name)
Given an object name, return a list of 3 float numbers [x,y,z] for the object position. the object can come from a list of [“apple"", “banana"", “bowl"", “box"", “drawer_handle"", “faucet_handle"", “drawer_center"", “rest_position""]
def get_normalized_joint_position(joint_name)
Given an joint name, return a float numbers x. the joint can come from a list of [“drawer"", “faucet""]
def reset()
Reset the agent.,Sample 1
import numpy as np
reset_reward() set_joint_fraction_reward(""faucet"", 1)
execute_plan()
Sample 2
import numpy as np
reset_reward() # Reset reward for a new task set_joint_fraction_reward(""faucet"",
1) # Turn on the faucet by setting the joint fraction to 1 (open)
execute_plan(2)
Code-as-Policies
Sample 1
reset() faucet_handle_position = get_object_position(""faucet_handle"") end_effector_to(faucet_handle_position) end_effector_close() normalized_joint_position = get_normalized_joint_position(""faucet"") end_effector_open()
Sample 2
#python import numpy as np
reset() faucet_handle_pos = get_object_position(""faucet_handle"") end_effector_to(faucet_handle_pos) end_effector_open() end_effector_close()
normalized_faucet_position = get_normalized_joint_position(""faucet"") target_faucet_position = np.clip(normalized_faucet_position + 0.5, 0, 1)
end_effector_to([faucet_handle_pos[0], faucet_handle_pos[1], faucet_handle_pos[2] + 0.1]) end_effector_to([faucet_handle_pos[0], faucet_handle_pos[1], faucet_handle_pos[2]]) end_effector_open()
A.9 Full outputs for the interactive examples
Below we show the output reward code from our system for the two interactive examples: tasking a quadruped robot to perform moonwalk, and tasking a dexterous manipulator robot to place an apple in a drawer.,Don’t forget to call execute_plan at the end.
iii) Baseline: Code-as-Policies Prompt for Quadruped
We have a quadruped robot.,The prompt consists of three parts: i) description of the reward APIs that the LLM can call to specify different parameters of the reward function, ii) an example response that we expect the Reward Coder to produce, and iii) the constraints and rules that the Reward Coder needs to follow.,We compare our proposed system to two baseline methods: i) an ablation of our approach that only uses Reward Coder without having access to the Motion Descriptor, and ii) Code-as-Policies [3] where the
LLM generates a plan for the robot motion using a set of pre-defined robot primitive skills instead of reward functions.,The capability of those models range from solving coding competition questions [38] and benchmarks [39], to drawing simple figures [40], generating policies that solve 2D tasks [41], and complex instruction following tasks [3].,R O
] 1
4 Ju
n 20
23
Language Model
Policy Code
block_names = detect_objects(""blocks"") bowl_names = detect_objects(""bowls"") for bowl_name in bowl_names: if is_empty(bowl_name): empty_bowl = bowl_name break objs_to_stack = [empty_bowl] + block_names stack_objects(objs_to_stack)
def is_empty(name): ...
Perception APIs Control APIs
User
Stack the blocks on the empty bowl.
def stack_objects(obj_names): n_objs = len(obj_names) for i in range(n_objs - 1): obj0 = obj_names[i + 1] obj1 = obj_names[i] pick_place(obj0, obj1)
Good!,We compare our proposed system to two baseline methods: i) an ablation of our approach that only uses Reward Coder without having access to the Motion Descriptor, and ii) Code-as-Policies [3] where the,These diverse applications have extended to the field of robotics as well, where substantial progress has been made in using LLMs to drive robot behaviors [3, 5, 4, 9, 10, 11]: from step-by-step planning [4, 9, 12], goal-oriented dialogue [10, 11], to robot-code-writing agents [3, 13].,def set_legs_to_pose(pose): set_target_joint_angles(""front_left"", pose[0]) set_target_joint_angles(""back_left"", pose[1]) set_target_joint_angles(""front_right"", pose[2]) set_target_joint_angles(""back_right"", pose[3]),These LLMs exhibit remarkable adaptability to new contexts (such as APIs [3], task descriptions [4], or textual feedback [5]), allowing for tasks ranging from logical reasoning [6, 7] to code generation [8] with minimal hand-crafted examples."
94bcf0390d5acb1b92323bd15cc1dc311314122c,a58b3f2ab75fdbda082e684d027ab4f552b0b5d3,True,"background,methodology","Later work explore mapping language corrections to composable cost functions similar to our work by training a prediction model from demonstration and apply trajectory optimization to perform control [25].,The idea of grounding language to reward has been explored by prior work for extracting user preferences and task knowledge [22, 23, 24, 25, 26, 27].,Although these methods can achieve challenging language conditioned robotic tasks such as object pushing [25], and drawer opening [42], they require considerable language-labeled data to train the reward model.,The idea of translating natural language instructions to rewards has been explored by several prior work [26, 23, 25, 42, 22, 43, 27].,A common strategy in this direction is to train domain-specific reward models that map language instructions to reward values [23, 22, 42] or constraints [25]."
ba63203d7f91d4135d2a392e841ce532c006e31c,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"background,methodology","We compare to a variant of Code as Policies [74] as a baseline that uses an LLM with action primitives.,Specifically, when an instruction is given as a comment in the code, LLMs can be prompted to 1) call perception APIs (which invoke visuallanguage models (VLM) such as an open-vocabulary detector [13–15]) to obtain spatial-geometrical information of relevant objects, 2) generate NumPy operations to manipulate 3D arrays, and 3) pre-
2Note that the decomposition and sequencing of these sub-tasks are also done by LLMs in this work, though we do not investigate this aspect extensively as it is not the focus of our contributions.
scribe precise values at relevant locations.,We do not provide primitives such as pick-and-place as they would be tailored for a particular suite of tasks that we do not constrain to in our study (similar to the control APIs for VoxPoser specified in Sec.,To allow language models to perceive the physical environments, textual descriptions of the scene [39, 11, 59] or perception APIs [74] can be given, vision can be used during decoding [67] or can be directly taken as input by multi-modal language models [68, 2].,While these are expressed in the text form, LLMs can generate Python code to invoke perception APIs to obtain spatial-geometric information of relevant objects or parts (e.g., “handle”) and then manipulate the 3D voxels to prescribe reward or cost at relevant locations in observation space (e.g., the target location of the handle is assigned a high value while the surrounding of the vase is assigned low values).,Besides exposing NumPy [16] and the Transforms3d library to the LLM, we provide the following environment APIs that LLMs can choose to invoke:
detect(obj name): Takes in an object name and returns a list of dictionaries, where each dictionary corresponds to one instance of the matching object, containing center position, occupancy grid, and mean normal vector.
execute(movable,affordance map,avoidance map,rotation map,velocity map,gripper map): Takes in an “entity of interest” as “movable” (a dictionary returned by detect) and (optionally) a list of value maps and invokes the motion planner to execute the trajectory.,LLM + Primitives [74] uses LLMs to sequentially compose primitives, thus not having a dynamics module.,For baselines, we ablate the two components of VoxPoser, LLM and motion planner, by comparing to a variant of [74] that combines an LLM with primitives and to a variant of [50] that learns a U-Net [112] to synthesize costmaps for motion planning.,[74] showed that LLMs exhibit behavioral commonsense that can be useful for low-level control.,We further compare to a variant of Code as Policies [74] that uses LLMs to parameterize a predefined list of simple primitives (e.g., move to pose, open gripper).,[74], which recursively calls LLMs using their own generated code, where each language model program (LMP) is responsible for a unique functionality (e.,We further compare to a variant of Code as Policies [74] that uses LLMs to parameterize a predefined list of simple primitives (e."
ba63203d7f91d4135d2a392e841ce532c006e31c,a58b3f2ab75fdbda082e684d027ab4f552b0b5d3,True,methodology,"UNet + MP [50] trains a U-Net [112, 113] to directly map RGB-D observations to value maps which are then used by a motion planner (MP), thus not having an independent perception module.,For baselines, we ablate the two components of VoxPoser, LLM and motion planner, by comparing to a variant of [74] that combines an LLM with primitives and to a variant of [50] that learns a U-Net [112] to synthesize costmaps for motion planning.,U-Net Language Models Train/Test Category MP [50] Prim.,[50], where an end-to-end cost predictor is optimized via supervised learning to map language instructions to 2D costmaps, which are used to steer a motion planner to generate preferred trajectories in a collisionfree manner."
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,background,"With its simplicity, zero sample complexity, and better practicality, BC has been extensively used in real-world environments, especially in robotics (Zeng et al., 2021; Florence et al., 2022; Qin et al., 2022; Zhang et al., 2018; Brohan et al., 2022; Rahmatizadeh et al., 2018; Florence et al., 2019; Zeng et al., 2020).,Comparison with RT-1 Robotic Transformer-1 (RT-1) (Brohan et al., 2022) is a concurrent work that also directly models low-level control actions with a Transformer.,While RT-1 shows great promise in developing decision foundation models for robotics, it adopts the conventional auto-regressive Transformer without explicitly leveraging the structural knowledge presented in low-level control tasks.,Another difference is that since RT-1 discretizes the action space, it might suffer from degraded performance for tasks that require high precision (such as peg insertion).,…sample complexity, and better practicality, BC has been extensively used in real-world environments, especially in robotics (Zeng et al., 2021; Florence et al., 2022; Qin et al., 2022; Zhang et al., 2018; Brohan et al., 2022; Rahmatizadeh et al., 2018; Florence et al., 2019; Zeng et al., 2020)."
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8,True,background,", 2021), Behavior Transformer (BeT) (Shafiullah et al., 2022), MaskDP (Liu et al.,…one-shot imitation learning, (Lynch et al., 2020; Singh et al., 2020) explore behavior priors from demos, (Chen et al., 2021; Liu et al., 2022; Janner et al., 2021; Shafiullah et al., 2022; Ajay et al., 2022; Janner et al., 2022) examine different sequence modeling strategies for policy learning.,Note that multimodality is a related but orthogonal issue (Shafiullah et al., 2022), i.,Despite the recent progress (Chen et al., 2021; Florence et al., 2022; Shafiullah et al., 2022; Liu et al., 2022; Ajay et al., 2022), it remains extremely challenging to solve lowlevel control tasks such as contact-rich object manipulations by IL in a scalable manner.,While theoretically sound, it is shown (Shafiullah et al., 2022) to be less practical for non-Markovian implicit models, and later explicit models outperform it.,Note that multimodality is a related but orthogonal issue (Shafiullah et al., 2022), i.e., when a unimodal estimate of the (continuous) action distribution leads to a significantly worse return.,, 2020) explore behavior priors from demos, (Chen et al., 2021; Liu et al., 2022; Janner et al., 2021; Shafiullah et al., 2022; Ajay et al., 2022; Janner et al., 2022) examine different sequence modeling strategies for policy learning.,Namely, Decision Transformer (DT) (Chen et al., 2021), Behavior Transformer (BeT) (Shafiullah et al., 2022), MaskDP (Liu et al., 2022) and Decision Diffuser (DD) (Ajay et al., 2022)."
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,4b516216d7d150a081fd74993bddf36b6b22c118,True,"methodology,background","Behavior Cloning The most straightforward approach in IL is BC, which assumes access to pre-collected demos D = {(st, at)}Nt=1 generated by expert policies and learns the optimal policy with direct supervision by minimizing the BC loss E(s,a)∼D[− log π(a|s)] w.r.t. a mapping π.,…et al., 2021), program execution (Reed & De Freitas, 2015; Nye et al., 2021), commonsense or general reasoning (Rajani et al., 2019; Clark et al., 2020; Liang et al., 2021; Wei et al., 2022), and robotics (Xu et al., 2018; Zhang & Chai, 2021; Jia et al., 2022b; Gu et al., 2022; Yang et al., 2022).,Comparison with Procedure Cloning Procedure cloning (PC) (Yang et al., 2022) is an extension of BC by imitating the intermediate computations of the demonstrators.,Procedure Cloning (Yang et al., 2022) handles a similar issue by imitating the intermediate computations.,Some existing methods (Florence et al., 2022; Yang et al., 2022) are not included because either they were compared unfavorably with the baselines or they assume access to information impractical in our tasks."
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"methodology,background","Comparison with Subgoal-conditioned Policies Unlike many existing work dealing with “long-horizon” tasks, e.g., SayCan (Ahn et al., 2022) and methods tackling ALFRED (Shridhar et al., 2020), which assume that low-level control is solved, we instead focus on solving low-level control with hierarchical information such as key states.,, SayCan (Ahn et al., 2022) and methods tackling ALFRED (Shridhar et al.,Comparison with Subgoal-conditioned Policies Unlike many existing work dealing with “long-horizon” tasks, e.g., SayCan (Ahn et al., 2022) and methods tackling ALFRED (Shridhar et al., 2020), which assume that low-level control is solved, we instead focus on solving low-level control with…"
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,5c74b1021aebd0575c339ce4dfd47e183009a9c5,True,"methodology,background","With its simplicity, zero sample complexity, and better practicality, BC has been extensively used in real-world environments, especially in robotics (Zeng et al., 2021; Florence et al., 2022; Qin et al., 2022; Zhang et al., 2018; Brohan et al., 2022; Rahmatizadeh et al., 2018; Florence et al., 2019; Zeng et al., 2020).,Despite the recent progress (Chen et al., 2021; Florence et al., 2022; Shafiullah et al., 2022; Liu et al., 2022; Ajay et al., 2022), it remains extremely challenging to solve lowlevel control tasks such as contact-rich object manipulations by IL in a scalable manner.,, 2021), discontinuity (Florence et al., 2022), randomness and noisiness (Sasaki & Yamashina, 2020; Wu et al.,Some existing methods (Florence et al., 2022; Yang et al., 2022) are not included because either they were compared unfavorably with the baselines or they assume access to information impractical in our tasks.,A recent method (Florence et al., 2022) deals with this by an energy-based implicit model in place of an explicit one.,Other issues for learning from demonstrations include non-Markovity (Mandlekar et al., 2021), discontinuity (Florence et al., 2022), randomness and noisiness (Sasaki & Yamashina, 2020; Wu et al., 2019) of the demos that lead to difficulties in neural network based policy representation and…,…zero sample complexity, and better practicality, BC has been extensively used in real-world environments, especially in robotics (Zeng et al., 2021; Florence et al., 2022; Qin et al., 2022; Zhang et al., 2018; Brohan et al., 2022; Rahmatizadeh et al., 2018; Florence et al., 2019; Zeng et al.,…"
e1bd151a3f670fd0f77580702fe7a85dc78a41cb,f5275f5eb6569ddb5ba9a959ede09875d56e3bac,True,background,"Among these, (Dasari & Gupta, 2021; Mandi et al., 2021) study one-shot imitation learning, (Lynch et al., 2020; Singh et al., 2020) explore behavior priors from demos, (Chen et al., 2021; Liu et al., 2022; Janner et al., 2021; Shafiullah et al., 2022; Ajay et al., 2022; Janner et al., 2022) examine…,, 2022) usually require demonstration with densely labelled rewards and methods that augment online RL with demos (Hester et al., 2018; Kang et al., 2018; Ross et al., 2011; Nair et al., 2020; Rajeswaran et al., 2017; Ho & Ermon, 2016; Pertsch et al., 2021; Singh et al., 2020) rely on on-policy interactions, BC (Pomerleau, 1988) formulates fully supervised or self-supervised learning problems.,, 2021) study one-shot imitation learning, (Lynch et al., 2020; Singh et al., 2020) explore behavior priors from demos, (Chen et al.,…RL with demos (Hester et al., 2018; Kang et al., 2018; Ross et al., 2011; Nair et al., 2020; Rajeswaran et al., 2017; Ho & Ermon, 2016; Pertsch et al., 2021; Singh et al., 2020) rely on on-policy interactions, BC (Pomerleau, 1988) formulates fully supervised or self-supervised learning problems."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d,True,"background,methodology","We refer to training on only DA as the Interactive Language (IL) [32] setting, training on only DB as the RT-1 [7] setting, and training on both DA and DB as the RT-1 + IL setting.,Using these various instruction augmented datasets, we train vision-based language-conditioned behavioral cloning policies with the RT-1 architecture [7].,Table II demonstrates that DIAL is able to solve over 40% more challenging novel tasks across the three evaluation categories compared to either RT-1 and/or IL, which do not use the instruction augmented data DC .,One main difference from RT-1 is that instead of utilizing USE [10] as the task representation for language conditioning, we instead use the language encoder of the fine-tuned CLIP model that was used for instruction
augmentation in Section III-B; full details are described further in Appendix G. Nonetheless, we treat the behavioral cloning policy as an independent component of our method and focus on studying instruction augmentation methods; we do not explore different policy architectures or losses in this work.,DB contains 80,000 episodes with structured teleoperator commands and is representative of the RT-1 [7] setting.,The policies used in this work are trained using the RT-1 [7] architecture on a large dataset of human-provided demonstrations."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,0e34addae55a571d7efd3a5e2543e86dd7d41a83,True,"background,methodology","We refer to training on only DA as the Interactive Language (IL) [32] setting, training on only DB as the RT-1 [7] setting, and training on both DA and DB as the RT-1 + IL setting.,However the performance of such methods is critically dependent on the quantity and breadth of instruction-labeled demonstration data that is available [32], and producing expert demonstrations of robot motion often requires expertise and time [33].,Similarly, Interactive Language [32]
uses crowd-sourced hindsight labels on diverse demonstration data for table-top object rearrangements.,DA contains 5,600 episodes with crowd-sourced language instructions and is representative of the Interactive Language (IL) [32] setting.,Similarly, Interactive Language [32] uses crowd-sourced hindsight labels on diverse demonstration data for table-top object rearrangements.,This experiment is practically motivated by the setting where large amounts of unstructured trajectory data are available but hindsight labels are expensive to collect, such as robot play data [14, 31, 32]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","For long-horizon language instructions, LLMs are used as planners both in a simulated [22] and real-world robotics [2].,Finally, we would like to thank the large team that built [1] and [2], upon which we develop DIAL.,2), where teleoperators perform 551 unique tasks motivated by common manipulation skills and objects in a kitchen environment [2].,A popular method used to learn such policies is behavioral cloning (BC) [24, 32], which has been applied to robotic datasets [2] with diverse language instructions [31].,Pretrained VLMs and LLMs for language-conditioned control Prior works have leveraged pretrained VLMs and LLMs for language-conditioned control, as part of reward modeling [17, 35], as part of the agent architecture [36, 41], or as planners for long-horizon tasks [2, 22, 23].,demonstrations is available, collected for downstream imitation learning [2, 24].,Recent advances in deep learning with large amounts of data have led to works following natural language for robotic manipulations [2, 27, 33, 42, 43].,In robotics, they have been used as representations for perception [36, 41], as task representation for language [24, 29], or as planners [2, 22].,We implement DIAL in a challenging real-world robotic manipulation setting in a kitchen environment similar to SayCan [2].,These trajectories may be collected from human teleoperated demonstrations on a wide variety of tasks [2], or from unstructured robotic “play” data [30]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,1d803f07e4591bd67c358eef715bcd443e821894,True,"background,methodology","We focus on the practically-motivated setting where a dataset of teleoperated demonstrations is available, collected for downstream imitation learning [1, 24].,In robotics, they have been used as representations for perception [37, 42], as task representation for language [24, 30], or as planners [1, 22].,of-the-art language embeddings and pretrained encoders with imitation learning on top of large, manually collected datasets of robotic demonstrations annotated with text commands [24].,a) Language instruction following in robotics: Languageinstruction following agents have been extensively explored with engineered symbolic representations [17, 45], with reinforcement learning (RL) [5, 20, 29], and with imitation learning [3, 6, 24, 30]."
53f64d91766bb74c7b5441026c9b7020fa34ea6b,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,methodology,"CLIPort [42] uses a frozen CLIP vision and text encoders in combination with Transporter networks [47] for imitation learning.,In robotics, they have been used as representations for perception [37, 42], as task representation for language [24, 30], or as planners [1, 22].,Motivated by promising results of CLIP in robotics in prior works [35, 42], our instantiation of DIAL uses CLIP [39] for both instruction augmentation and task representation; nonetheless, other VLMs or captioning models could also be used to propose instruction augmentations.,b) Pretrained VLMs and LLMs for language-conditioned control: Prior works have leveraged pretrained VLMs and LLMs for language-conditioned control, as part of reward modeling [18, 36], as part of the agent architecture [37, 42], or as planners for long-horizon tasks [1, 22, 23]."
9cf66efb5ddc0eef574f909fd4e1fa09994c0184,e00867c7108395d56b823bc5c75d2f4591e2878d,True,"background,methodology","but both [4] and [13] look at predicting action sequences, while we focus on the problem of building an open-vocabulary, queryable 3D scene representation.,Concurrently with our work, NLMap-SayCan [4] and VLMaps [13] proposed two approaches for real-world visionlanguage navigation.,NLMap-SayCan uses a 2D grid-based map and a discrete set of objects predicted by a regionproposal network [4], while CLIP-Fields can make predictions at different granularities."
9cf66efb5ddc0eef574f909fd4e1fa09994c0184,6536f36648d39f0f9f6105562f76704fcc0b19e8,True,background,"Much recent progress on vision-language navigation problems such as ALFRED [25] or RXR [16] has used spatial representations or structured memory as a key component to solving the problem [19, 2, 33, 10].,HLSM [2] and FiLM [19] are built as the agent moves through the environment, and rely on a fixed set of classes and a discretization of the world that is inherently limiting.,In order to perform a variety of complex tasks in human environments, robots often rely on a spatial semantic memory [2, 19, 11].,However, existing representations are coarse, often relying on a preset list of classes and capturing minimal semantics [2, 11]."
25425e299101b13ec2872417a14f961f4f8aa18e,0dc3efa4d4cef4d8a9623dbcaf9344480a2ef1bf,True,background,"In fact, MOO (Stone et al., 2023) includes a baseline called “VIMA-like” that already demonstrates strong performance on real robots under real-world scenarios.,MOO (Minderer et al., 2022) adopts a similar object-centric representation as ours for open-world object manipulation.,MOO (Stone et al., 2023) also shows that a robot agent with OWL-ViT (Minderer et al.,MOO (Stone et al., 2023) also shows that a robot agent with OWL-ViT (Minderer et al., 2022) as the object detector significantly outperforms RT-1 (Brohan et al., 2022), which directly learns from raw pixels, on various real-world manipulation tasks."
25425e299101b13ec2872417a14f961f4f8aa18e,5922f437512158970c417f4413bface021df5f78,True,"methodology,background","Transformers have enabled task unification across many AI domains (Raffel et al., 2020; Radford et al., 2019; Brown et al., 2020; Chen et al., 2022a;b; Lu et al., 2022; Wang et al., 2022c; Alayrac et al., 2022; Reed et al., 2022).,, 2022) and Gato (Reed et al., 2022) leverage the powerful self-attention models for sequential decision making.,Our approach outperforms strong prior SOTA methods such as Gato (Reed et al., 2022), Decision Transformer (Chen et al.,Gato (Reed et al., 2022) introduces a decoder-only model that solves tasks from multiple domains where tasks are specified by prompting the model with the observation and action subsequence."
25425e299101b13ec2872417a14f961f4f8aa18e,183984d0426fd0702fbbe8bd890fe32058ecdfff,True,background,"procedures (Aceituno et al., 2021; Stengel-Eskin et al., 2022; Lynch & Sermanet, 2021), leading to siloed robot systems that cannot be easily combined for a rich set of use cases.,R O
] 2
8 M
ay 2
02 3
procedures (Aceituno et al., 2021; Stengel-Eskin et al., 2022; Lynch & Sermanet, 2021), leading to siloed robot systems that cannot be easily combined for a rich set of use cases.,…et al., 2021; Srinivasan et al., 2020; Thananjeyan et al., 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Dasari & Gupta, 2020; Aceituno et al., 2021; Zhao et al., 2022), rearrangement (Weihs et al., 2021; Szot et al., 2021; Liu et al., 2021; Ehsani et al., 2021; Gan et al.,…,, 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Dasari & Gupta, 2020; Aceituno et al., 2021; Zhao et al., 2022), rearrangement (Weihs et al."
25425e299101b13ec2872417a14f961f4f8aa18e,fd399c7068512858b27535f75c8c31d2442dbaac,True,"methodology,background","From the perspective of leveraging transformer as agent architecture, methods such as Dasari & Gupta (2020) and MOSAIC (Zhao et al., 2022) achieve superior performance in one-shot video imitation tasks.,, 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Aceituno et al., 2021; Zhao et al., 2022), and rearrangement (Liu et al.,They both use the self-attention mechanism with auxiliary losses such as inverse dynamics loss (Dasari & Gupta, 2020) and contrastive loss (Zhao et al., 2022) to learn robot controllers.,…et al., 2020; Thananjeyan et al., 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Dasari & Gupta, 2020; Aceituno et al., 2021; Zhao et al., 2022), rearrangement (Weihs et al., 2021; Szot et al., 2021; Liu et al., 2021; Ehsani et al., 2021; Gan et al., 2021; Stengel-Eskin et…"
25425e299101b13ec2872417a14f961f4f8aa18e,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"methodology,result,background","CLIPort (Shridhar et al., 2021), Perceiver-Actor (Shridhar et al., 2022), and RT-1 (Brohan et al., 2022) apply large transformers to robot manipulation tasks.,Similar to prior work (Zeng et al., 2020; Shridhar et al., 2021), VIMA-BENCH provides scripted oracles to generate successful demonstrations for all tasks.,VIMA-BENCH inherits the same action space from Zeng et al. (2020) and Shridhar et al. (2021), which consists of primitive actions of “pick and place” for tasks with a suction cup as the end effector, or “push” for tasks with a spatula.,CLIPort (Shridhar et al., 2021), Perceiver-Actor (Shridhar et al.,There are many prior works that are not mentioned in the main paper that study different robotic manipulation tasks, such as instruction following (Shridhar et al., 2021; Lynch & Sermanet, 2021), constraint satisfaction (Bharadhwaj et al.,There are many prior works that are not mentioned in the main paper that study different robotic manipulation tasks, such as instruction following (Shridhar et al., 2021; Lynch & Sermanet, 2021), constraint satisfaction (Bharadhwaj et al., 2021; Srinivasan et al., 2020; Thananjeyan et al., 2021),…,We build our VIMA-BENCH simulation suite upon the Ravens physics simulator (Zeng et al., 2020; Shridhar et al., 2021).,To systematically evaluate agents with multimodal prompts, we develop a new benchmark, named VIMA-BENCH, built on the Ravens simulator (Zeng et al., 2020; Shridhar et al., 2021)."
25425e299101b13ec2872417a14f961f4f8aa18e,8d0eeb8aee3ce93c9c04f0662ee058e8eefee6bf,True,"methodology,background","In prior literature (Stepputtis et al., 2020; Dasari & Gupta, 2020; Brunke et al., 2021b), different tasks often require diverse and incompatible interfaces, resulting in siloed robot systems that do not generalize well
across tasks.,They both use the self-attention mechanism with auxiliary losses such as inverse dynamics loss (Dasari & Gupta, 2020) and contrastive loss (Zhao et al., 2022) to learn robot controllers.,…(Bharadhwaj et al., 2021; Srinivasan et al., 2020; Thananjeyan et al., 2021), one-shot imitation (Paine et al., 2018; Huang et al., 2019; Dasari & Gupta, 2020; Aceituno et al., 2021; Zhao et al., 2022), rearrangement (Weihs et al., 2021; Szot et al., 2021; Liu et al., 2021; Ehsani et…,We follow prior works (Finn et al., 2017; Dasari & Gupta, 2020; Duan et al., 2017) to formulate the problem by giving one video demonstration (represented as key frames in prompts), then test the learned imitator’s ability to produce target trajectories.,From the perspective of leveraging transformer as agent architecture, methods such as Dasari & Gupta (2020) and MOSAIC (Zhao et al., 2022) achieve superior performance in one-shot video imitation tasks."
25425e299101b13ec2872417a14f961f4f8aa18e,3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2,True,"methodology,result,background","We inherit the same high-level action space from well-established prior works, such as Transporter (Zeng et al., 2020).,, 2022), while others are from Ravens (Zeng et al., 2020).,Similar to prior work (Zeng et al., 2020; Shridhar et al., 2021), VIMA-BENCH provides scripted oracles to generate successful demonstrations for all tasks.,VIMA-BENCH inherits the same action space from Zeng et al. (2020) and Shridhar et al. (2021), which consists of primitive actions of “pick and place” for tasks with a suction cup as the end effector, or “push” for tasks with a spatula.,We build our benchmark by extending the Ravens robot simulator (Zeng et al., 2020).,We build our VIMA-BENCH simulation suite upon the Ravens physics simulator (Zeng et al., 2020; Shridhar et al., 2021).,We inherit the high-level action space from Zeng et al. (2020), which consists of primitive motor skills like “pick and place” and “wipe”.,Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Wahid, A., Sindhwani, V., and Lee, J. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv: Arxiv-2010.,To systematically evaluate agents with multimodal prompts, we develop a new benchmark, named VIMA-BENCH, built on the Ravens simulator (Zeng et al., 2020; Shridhar et al., 2021).,Ravens (Zeng et al., 2020) and Robosuite (Zhu et al., 2020; Fan et al., 2021) design various tabletop manipulation tasks with realistic robot arms."
60c8d0619481eaafdd1189af610d0e636271fed5,5922f437512158970c417f4413bface021df5f78,True,background,"However, Gato relies on extremely large datasets like 15K episodes for block stacking and 94K episodes for Meta-World [48] tasks.,Even in domains that do not conventionally involve sequence modeling [7, 8], Transformers have been adopted as a general architecture [9].,Our voxel-based approach might complement agents like Gato, which could use our problem formulation for greater efficiency and robustness in 6-DoF manipulation settings.,Agents like Gato [9] and BC-Z [12, 13] have shown impressive multi-task capabilities, but they require several weeks or even months of data collection.,Transformers have also achieved impressive results in multi-domain settings like in Gato [9] where a single Transformer was trained for 16 domains such as captioning, language-grounding, robotic control etc."
60c8d0619481eaafdd1189af610d0e636271fed5,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,methodology","We use CLIP’s [68] language encoder, but any pre-trained language model would suffice [13, 61].,As such, several prior works [77, 13, 78, 79] have used language as medium for planning high-level actions, which can then be executed with pre-trained low-level skills.,Recently, a number of end-to-end approaches [13, 12, 61] have been proposed for conditioning BC agents with language instructions.,Following BC-Z, we use FiLM [73] for conditioning with CLIP [68] language features, but the vision encoders take in RGB-D images instead of just RGB.,Image-BC is an image-to-action agent similar to BC-Z [12].,Agents like Gato [9] and BC-Z [12, 13] have shown impressive multi-task capabilities, but they require several weeks or even months of data collection."
60c8d0619481eaafdd1189af610d0e636271fed5,96a1a24fb75635bd5a27b8e4034d0faef5f99ad5,True,"background,methodology","Future works with PERACT could replace uniform task weighting with Auto-λ for better multi-task performance.,Each datapoint in the demonstration ζ can then be cast as a “predict the next (best) keyframe action” task [14, 64, 65].,In the context of RLBench, Auto-λ [65] presents a multi-task optimization framework that goes beyond uniform task weighting from Section 3.4.,In the context of RLBench, Auto-λ [65] presents a multi-task optimization framework that goes beyond uniform task weighting from Section 3.,Future works, could use dynamic task-weighting methods like Auto-λ [65] for better multi-task optimization."
60c8d0619481eaafdd1189af610d0e636271fed5,1d803f07e4591bd67c358eef715bcd443e821894,True,"background,methodology","This could be addressed in future works by using HG-DAgger style approaches to correct the agent [12].,Recently, a number of end-to-end approaches [13, 12, 61] have been proposed for conditioning BC agents with language instructions.,Following BC-Z, we use FiLM [73] for conditioning with CLIP [68] language features, but the vision encoders take in RGB-D images instead of just RGB.,Image-BC is an image-to-action agent similar to BC-Z [12].,Agents like Gato [9] and BC-Z [12, 13] have shown impressive multi-task capabilities, but they require several weeks or even months of data collection."
60c8d0619481eaafdd1189af610d0e636271fed5,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","PERACT is the only agent that achieves > 70% success, whereas all C2FARM-BC versions perform at chance with ∼ 33%, indicating that the global receptive field of the Transformer is crucial for solving the task.,In contrast, recent methods [16, 24, 25] learn actioncentric representations without any “objectness” assumptions, but they are limited to top-down 2D settings with simple pick-and-place primitives.,Figure 4 reports PERACT and C2FARM-BC agents trained with 100 demonstrations.,Image-BC is disadvantaged with single-view observations and has to learn hand-eye
open drawer
slide block
sweep to dustpan meat off grill
turn tap
put in drawer
close jar
drag stick
stack blocks
Method 10 100 10 100 10 100 10 100 10 100 10 100 10 100 10 100 10 100
Image-BC (CNN) 4 4 4 0 0 0 0 0 20 8 0 8 0 0 0 0 0 0 Image-BC (ViT) 16 0 8 0 8 0 0 0 24 16 0 0 0 0 0 0 0 0 C2FARM-BC 28 20 12 16 4 0 40 20 60 68 12 4 28 24 72 24 4 0 PERACT (w/o Lang) 20 28 8 12 20 16 40 48 36 60 16 16 16 12 48 60 0 0 PERACT 68 80 32 72 72 56 68 84 72 80 16 68 32 60 36 68 12 36
screw bulb
put in safe
place wine
put in cupboard
sort shape
push buttons
insert peg
stack cups
place cups
10 100 10 100 10 100 10 100 10 100 10 100 10 100 10 100 10 100
Image-BC (CNN) 0 0 0 4 0 0 0 0 0 0 4 0 0 0 0 0 0 0 Image-BC (ViT) 0 0 0 0 4 0 4 0 0 0 16 0 0 0 0 0 0 0 C2FARM-BC 12 8 0 12 36 8 4 0 8 8 88 72 0 4 0 0 0 0 PERACT (w/o Lang) 0 24 8 20 8 20 0 0 0 0 60 68 4 0 0 0 0 0 PERACT 28 24 16 44 20 12 0 16 16 20 56 48 4 0 0 0 0 0
Table 1.,Following CLIPort [16], we tried using pre-trained vision features from CLIP [68], instead of raw RGB values, to bootstrap learning and also to improve generalization to unseen objects.,Note that at the finest level, C2FARM-BC has a higher resolution (0.47cm) than PERACT (1cm).,For a number of tasks, C2FARM-BC actually performs worse with more demonstrations, likely due to insufficient capacity.,We also condition it with CLIP [68] language features at the bottleneck like in LingUNets [74, 16].,Su cc es s R at e PerAct C2FARM-BC [16,16] C2FARM-BC [32,32] C2FARM-BC [64,64] C2FARM-BC [16] C2FARM-BC [32] C2FARM-BC [64] C2FARM-BC [88],We study the effectiveness of our problem formulation by benchmarking against two language-conditioned baselines: Image-BC and C2FARM-BC.,ACT is essentially a classifier trained with supervised learning to detect actions akin to prior work like CLIPort [16], except our observations and actions are represented with 3D voxels instead of 2D image pixels.,PERACT outperforms C2FARM-BC [14], the most competitive baseline, with an average improvement of 1.33× with 10 demos and 2.83× with 100 demos.
coordination from scratch.,PERACT outperforms C2FARM-BC in 25/36 evaluations in Table 1 with an average improvement of 1.33× with 10 demonstrations and 2.83× with 100 demonstration.,Our choice of CLIP opens up possibilities for future work to use pre-trained vision features that are aligned with the language for better generalization to unseen semantic categories and instances [16].,C2FARM-BC is the most competitive baseline, but it has a limited receptive field mostly because of the coarse-to-fine-grain scheme and partly due to the convolution-only architecture.,C2FARM-BC is a 3D fully-convolutional network by James et al. [14] that has achieved state-of-the-art results on RLBench tasks.,Another line of work tackles data inefficiency by using pre-trained image representations [16, 35, 36] to bootstrap BC.,For instance, [16, 16] indicates two levels of 16(3) voxel grids at 1m(3) and 0.,We include several versions of C2FARM-BC with different voxelization schemes."
60c8d0619481eaafdd1189af610d0e636271fed5,5c74b1021aebd0575c339ce4dfd47e183009a9c5,True,background,"We also note that the cross-entropy based training method from Section 3.4 is closely related to Energy-Based Models (EBMs) [98, 99].,Future works could look into EBM [99] training and inference methods for better generalization and execution performance.,4 is closely related to Energy-Based Models (EBMs) [98, 99]."
60c8d0619481eaafdd1189af610d0e636271fed5,395a4db5fef867d5bd352585aa00b97004994972,True,"background,methodology","In contrast, recent works in reinforcement-learning like C2FARM [14] construct a voxelized observation and action space to efficiently learn visual representations of 3D actions with 3D ConvNets.,[14], we construct a structured observation and action space through keyframe extraction and voxelization.,The per-voxel features are then used to predict discretized actions [14].,proposed C2FARM [14], an action-centric reinforcement learning (RL) agent with a coarse-to-fine-grain 3D-UNet backbone.,PERACT outperforms C2FARM-BC [14], the most competitive baseline, with an average improvement of 1.,[14], but instead of training it with RL, we do BC with cross-entropy loss (from Section 3.,Success rates of PERACT against various C2FARM-BC [14] baselines Sensitivity Analysis.,Each datapoint in the demonstration ζ can then be cast as a “predict the next (best) keyframe action” task [14, 64, 65].,[14] that has achieved state-of-the-art results on RLBench tasks.,The keyframe actions k are discretized such that training our BC agent can be formulated as a “next best action” classification task [14]."
97e6b89f8f256289b01b9f31799d957db81f2d4e,1c3b3639a335fd0cfbe4a3ebab518561fdef660d,True,"background,methodology","1, we show that using the object’s images or object’s names (as done in [29]) brings equivalent results, since CLIP maps both images and text to a joint latent space.,The work in [29] proposed a transformer based approach for 2D trajectory reshaping on using NL.,Particularly highlight should also be given to the works [28, 29], which are more closely related to our approach."
e62a36ebaf06f46540169c48e6f87a82b1fca24f,826383e18568c9c37b5fc5dd7e2913352db22b47,True,background,"Furthermore, methods that load dissolution models are already deployed on real robots [36, 56, 97, 103, 110].,Despite this toxicity, robotics papers [36, 56, 97, 103, 110] (Sec.,Simple but Effective: CLIP Embeddings for Embodied AI [56] loads clip on an embodied mobile robot for navigating to specific objects within a household as described with language, topping robot navigation leaderboards.,3, is a dissolution model for matching images to captions that the robotics community has found to be particularly appealing [36, 56, 97, 103, 110] across multiple papers: Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement [36] uses CLIP to assist in cropping to specific objects on a tabletop on which to take actions."
e62a36ebaf06f46540169c48e6f87a82b1fca24f,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"methodology,background","Baseline trains one multi-task policy that they train and evaluate on 10 virtual and 9 real physical robot experiments to back up their claim that their method is capable across both environments.,Baseline uses an encoder-decoder network to learn to predict robot actions defined as ‘grasp, move, then release’, with a start and end location, when given a projected overhead RGBD image of the whole workspace and a natural language command such as ‘pack the blue ball in the brown box.’,Baseline augments this architecture with a pretrained, unmodified, and frozen OpenAI CLIP model, inserting the image ‘fingerprint’ (vector) CLIP infers from the natural language command into the decoder network to improve with what objects and where the robot should act.,CLIPort [97] combines CLIP to detect what is present and Transporter Networks [111] to detect where to move for tabletop tasks.,Baseline’s stratification bears a distinct resemblance to harmful patriarchal White supremacist ideologies [53, 54, 63, 113].,2, because identity based stratification in Baseline could lead to identity-based product price discrimination in a packaging or warehousing system.,Baseline’s virtual box-packing experiment evaluates previously “unseen” object models placed on a flat grey surface near a UR5 robot arm (Fig.,Despite this toxicity, robotics papers [36, 56, 97, 103, 110] (Sec.,Baseline controls a robot arm to manipulate various tabletop objects, placing them in bins, rearranging them, stacking them, and other similar tasks.,Baseline was developed independently around the same time as LAION-400M.,We examine the race and gender values encoded in CLIP and integrated into the Baseline robotics algorithm (Sec.,Notably for our experiments, Baseline emphasizes their capability of generalizing to previously unseen cases and loads object models selected from a dataset of household objects with product boxes that contain faces.,3, is a dissolution model for matching images to captions that the robotics community has found to be particularly appealing [36, 56, 97, 103, 110] across multiple papers: Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement [36] uses CLIP to assist in cropping to specific objects on a tabletop on which to take actions.,In this paper, we examine a recently publishedmulti-task languageconditioned imitation-learning algorithm and robotic system,which we call Baseline [97], that uses CLIP to help a robot pattern match scenes and the objects within scenes.,3) by loading Baseline’s primary multi-task model was pretrained on 10 separate tasks in a simulated scene with digital scans of real object models.,Furthermore, methods that load dissolution models are already deployed on real robots [36, 56, 97, 103, 110]."
e62a36ebaf06f46540169c48e6f87a82b1fca24f,36c95e3ef362742a5c1844257e8b79d3251a781e,True,background,"Language Grounding with 3D Objects [103] employs a CLIP backbone across several models to identify objects described with language, enhancing performance with multiple views.,Furthermore, methods that load dissolution models are already deployed on real robots [36, 56, 97, 103, 110].,Despite this toxicity, robotics papers [36, 56, 97, 103, 110] (Sec.,3, is a dissolution model for matching images to captions that the robotics community has found to be particularly appealing [36, 56, 97, 103, 110] across multiple papers: Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement [36] uses CLIP to assist in cropping to specific objects on a tabletop on which to take actions."
3f6d6ed110f3262fdc194184c54dd63701b3bca9,4f68e07c6c3173480053fd52391851d6f80d651b,True,background,"Of particular interest are “foundation models” (Bommasani et al., 2021) – deep neural network models trained on massive internet datasets – that have powered impressive advances in downstream vision and NLP tasks.,Motivated by the advancement in foundation models (Bommasani et al., 2021) that were trained using massive amounts of data from the internet, which contain in part data generated by humans in human-centric environments, we specifically study if foundation models can enable these generic forms of…,, 2021; Izacard and Grave, 2020), and for this reason have been touted as “foundation models” (Bommasani et al., 2021).,…impacted downstream applications in vision (Tian et al., 2020; Conde and Turgutlu, 2021; Patashnik et al., 2021; Frans et al., 2021) and NLP (Lin et al., 2021; Bugliarello et al., 2021; Izacard and Grave, 2020), and for this reason have been touted as “foundation models” (Bommasani et al., 2021).,Motivated by the advancement in foundation models (Bommasani et al., 2021) that were trained using massive amounts of data from the internet, which contain in part data generated by humans in human-centric environments, we specifically study if foundation models can enable these generic forms of goal-conditioning."
1bf46fd55008c3fe2dd531c5cdb97dceafd6b217,4c5d4601a3a19c31da6588d2a34adfb161f68c0e,True,background,"Playroom We next show that pretrained vision-language representations significantly speed up learning across all Playroom tasks (Figure 7).,Although LSENGU and image-based ND agents do not access a language oracle, they are similarly effective as their annotation-dependent counterparts in the Playroom tasks (Appendix Figure S6), suggesting that our method could be robust to the availability of a language oracle.,Playroom Our first domain, Playroom [1, 52], is a randomly-generated house containing everyday household items (e.,To keep the number of experiments tractable, we only perform a full comparison on the Playroom tasks.,We evaluate performance and sample efficiency on object manipulation, search, and navigation tasks in two challenging 3D environments simulated in Unity: Playroom (a house containing toys and furniture) and City (a large-scale urban setting).,Acknowledgments and Disclosure of Funding
We would like to thank Iain Barr for ALM models and Nathaniel Wong and Arthur Brussee for the Playroom environment.,In Playroom, the caption describes if and how the agent
interacts with objects and lists what is currently visible to it.,In contrast to Playroom, City tests long horizon exploration.,A Playroom episode lasts only 600 timesteps, whereas a City episode lasts 11,250 and requires hundreds of timesteps to fully traverse the map even once.,This benefit is seen across on-policy and off-policy algorithms (Impala and R2D2), different exploration methods (RND and NGU), different 3D domains (Playroom and City), and various task specifications (lifting/putting, searching, and intrinsically motivated navigation).,Playroom Our first domain, Playroom [1, 52], is a randomly-generated house containing everyday household items (e.g. bed, bathtub, tables, chairs, toys).,Our results show that language-based exploration with pretrained visionlanguage representations improves sample efficiency on Playroom tasks by 18-70%.,We use Impala [17] on Playroom and R2D2 on City [25].,We study two settings in Playroom.,Q-learning is more suitable for the City, because the action space is more restricted compared to the one needed for Playroom tasks."
69ee9b3a915951cc84b74599a3a2699a66d4004f,89fba716d11de5b29ceb8d137f6b09f05be079fd,True,"background,methodology","One-Stream Image-Goal Transporter is a goal-conditioned version of Transporter [6] which receives a goal-image as input.,On the other hand, end-to-end perception-toaction models can learn precise sequential policies [2, 4, 6, 33, 34, 35], but these methods have limited understanding of semantic concepts and rely on goal-images to condition policies.,As in Transporter [2, 6], our framework can be extended to handle any motion primitive like pushing, sliding, etc.,Switching from packing red pens to blue pens involves collecting a new training set [2], or if using goal-conditioned policies, involves the user providing a goal-image from the scene [5, 6].,Following Transporter [2, 6], the policy π is composed of two action-value modules (Q-functions): The pick module Qpick decides where to pick, and conditioned on this pick action the place module Qplace decides where to place.,Transporter has been applied to a wide range of rearragement tasks from industrial packing [2] to manipulating deformable objects [6].,The implementation follows the goal-conditioned Transporter proposed in [6], except we found that element-wise addition worked better than element-wise product for combining goal-image features with Qplace features."
69ee9b3a915951cc84b74599a3a2699a66d4004f,3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2,True,"background,methodology","To study these benefits, we conduct large-scale experiments in the Ravens [2] framework with a simulated suction-gripper robot.,Specifically, we present CLIPORT, a languageconditioned imitation-learning agent that integrates the semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2].,On the other hand, end-to-end perception-toaction models can learn precise sequential policies [2, 4, 6, 33, 34, 35], but these methods have limited understanding of semantic concepts and rely on goal-images to condition policies.,1 describes the problem formulation, gives an overview of Transporter [2], and presents our language-conditioned model.,Switching from packing red pens to blue pens involves collecting a new training set [2], or if using goal-conditioned policies, involves the user providing a goal-image from the scene [5, 6].,Specifically, we present CLIPORT, a language-conditioned imitationlearning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2].,We conduct large-scale experiments in Ravens [2] on 10 simulated tasks (a-j) with 1000s of unique instances per task.,Transporter has been applied to a wide range of rearragement tasks from industrial packing [2] to manipulating deformable objects [6].,Recently, a number of end-to-end frameworks have been proposed for vision-based manipulation [2, 3, 4, 5].,In summary, our contributions are as follows: • An extended benchmark of language-grounding tasks for manipulation in Ravens [2]."
c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,"background,methodology","Prior work has also found visual representations informed by language, like CLIP [12], to be effective for control [35, 36].,aligned with paired natural language through contrastive learning and has been shown to be useful for some manipulation [35] and navigation tasks [36], ImNet Supervised which uses features pre-trained for ImageNet classification task [2] and has been shown to be effective for reinforcement learning [37], and MoCo (345) [23], which compresses and fuses the third, fourth, and fifth convolutional layers of a ResNet-50 model trained with MoCo [24] on ImageNet, and has been shown to be effective for imitation learning [23].,supervised MS-COCO, supervised ImageNet, or MoCo ImageNet features [34, 35, 36, 37, 23].,Prior works have explored the use of natural language in robot manipulation, primarily as a means of task specification [51, 52, 35, 53] or reward learning [54]."
f69f95835deec7748a688675721b6d581b60d42b,7ca4abace88db259faed67686ed7bba02b46eb82,True,background,"Then, as before, we freeze the fine-tuned representations and train policies on top using LCBC.
Baselines.,In contrast, not only is LIV an effective pre-trained representation for LCBC, it can also be used as a languageconditioned visual reward model that supports autonomous skill acquisition via reinforcement learning (Goyal et al., 2021; Nair et al., 2022a; Mahmoudieh et al., 2022).,We present the LCBC imitation learning hyperparameters in Table 4.,On the FrankaKitchen environment, instead of 50 trajectories per task, we repeat the same fine-tuning+LCBC experiment with just 10 and 25 trajectories per task.,Likewise, we use the pre-trained LIV as the base model since CLIP LCBC performs poorly (Section 5.3).,Most existing works in this area focus on language-conditioned behavior cloning (LCBC) (Lynch & Sermanet, 2020; Stepputtis et al., 2020).,We evaluate LIV’s effectiveness for pre-training (Section 5.3) and fine-tuning (Section 5.4) by using the resulting representations as the vision-language backbone in language-conditioned imitation learning (LCBC) in both simulations and a real robot platform.,At a high level, we perform language-conditioned behavior cloning (LCBC), where a single multi-task policy, which takes concatenated current observation embedding and language task embedding as input, is trained for all tasks within an environment using the given environment dataset."
492987781038027ccf869b6992f48eb14022bac2,bfba05093314e52317536b6cfc8b7fded8371e02,True,"methodology,background","Learned latent actions – and specifcally, the latest work on Language-Informed Latent Actions (LILA) [23] – ofer a compromise: use a small number of task-specifc demonstrations to learn a nonlinear mapping from joystick axes to end-efector control axes, such that each axis of the joystick represents semantically meaningful movement through task space.,To address the sample efciency problem, other work such as LILA [23] have turned to the shared autonomy regime, learning collaborative human-robot policies from orders of magnitude fewer demonstrations.,[23] by incorporating natural language corrections during the course of execution.,We evaluate LILAC via a within-subjects user study (� = 12), where users complete a complex set of manipulation tasks on a Franka Emika Panda arm using LILAC and two baselines – the stateof-the-art language-informed latent actions (LILA) model [23], as well as a fully autonomous language-conditioned imitation learning approach.,To address this, we adopt the insight used in prior work on learned latent actions [23, 24, 32]: frame the training process as learning a state-and-language conditional autoencoder, using compression as a way to induce meaningful latent action control manifolds.,This process, similar to that used in LILA [23] prevents LILAC from degenerating in the presence of slight variations of language, which could lead to practical user safety issues."
492987781038027ccf869b6992f48eb14022bac2,8095bdd5861d1dbe43b77997bc0dbc2fd51acb93,True,"methodology,background","As a result, more recent work in this space learn language-conditioned policies directly via imitation learning from large datasets of paired (language, demonstration) pairs [19, 34, 38, 44, 45].,For each task, we collected dense, human-guided kinesthetic demonstrations – 50 full-task demonstrations total (orders of magnitude fewer demonstrations than what is typically required for fully autonomous instruction following approaches [34, 38]).,With LILAC, we can learn to perform complex manipulation tasks like those in Figure 1 from 10-20 demonstrations instead of the thousands to tens of thousands of demonstrations required by fully autonomous imitation or reinforcement learning approaches[10, 19, 33, 34].,This explicit division of agency between humans and robots places a tremendous burden on learning; existing systems either require large amounts of language-aligned demonstration data to learn policies [10, 34, 44, 45], or make other restrictive assumptions about known environment dynamics, in addition to the ability to perform perfect object localization and afordance prediction to plug into task and motion planners [27, 37]."
67188a50e1d8a601896f1217451b99f646af4ac8,4f68e07c6c3173480053fd52391851d6f80d651b,True,"methodology,background","1 INTRODUCTION In recent years, the literature has seen a series of remarkable Deep Learning (DL) success stories (3), with breakthroughs particularly in the fields of Natural Language Processing (4; 19; 8; 29) and Computer Vision (2; 25; 36; 37).,4 A FRAMEWORK FOR LANGUAGE-CENTRIC AGENTS The goal of this work is to investigate the use of Foundation Models (3), pre-trained on vast image and text datasets, to design a more general and unified RL robotic agent.,The goal of this work is to investigate the use of Foundation Models (3), pre-trained on vast image and text datasets, to design a more general and unified RL robotic agent.,Exploiting the knowledge stored into Foundation Models, can bootstrap this process tremendously.,(3) It executes the list of sub-goals as described in Fig.,The size and abilities of these models led the community to coin the term Foundation Models (3), suggesting how these models can be used as the backbone for downstream applications involving a variety of input modalities.,Through a series of experiments, we demonstrate how this framework, by leveraging the knowledge and capabilities of Foundation Models, can provide a more unified approach with respect to the current literature to tackle a series of core RL challenges, that would normally require separate algorithms and models: 1) exploring in sparse-reward tasks 2) reusing experience data to bootstrap learning of new skills 3) scheduling learned skills to solve novel tasks and 4) learning from observing expert agents.,In the recent literature, these tasks need different, specifically designed algorithms to be tackled individually, while we demonstrate that the capabilities of Foundation Models unlock the possibility of developing a more unified approach."
9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3,c03fa01fbb9c77fe3d10609ba5f1dee33a723867,True,"background,methodology","We compare our methods with other LLM-based planners, including Zero-shot Planner (Huang et al., 2022a), ProgPrompt (Singh et al., 2022), Chain of Thought (Wei et al., 2022), Inner Monologue (Huang et al., 2022b), Code as Policies (Liang et al., 2022).,, 2021) can be used as zero-shot planners to generate sub-goal sequences for various tasks in embodied environments (Huang et al., 2022a; Singh et al., 2022).,…and detailed success rate table of all tasks for different methods in Table 6, including Zero-shot Planner (Huang et al., 2022a), ProgPrompt (Singh et al., 2022), Chain-of-Thought (Wei et al., 2022), Inner Monologue (Huang et al., 2022b), Code as Policies (Liang et al., 2022), and proposed…,, 2022a), ProgPrompt (Singh et al., 2022), Chain of Thought (Wei et al.,Previous works have shown that large language models (LLMs) such as InstructGPT (Ouyang et al., 2022) and Codex (Chen et al., 2021) can be used as zero-shot planners to generate sub-goal sequences for various tasks in embodied environments (Huang et al., 2022a; Singh et al., 2022).,Current LLM-based planners usually query the LLM once at the beginning of every episode and use the output plan throughout the episode (Huang et al., 2022a; Singh et al., 2022).,We report the complete and detailed success rate table of all tasks for different methods in Table 6, including Zero-shot Planner (Huang et al., 2022a), ProgPrompt (Singh et al., 2022), Chain-of-Thought (Wei et al., 2022), Inner Monologue (Huang et al., 2022b), Code as Policies (Liang et al., 2022), and proposed methods (i.e., DEP w/o Selector, and DEPS).,All planner methods access the LLM model through OpenAI API (text-davinci-03 model (Ouyang et al., 2022) for Zero-shot Planner, Chain of Thought, and Inner Monologue, and code-davinci-02 model (Chen et al., 2021) for Code as Policies, ProgPrompt, and Ours).,(Singh et al., 2022) and (Liang et al., 2022) use the pythonic-style prompt to produce more executable plans."
9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"result,background,methodology",", 2022), success detector (Liang et al., 2022) or scene descriptor (Huang et al.,, 2022) and (Liang et al., 2022) use the pythonic-style prompt to produce more executable plans.,, 2022), 2D shape drawing (Liang et al., 2022) and table rearrangement (Huang et al.,For better executing the plan in embodied environments, some methods use an object detector describing the initial environment into the language prompt to produce environment-suitable plans and adopt success detectors to check that each step is executed successfully (Huang et al., 2022b; Liang et al., 2022).,, from success detector or scene descriptor) to reflect on the results of previous executions (Huang et al., 2022b; Liang et al., 2022; Brohan et al., 2022)."
9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"result,background,methodology","To enable such capabilities, earlier works have suggested employing a hierarchical goal execution architecture (Bacon et al., 2017; Brohan et al., 2022), where a planner generates action plans that would then be executed by low-level goal-conditioned controllers.,Prior works enabled feedback to the LLM through external components such as affordance functions (Brohan et al., 2022), success detector (Liang et al.,(2022a) decompose natural language commands into sequences of executable actions by text completion and semantic translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions (Brohan et al., 2022).,The feedback can be practically obtained either by a person (human feedback (Brohan et al., 2022)), or by a pre-trained visionlanguage model CLIP (Radford et al.,, from success detector or scene descriptor) to reflect on the results of previous executions (Huang et al., 2022b; Liang et al., 2022; Brohan et al., 2022).,This architecture has been delivering promising progress in many robotics domains, including table-top and mobile manipulation (Zeng et al., 2022; Brohan et al., 2022), 2D shape drawing (Liang et al."
2d1ad38d83a5b8a6bb47630972ada82b62ea4aac,65fc1f1c567801fee3788974e753cdbf934f07e9,True,background,"VPT [2] labels internet-scale datasets and pre-trains a behavior-cloning agent to initialize for diverse tasks.,Previous works usually build policies in Minecraft upon imitation learning, which requires expert demonstrations [9, 4, 31] or large-scale video datasets [2].,Recent works [4, 31] learn skills based on VPT.,Other works explore multi-task learning [30, 16, 4], unsupervised skill discovery [23], planning [31], and pre-training from videos [2, 8, 6].,In VPT [2], a large policy model is pre-trained on a massive labeled dataset using behavior cloning."
f197bf0fc2f228483f6af3285000d54d8d97f9eb,c03fa01fbb9c77fe3d10609ba5f1dee33a723867,True,"background,methodology","[22] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.,We opt to use code as the action space instead of low-level motor commands because programs can naturally represent temporally extended and compositional actions [16, 22], which are essential for many long-horizon tasks in Minecraft.,Recent advances in large language model (LLM) based agents harness the world knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable policies [16, 22, 19].,Code as Policies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies.,Inner Monologue [26] incorporates environment feedback for robot planning with LLMs. Code as Policies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies."
f197bf0fc2f228483f6af3285000d54d8d97f9eb,41531594d7e0f3b2e138ae43e0a0f6e24a9b014c,True,"background,methodology","[16] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng.,We opt to use code as the action space instead of low-level motor commands because programs can naturally represent temporally extended and compositional actions [16, 22], which are essential for many long-horizon tasks in Minecraft.,Recent advances in large language model (LLM) based agents harness the world knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable policies [16, 22, 19].,Code as Policies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies.,Inner Monologue [26] incorporates environment feedback for robot planning with LLMs. Code as Policies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies."
af6d0ba799213cbbcbfceb1fb9b78d2858486308,bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2,True,"methodology,background","We chose to extend diffusion policy [16], the state-of-the-art approach for single-task behavior cloning, to the multi-task domain by adding language-conditioning.,Our work presents the novel formulation of bringing diffusion-based [44, 45] policies [16] into the language-conditioned [46, 47] visuomotor domain.,To enable effective learning of high entropy, diverse robot trajectories, we extend the diffusion policy formulation [16] to handle language-based conditioning for multi-task learning.,2), we filter this data for success according to the LLM inferred success condition and distill it into a multi-task vision-and-language-conditioned diffusion policy [16].,The second aspect of this question concerns effective learning from the offline data [8], which delves into dataset filtering [9, 10] and exploring effective observation [11], action [12–15], and policy [9, 16] formulations that can robustly model the training data and generalize to novel scenarios."
af6d0ba799213cbbcbfceb1fb9b78d2858486308,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,,"from task and motion planning (TAMP) [29].,Unlike classical TAMP planners, our framework does not require domain-specific engineering and transition function design to work with new tasks.,Another option for the autonomous data collection policy is to use a model-based policy, e.g. task and motion planning (TAMP) [33].,• We are inspired by the effectiveness of robotic planning methods, e.g. TAMP, but wish to be flexible to novel tasks and domains and non-reliant on ground truth state during policy inference."
5922f437512158970c417f4413bface021df5f78,61d4b1bb56322c85e961a0aa8b3eca84b6637982,True,methodology,"In Skill Generalization, for both simulation and real, we use data collected by the best generalist sim2real agent from Lee et al. (2021).,Table 2 shows that our generalist agent’s success rate on each test triplet is comparable to the single task BC-IMP (filtered BC) baseline in Lee et al. (2021).,As a testbed for taking physical actions in the real world, we chose the robotic block stacking environment introduced by Lee et al. (2021).,For Skill Mastery we used data from the best per group experts from Lee et al. (2021) in simulation and from the best sim2real policy on the real robot (amounting to 219k trajectories in total).,We use the sparse reward function described in Lee et al. (2021) for data filtering."
5922f437512158970c417f4413bface021df5f78,4f68e07c6c3173480053fd52391851d6f80d651b,True,background,"However, Bommasani et al. (2021) note that in robotics “the key stumbling block is collecting the right data.,Since our generalist agent can act as a vision-language model, it inherits similar concerns as discussed in (Weidinger et al., 2021; Bommasani et al., 2021; Rae et al., 2021; Alayrac et al., 2022).,Recent position papers advocate for highly generalist models, notably Schmidhuber (2018) proposing one big net for everything, and Bommasani et al. (2021) on foundation models.,…Alayrac et al., 2022), code generation (Chen et al., 2021c; Li et al., 2022b), dialogue systems with retrieval capabilities (Nakano et al., 2021; Thoppilan et al., 2022), speech recognition (Pratap et al., 2020), neural machine translation (Johnson et al., 2019) and more (Bommasani et al., 2021)."
52289f27ef3ce416ae360691d91fc8608f995cc7,826383e18568c9c37b5fc5dd7e2913352db22b47,True,"result,methodology","[36] train separate models for Habitat and RoboTHOR to handle the domain shifts between datasets and further limit their zero-shot testing to four RoboTHOR categories (with training conducted on the other eight RoboTHOR categories).,To further contextualize our results, we compare our CoW models to prior zero-shot object navigation work: the EmbCLIP zero-shot model [36].,Table 3: Comparison to EmbCLIP zero-shot [36].,Inspired by the recent success of frozen CLIP backbones for embodied AI [36], we adopt a frozen CLIP ViT-B/32 backbone as our vision encoder."
52289f27ef3ce416ae360691d91fc8608f995cc7,98789b46f7be983300a0e93e1c53bab56b36efd1,True,"background,methodology",", Habitat [59] and RoboTHOR [17]), an agent could experience drastically different object and layout distributions, visual appearance and style, as well as different noise patterns from sensors and actuators.,We train agents independently in Habitat [59] and RoboTHOR [17] simulation environments for 60M steps each, using DD-PPO [60, 67] and the AllenAct [66] framework.,Since our method is zero-shot, we evaluate CoWs on both the Habitat [5] and RoboTHOR [17] object navigation domains.,We adopt the Habitat-challenge [5] and RoboTHOR-challenge [17] object navigation validation sets as our test sets."
0e34addae55a571d7efd3a5e2543e86dd7d41a83,1d803f07e4591bd67c358eef715bcd443e821894,True,"methodology,background","often requiring manual post-hoc success filtering [9], [10].,Episodic Demonstrations BC-Z [9] 25 0.,prior work in broader AI research [1]–[6], [11] has sought a more convenient form of specification in the form of natural language conditioning (survey [27]), with some results on physical robots [7]–[9], [12].,Although we might expect such a robot to be possible given current methods, natural language-interactable robots are frequently slow in practice, and often use blocking parameterized skills [7], [10] or simplifying self-resetting behaviors [9], [12] that,task imitation learning frameworks determine the set of tasks to be learned upfront [7], [9], [10], [12], [14].,While recent research on this topic has been abundant [2]–[9], few efforts have actually produced a robot that (i) exists in the real world, and (ii) can capably respond to a large number of,our transformer-based policy architecture LAVA against the FiLM-conditioned ResNet architecture in [9] and (ii) the amount of data provided to policy training.,One of the largest bottlenecks in robot imitation is often simply the amount of diverse robot data made available to learning [9], [22], [23].,We compare our LAVA transformer architecture to a baseline ResNet-18 FiLM model from [9], as well as ablate the amount of data provided to training."
0e34addae55a571d7efd3a5e2543e86dd7d41a83,7ca4abace88db259faed67686ed7bba02b46eb82,True,background,"prior work in broader AI research [1]–[6], [11] has sought a more convenient form of specification in the form of natural language conditioning (survey [27]), with some results on physical robots [7]–[9], [12].,Although we might expect such a robot to be possible given current methods, natural language-interactable robots are frequently slow in practice, and often use blocking parameterized skills [7], [10] or simplifying self-resetting behaviors [9], [12] that,feedback, over the more common “open-loop” evaluation setting where the sequence of subgoals is decided up front [10]–[12], [43].,task imitation learning frameworks determine the set of tasks to be learned upfront [7], [9], [10], [12], [14].,conditioning is typically presumed fixed over robot execution [8]– [10], [12], with little opportunity for subsequent interaction by the instructor."
0e34addae55a571d7efd3a5e2543e86dd7d41a83,1301e9d11b728268ed1ff3f1a9adc155308d5250,True,"methodology,background","The purpose of Dcollect is to provide a sufficiently diverse basis for crowdsourced hindsight language relabeling [8], [11], described next.,prior work in broader AI research [1]–[6], [11] has sought a more convenient form of specification in the form of natural language conditioning (survey [27]), with some results on physical robots [7]–[9], [12].,As in prior works [11], we treat natural-languageconditioned visuomotor skill learning as a contextual imitation learning problem [14].,Closest to our approach is [11] and [30], which study language-interactive agents,We cast real time language guidance as a large scale imitation learning problem [11], [13], [14] (Figure 2).,In contrast to the “random window” relabeling explored in [11], we give annotators precise control over the start and end of behaviors they are annotating, which we find in practice better aligns relabeled training data to the actual commands given at test time.,We sidestep both these scaling concerns by instead having operators continuously teleoperate long-horizon behaviors, with no requirements on low level task segmentation or resets [11], [25], [43] and,We expect that future research will continue to produce larger and more diverse sets of behaviors, either by sequencing raw skills together [10] or growing the number of raw skills themselves [11].,then leverage after-the-fact crowdsourced language annotation [8], [11].,Random window [8], [11] 86% 47% 16% Event-selectable (ours) 91% 83% < 1% Real test instructions 89% 84% < 1%"
32c9b3859086d15184989454eb878638659e64c6,65fc1f1c567801fee3788974e753cdbf934f07e9,True,"background,methodology","[10] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.,0 (VPT) [10] Mouse and keyboard control X 5 Labeled contractor data; unlabeled videos scraped from the Internet 2K hours of contractor data; 270K hours of unlabeled videos MarLÖ [87] Cooperative and competitive multiagent tasks; parameterizable environments X 14,VPT [10] is a concurrent work that learns an inverse dynamics model from human contractors to pseudo-label YouTube videos for behavior cloning.,VPT is complementary to our approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned reward model."
32c9b3859086d15184989454eb878638659e64c6,cb5e3f085caefd1f3d5e08637ab55d39e61234fc,True,"background,result,methodology","SayCan [3] leverages large language models (LMs) to ground value functions in the physical world.,Concurrent works [3, 56, 143] have explored similar ideas and showed excellent results on robot learning, which is encouraging for more future research in MINEDOJO.,Inspired by concurrent works SayCan [3] and Socratic Models [143], one potential idea is to feed each step in the guidance to our learned reward model sequentially so that it becomes a stagewise reward function for a complex multi-stage task.,[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan."
4aa88c1406414cda3ce9cf76c8af0abaa8391760,ce10bb0393bd41b7614edfe0f0632b01073dfa94,True,"background,methodology","– Agent action space: Similarily to [100], the navigation is handeled by a discrete action space which is then translated into continuous actions.,[100] Naoki Yokoyama, Sehoon Ha, and Dhruv Batra.,To further investigate the hypothesis that the blind ‘feels its way’ to the goal, we analyze how efficient the two are at picking up objects, using the Success weighted by Completion Time (SCT) metric [100]."
806725595f04849b3b4cc9f6c28d4a84744e8d95,e0d2852be2a3f5952a6600eaa933f23bee448210,True,"methodology,background","In iGibson, we enrich the articulated objects models from the PartNet-Mobility dataset introduced by Sapien with materials and dynamics properties.,0 (ours) Gibson [25] Habitat [18] Sapien [15] AI2Thor [19] VirtualH [20] TDW [26],Similar to iGibson, Sapiens also uses RBP without PA, but with smaller scenes, focusing on interaction with articulated objects.,A variety of simulation environments have been proposed recently for scene-level interactive tasks, such as Sapien [15], AI2Thor [19], VirtualHome [20], and ThreeDWorld (TDW [26]).,The object models are curated from open-source datasets: ShapeNet [21], PartNet Mobility [15, 45], and SketchFab.,open-source datasets [21, 22, 15] enriched with annotations of material and dynamic properties.,set of tasks and include only clean, small-scale scenes [12, 13, 14, 15, 16, 17].,AI2Thor and VirtualHome use predefined actions (PA) as an abstraction of physical interactions and allow agents to
TABLE I: Comparison of Simulation Environments
iGibson 1.0 (ours) Gibson [25] Habitat [18] Sapien [15] AI2Thor [19] VirtualH [20] TDW [26] Provided Large Scenes Real-World / Designed 15 homes (108 rooms) / – 1400 / – – – – / 120 rooms – / 7 – / 25 Provided Objects Number / Materials 570 / Yes – / – – / – 2346 / No 609 / Yes 308 / No 200 / Yes Agent/World Interaction Forces, Predefined Actions F – –"
c305ab1bdba79442bec72ec7f5c5ee7c49c2a566,cdf54c147434c83a4a380916b6c1279b0ca19fc2,True,"background,methodology","• LM-Nav [13] creates a graph where image observations of an environment are stored as nodes while the proximity between images are represented as edges.,, CoW [12] and LM-Nav [13], and, in particular,,Most related to our work is the approach denoted LM-Nav [13], which combines three pre-trained models to navigate via a topological graph in the real world.,LM-Nav [13] 26 4 1 1 26 CoW [12] 42 15 7 3 36 CLIP Map 33 8 2 0 30 VLMaps (ours) 59 34 22 15 59,For LM-Nav, we simply use the same parsing method in the original paper [13] to break down the language instruction into subgoals.,Notably different from prior work [12], [13], VLMaps allow us to reference precise spatial goals such as: “in between the sofa at the TV” or “three meters to the east of the chair.,” Specifically, we use a large language model (LLM) to interpret the input natural language commands and break them down into subgoals [35], [13], [14].,and can be combined with exploration algorithms to search for the first instance of any object (CoW) [12] or traverse objectcentric landmarks in graphs (LM-Nav) [13].,The recent success of large pretrained vision and language models [10], [27] has spurred a flurry of interest in applying their zero-sot capabilities to different domains including object detection and segmentation [28], [29], [11], robot manipulation [30], [31], [32], [33], and navigation [13], [12], [34].,However, both LM-Nav [13] and CoW [12] are limited to navigating to object landmarks and are less capable to understand finer-grained queries, such as “to the left of the chair” and “in between the TV and the sofa”."
c305ab1bdba79442bec72ec7f5c5ee7c49c2a566,52289f27ef3ce416ae360691d91fc8608f995cc7,True,"background,methodology",", CoW [12] and LM-Nav [13], and, in particular,,LM-Nav [13] 26 4 1 1 26 CoW [12] 42 15 7 3 36 CLIP Map 33 8 2 0 30 VLMaps (ours) 59 34 22 15 59,LM-Nav [13] 5 5 0 0 CoW [12] 33 5 0 0 CLIP Map 19 0 0 0 VLMaps (ours) 62 33 14 10,• CLIP on Wheels (CoW) [12] achieves language-based object navigation by building a saliency map for the target category with CLIP and GradCAM [47].,Notably different from prior work [12], [13], VLMaps allow us to reference precise spatial goals such as: “in between the sofa at the TV” or “three meters to the east of the chair.,CoW [12] performs zero-shot languagebased object navigation by combining CLIP-based [10] saliency maps and traditional exploration methods.,and can be combined with exploration algorithms to search for the first instance of any object (CoW) [12] or traverse objectcentric landmarks in graphs (LM-Nav) [13].,The recent success of large pretrained vision and language models [10], [27] has spurred a flurry of interest in applying their zero-sot capabilities to different domains including object detection and segmentation [28], [29], [11], robot manipulation [30], [31], [32], [33], and navigation [13], [12], [34].,However, both LM-Nav [13] and CoW [12] are limited to navigating to object landmarks and are less capable to understand finer-grained queries, such as “to the left of the chair” and “in between the TV and the sofa”."
2e196c3143f8ca8401d9bb6e7510b40ea4ef4cf7,52289f27ef3ce416ae360691d91fc8608f995cc7,True,"methodology,background","The SPL of our approach is 1.5% lower than CoW.,In concurrent work, CLIP-on-Wheels (CoW) [21] uses a gradient-based visualization technique 70 (GradCAM [22]) with CLIP to localize objects in the agent’s observations.,CoW [21] is a zero-shot method that does not require training a navigation policy.,Instead, CoW uses a heuristically defined policy that has no ability to learn about indoor layouts of home environments (e.g., the fact that “stoves” are found in “kitchens” as illustrated in Fig.,In concurrent work, CLIP-on-Wheels (CoW) [21] uses a gradient-based visualization technique (GradCAM [26]) with CLIP to localize objects in the agent’s observations.,In contrast, we demonstrate 72 that learning a navigation policy can substantially outperform the heuristic exploration approach 73 proposed in [21] without using explicit object localization techniques.,However, unlike CoW, our agent navigates without depth observations, which may reduce path efficiency.,In Table 1b we compare with CoW [21] using agent configuration B.,Such scaling is simply not possible with heuristic methods such as CoW because the navigation policy is not learned.,Two recent works [20, 21] directly address our motivation (zero-shot 60 ObjectNav) and are most related.,These results demonstrate that learning a navigation policy improves zero-shot ObjectNav SR over the hand-designed exploration strategy and stopping criteria proposed by CoW.,125 In Table 1b we compare with CoW [21] using configuration B.,2% absolute gain over existing zero-shot methods[21].,However, CoW is able to perform open-world ObjectNav through the use of CLIP visual and text encoders.,We compare 114 with, to the best of our knowledge, the only two existing zero-shot methods for object-goal navigation 115 (ObjectNav): (1) Zero Experience Required (ZER) [20] and(2) CLIP on Wheels (CoW) [21]."
5e2bceb56f116e98baf7e418208057bc0e1c1861,d5c4550d285b57111e52c5956dbc40942d36b117,True,background,"We refer the interested reader to [51] for more details.,The authors thank Tamar Rott Sha-
ham, Chuang Gan, Joanna Materzynska, Songyou Peng, and Toni Rosinol for discussions and useful feedback over the course of this project.,Perhaps the closest approach to ours is semantic abstraction by Ha and Song [51], which also proposes a zeroshot approach to computing 3D-aligned CLIP features."
0af51c6a6041a439796a334f65327a992c8a3e63,22574ad98c141b45d26d66e8e7713bac8a7d6964,True,"methodology,background","Inspired by masked region modeling tasks (Chen et al., 2020b; Shrivastava et al., 2021), we select with %15 probability some objects part of the agent view in a given timestep t and we ask the model to predict their classes.,Similarly, navigation benchmarks incorporate objects as targets in tasks like object navigation (Qi et al., 2020b; Batra et al., 2020b; Kurenkov et al., 2020), and explicitly modeling those objects assists generally at navigation success (Shrivastava et al., 2021; Qi et al., 2020a, 2021).,…to utilize an auxiliary, object-centric navigation prediction loss during joint navigation and manipulation tasks, building on prior work that predicted only the direction of the target object (Storks et al., 2021) or honed in on landmarks during navigation-only tasks (Shrivastava et al., 2021).,, 2020), and explicitly modeling those objects assists generally at navigation success (Shrivastava et al., 2021; Qi et al., 2020a, 2021).,, 2021) or honed in on landmarks during navigation-only tasks (Shrivastava et al., 2021)."
0af51c6a6041a439796a334f65327a992c8a3e63,a9dc44231239ef010dc2617bc4c373c00e4bee72,True,background,"While mapping environments during inference has shown promise on both VLN (Fang et al., 2019; Chen et al., 2021) and ALFRED (Blukis et al.,While mapping environments during inference has shown promise on both VLN (Fang et al., 2019; Chen et al., 2021) and ALFRED (Blukis et al., 2021), we leave the incorporation of mapping to future work.,Rather than processing dense, single vector representations (Shridhar et al., 2020; Singh et al., 2020; Pashevich et al., 2021; Kim et al., 2021; Blukis et al., 2021), EmBERT attends directly over object bounding box predictions embedded with their spatial relations to the agent, inspired by LWIT (Nguyen et al., 2021) and a recurrent VLN BERT model (Hong et al., 2020).,For language supervision, we could train and apply a speaker model for ALFRED to generate additional training data for new expert demonstrations, providing an initial multimodal alignment for EmBERT, a strategy shown effective in VLN tasks (Fried et al., 2018).,The EmBERT architecture is applicable to single-instruction tasks like VLN, as long as auxiliary navigation object targets can be derived from the data as we have done here for ALFRED, by treating the “recipe” of step-by-step instructions as empty.,In this way, EmBERT can be applied to other language-guided tasks such as VLN and Cooperative Vision-andDialog Navigation (Thomason et al., 2019)."
15ac70d077bb735eed4a8502ce49aa7782c803fd,1d803f07e4591bd67c358eef715bcd443e821894,True,"background,methodology","Moreover, BC-Z leverages expert trajectories and task labels, and MIA includes mobile navigation, making them difficult to implement directly in CALVIN, which contains unlabeled play data on different tabletop environments.,For example, the methods BC-Z and MIA [7], [8] use both behavior cloning, but different actions spaces and multi-modal alignment losses,,Concretely, we compare our contrastive loss against predicting the language embedding from the sequence’s visual observations with a cosine loss [7], cross-modality matching [8] and not having an auxiliary visuo-lingual alignment loss.,There have been a number of multi-modal alignment losses proposed, such as regressing the language embedding from the visual observation [7] or cross-modality matching [8].,For example, the methods BC-Z and MIA [7], [8] use both behavior cloning, but different actions spaces and multi-modal alignment losses, such as regressing the language embedding from visual observations [7] or cross-modality matching [8].,such as regressing the language embedding from visual observations [7] or cross-modality matching [8]."
15ac70d077bb735eed4a8502ce49aa7782c803fd,5a980aa843e40de5f91a243cbf680af273c797ba,True,"background,methodology","For example, the methods BC-Z and MIA [7], [8] use both behavior cloning, but different actions spaces and multi-modal alignment losses,,Concretely, we compare our contrastive loss against predicting the language embedding from the sequence’s visual observations with a cosine loss [7], cross-modality matching [8] and not having an auxiliary visuo-lingual alignment loss.,ronments via imitation learning [5]–[8] or reinforcement learning [9], [10].,There have been a number of multi-modal alignment losses proposed, such as regressing the language embedding from the visual observation [7] or cross-modality matching [8].,such as regressing the language embedding from visual observations [7] or cross-modality matching [8]."
15ac70d077bb735eed4a8502ce49aa7782c803fd,4be02694125b71876552900a53c85c47a2a83614,True,"background,methodology","recently proposed CALVIN benchmark [14] to further our understanding and provide a unified framework for longhorizon language conditioned policy learning.,In CALVIN [14] the action space A consists of the 7-DoF control of a Franka Emika Panda robot arm with a parallel gripper.,Our model sets a new state of the art on the challenging CALVIN benchmark [14], on learning a single 7-DoF policy that can perform longhorizon manipulation tasks in a 3D environment, directly from images, and only specified with natural language."
15ac70d077bb735eed4a8502ce49aa7782c803fd,1301e9d11b728268ed1ff3f1a9adc155308d5250,True,"background,methodology","MCIL [5] uses global actions learned from a single static RGB camera.,Most related to our approach is multi-context imitiation learning (MCIL) [5], which also uses relabeled imitation learning to distill reusable behaviors into a goal-reaching policy.,In order to analyze the big performance difference with respect to the original MCIL baseline, we train a MCIL baseline with relative actions and observe that its performance improves significantly from the original MCIL baseline with absolute actions, but performs worse than our models.,Finally, we evaluate replacing the transformer encoder in the posterior with a GRU bidirectional recurrent network of the same hidden dimension of 2048, similar to MCIL.,[5] showed that pairing a small number of random windows with language after-thefact instructions enables learning a single language conditioned,More recently, end-to-end deep learning has been used to condition agents on natural language instructions [5]–[10], which are then trained under an imitation or reinforcement learning objective.,ronments via imitation learning [5]–[8] or reinforcement learning [9], [10].,To better compare the differences between approaches, we use the same convolutional encoders as the MCIL baseline available in CALVIN for processing the images of the static and gripper camera.,We observe that the performance for 5-chain evaluation drops from 28.3% to 23.6% when we train our model with a diagonal Gaussian distribution as in MCIL.,We base our model on MCIL [5] and improve it by decomposing control into a hierarchical approach of generating global plans with a static camera and learning local policies with a gripper camera conditioned on the plan.,(MCIL) [5], which also uses relabeled imitation learning to distill reusable behaviors into a goal-reaching policy.,Additionally, we compare against a goal-conditioned Behavior Cloning (GCBC) baseline [1] which does not condition the policy on a latent plan, and observe that it performs worse than MCIL with relative actions, highlighting the importance of modeling latent behaviors in free-form imitation datasets.,the plan encodings and conditioning the policy, as many works report improved performance from it [1], [2], [5].,On the zero-shot split, which consists on training on three environments and testing on an unseen environment with unseen instructions, we observe that despite modest improvements over the MCIL baseline, the policy achieves just an average sequence length of 0.67.,We model the interactive agent with a general-purpose goal-reaching policy based on multi-context imitation learning (MCIL) from play data [5].,Concretely, given multiple contextual imitation datasetsD = {D0, D1, . . . , DK}, with a different way of describing tasks, MCIL trains a single latent goal conditioned policy πθ(at | st, z) over all datasets simultaneously, as well as one parameterized encoder per dataset.,MCIL [5] uses bidirectional recurrent neural networks (RNN) to encode a randomly sampled play sequence and map it into a latent Gaussian distribution."
99d95b67e34138f006ace406f4e53a97bbd37431,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,background,"TAMP is a broad area of study that looks at integrating discrete high-level planning (which objects to grab, which actions to execute) with continuous low level planning (which positions in which to place objects) [7].,motion planning (TAMP) [7], these often come with a wide,Rearrangement planning is a common subset of task and motion planning [3, 15, 21, 16, 7].,While many solutions exist that can solve these problems, particularly throughout the sub-field of task-andmotion planning (TAMP) [7], these often come with a wide
1Please refer to our supplementary video: https://youtu.be/CJb1IzH94eo
range of significant limitations that restrict their utility in the real world.,Perhaps the most relevant area of work to rearrangement planning is the field of task and motion planning (TAMP)."
99d95b67e34138f006ace406f4e53a97bbd37431,2d1a894f4ba4090c993acb76ed333846c686345a,True,"background,methodology","For the given observations, X(t) and X(T ), our method begins by computing their instance segmentation K using UCN [34].,We use UCN [34] to segment out unique objects in the scene, and then compute latent embeddings w for each object alignment in the current and target observation for scene graph generation.,We can also more accurately segment unknown objects from the world, giving us ways to identify and pick up objects that we have never seen before [34, 35].,We use Unknown Objects Instance Segmentation (UCN) [34] to extract specific object’s RGB-information from the given scene and it’s corresponding point clouds Xi ⊂ X , where i denotes the selected object’s index.,We use Unknown Objects Instance Segmentation (UCN) [34] to extract specific object’s RGB-information from the given scene and it’s corresponding point clouds X ⊂ X , where i denotes the selected object’s index."
efb3b4550b7308ddeb2f382d8c1439e6ec7a99ec,30432eb1deae9c35ff855e8d1422e14361fb123c,True,"result,methodology,background","INGRESS-POMDP extends FETCH-POMDP by replacing the fixed questions with the referring expressions (generated by pre-trained language generators) for each candidate object.,Having O(n) actions at each tree node, both FETCH-POMDP and INGRESS-POMDP have O(nd) complexity, where n is the number of objects and d is the tree depth.,In [22], [23], INGRESS-POMDP is applied for disambiguation in addition to their matching modules.,As a result, these two POMDP approaches have to exclude other candidate objects by either only modeling yes/no responses (any correcting response is discarded) [22] or sampling a single word from the whole dictionary [21].,Due to the influence of domain gaps on belief initilization and actions, the baseline planners FETCH-POMDP and INGRESS-POMDP show performance declines as well.,3) FETCHPOMDP [21] and 4) INGRESS-POMDP [22] are other variants of POMDP (see Sec.,INGRESS-POMDP extends FETCH-POMDP with spatial-semantic (e.g., red, middle, etc.) questions about candidate objects.,2) Observation Model: While there are no restrictions on language expressions in our system, for efficiency, we sample question-related words1 during planning, contrary to the vocabulary-level sampling in [21] or the non-sampling in [22]."
3396609b96dd24cac3b1542aec686ce362f32fe2,979810ca765695a481c37126103b8ba256ee2192,True,"background,result,methodology","We do not have access to the compute resources to train models of the same scale and data used in prior work [65, 58].,recently, multiple approaches have shown increased dividends in applying such representations to visuomotor control, for example by combining features at different layers of pretrained ResNets [62] or by pretraining such representations on human videos, conjecturing that such data captures features useful for robotic manipulation [58, 97, 65, 53].,approaches however is a notion of semantics; works such as MVP [97, 65] purely learn to perform masked reconstruction, and even works that leverage some temporal and linguistic signals do so in a limited way [58, 53].,Imitation learning for visuomotor control has been the de-facto evaluation for prior work [62, 58, 65], giving us the closest comparison to the evaluations used in MVP and R3M.,Towards this goal, recent work in robotics present approaches for learning visual representations to bootstrap learning for visuomotor control [62, 58, 65]."
3396609b96dd24cac3b1542aec686ce362f32fe2,d28b9f65c849eba9ba2b27f7e91906f46fbe7fa1,True,"background,methodology","14 presents additional intent scoring qualitative visualizations for two other tasks from the WHiRL dataset [5] – specifically “lifting the lid off a pot” and “stacking cups.”,We provide more examples from WHiRL in §C-E, and additional evaluation details in §E.
VI. ABLATIONS, EXTENSIONS, & FURTHER ANALYSIS
The comparative results across the various evaluation problem domains paint Voltron’s language-driven representations in a favorable light relative to MVP and R3M baselines.,We use videos from WHiRL [5] of humans and robots performing the same tasks from different views; we choose to evaluate intent scoring for both agents to better capture the robustness and transfer potential for these approaches in similar real-world settings.,This evaluation is motivated by two active areas of research: reward learning from language and demonstrations [83, 77, 13, 5], and belief modeling for human-robot collaboration [31, 27, 6].,§C-E – Qualitative: Additional Intent Scoring Visualizations
Additional intent scoring visualizations using videos from the WHiRL dataset [5].,Grasp §V-A Single Frame 1470 V – Cond R-MVP Referring Expressions §V-B Single Frame, Language Expression 259,839 V – Cond R-R3M (ViT) Single-Task Control §V-C Frame History n ∈ [5, 10, 25] Demos V – Dual R-R3M (RN-50) Language-Conditioned Imitation §V-D Frame History, Language Instruction 100 = 5 x 20 Demos V – Dual / V – Gen R-R3M (ViT) Intent Scoring §V-E Frame History, Language Intent N/A (Zero-Shot) V – Gen N/A,Given a pair of videos from the WHiRL dataset [5] of a human and robot performing a task, we evaluate the ability of V – Gen, R3M (from Nair et al.,We download videos from the WHiRL dataset off of the WHiRL website: https://human2robot.github.io/."
3396609b96dd24cac3b1542aec686ce362f32fe2,c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,True,"background,result,methodology","We do not have access to the compute resources to train models of the same scale and data used in prior work [65, 58].,recently, multiple approaches have shown increased dividends in applying such representations to visuomotor control, for example by combining features at different layers of pretrained ResNets [62] or by pretraining such representations on human videos, conjecturing that such data captures features useful for robotic manipulation [58, 97, 65, 53].,approaches however is a notion of semantics; works such as MVP [97, 65] purely learn to perform masked reconstruction, and even works that leverage some temporal and linguistic signals do so in a limited way [58, 53].,Imitation learning for visuomotor control has been the de-facto evaluation for prior work [62, 58, 65], giving us the closest comparison to the evaluations used in MVP and R3M.,Towards this goal, recent work in robotics present approaches for learning visual representations to bootstrap learning for visuomotor control [62, 58, 65].,, “the blue coffee mug to the left of the plate”) in cluttered scenes [94], 3) imitation learning for visuomotor control (in simulation) [58], 4) learning multi-task language-conditioned policies for real-world manipulation [86] (on a real-world Franka Emika fixed-arm manipulator), and 5) zero-shot intent scoring [38, 13].,[58]) and CLIP in scoring various frames subject to the utterance “opening the faucet."
3396609b96dd24cac3b1542aec686ce362f32fe2,6b7d21cf7355e08d53cb2ad17c144002fca3f2c8,True,methodology,"We adopt the keyframe-based action space proposed in James and Davison [36] for learning.,[37] Stephen James, Kentaro Wada, Tristan Laidlow, and Andrew J. Davison.,[36] Stephen James and Andrew J. Davison."
3396609b96dd24cac3b1542aec686ce362f32fe2,4ae70ed20691908996c6b16bb74f47923c938780,True,"background,methodology","We use the OCID-Ref Dataset [94] grounded in scenes that are representative of robotics settings; other datasets such as RefCoCo [100] are grounded in more global scenes (e.g., multiple humans playing frisbee on a field) that are less informative for robot learning.,We use the OCID-Ref Dataset [94] grounded in scenes that are representative of robotics settings; other datasets such as RefCoCo [100] are grounded in more global scenes (e.,4: Referring Expression Grounding (Object Detection) from the OCID-Ref Dataset [94].,, “the blue coffee mug to the left of the plate”) in cluttered scenes [94], 3) imitation learning for visuomotor control (in simulation) [58], 4) learning multi-task language-conditioned policies for real-world manipulation [86] (on a real-world Franka Emika fixed-arm manipulator), and 5) zero-shot intent scoring [38, 13]."
20e6909ce6c5f12b61e5c9022d97134137360273,2e08702b428d7948052a05ccf20ed0ecb261aacc,True,methodology,"A.1.3 Visual Dynamics Model (Real Robot)
On the real robot setup we leverage a pre-trained visual dynamics model on the robot desk setup using the GHVAE [72] architecture, that trains a VAE, encodes each of the 4 camera views, and learns a predictive model in the latent space.,On the real robot setup we leverage a pre-trained visual dynamics model on the robot desk setup using the GHVAE [74] architecture, that trains a VAE, encodes each of the 4 camera views, and learns a predictive model in the latent space.,We leverage off-the-shelf action-conditioned video prediction frameworks for learning this model [73, 74], which we describe in detail in the supplement."
20e6909ce6c5f12b61e5c9022d97134137360273,243467598d7db59aa7467763f5863a0f1ae0b6b4,True,"background,result","Reward functions trained using classifiers have been shown to be prone to over-fitting creating a sparse or incorrect reward signal [53].,Unlike these works, we don’t make any assumptions about the optimality of the actions in the collected data, allowing the agent to learn from broader offline datasets, including autonomously collected data, which can be considerably easier to collect at scale on a real robot than human tele-operation data [52, 53].,Note that since there may be instructions l′ 6= l which describe the same task, this may occasionally yield false negatives, however like prior work [68, 53] we find that we can learn an effective reward despite noisy negatives.,Lastly, unlike other works which use classifiers for single-task reward learning [68, 69, 53] on robots, a languageconditioned reward classifier can flexibly represent many tasks with an easy to provide form of task-specification."
20e6909ce6c5f12b61e5c9022d97134137360273,7ca4abace88db259faed67686ed7bba02b46eb82,True,"background,methodology","Other works have studied using offline data in the form of demonstrations [7] or human teleoperated trajectories (i.,Recent works have made progress towards learning such grounding by annotating data collected by humans [5, 6, 7]; however, collecting many human teleoperated trajectories on real robots can be costly and time consuming, and is thus difficult to scale to a broad set of language conditioned behaviors.,We compare LORL (Ours) to language-conditioned behavior-cloning (LCBC), which imitates the behavior in the offline dataset conditioned on the language instruction label, which is reflective of prior works that use imitation learning to learn language-conditioned behavior [6, 7].,More recently, end-to-end deep learning has been used to condition agents on natural language instructions [37, 24, 38, 27, 39, 6, 7, 40], which are then trained under an imitation and/or reinforcement learning objective."
20e6909ce6c5f12b61e5c9022d97134137360273,8095bdd5861d1dbe43b77997bc0dbc2fd51acb93,True,"background,result,methodology","Furthermore, we compare performance with and without using the pre-trained language model, and observe that without the pre-trained language model performance is worse on seen instructions and drops significantly more on unseen instructions (up to 23% vs 10%), suggesting that the pre-trained language model is essential to learning and generalization, consistent with results in prior work on language-conditioned imitation [6].,We compare LORL (Ours) to language-conditioned behavior-cloning (LCBC), which imitates the behavior in the offline dataset conditioned on the language instruction label, which is reflective of prior works that use imitation learning to learn language-conditioned behavior [6, 7].,“play data”) [6], to learn language-conditioned robotic agents in simulation.,Recent works have made progress towards learning such grounding by annotating data collected by humans [5, 6, 7]; however, collecting many human teleoperated trajectories on real robots can be costly and time consuming, and is thus difficult to scale to a broad set of language conditioned behaviors.,Additionally, we observe that by virtue of leveraging pretrained language models our learned reward is capable of generalizing from scripted language instructions to unseen natural language zero-shot, suggesting that knowledge in pretrained language models can enable more efficient learning of grounded language as observed in prior work [6, 8].,More recently, end-to-end deep learning has been used to condition agents on natural language instructions [37, 24, 38, 27, 39, 6, 7, 40], which are then trained under an imitation and/or reinforcement learning objective.,Most related is Lynch and Sermanet [6] who also use crowd-sourcing to annotate play data with natural language instructions."
f27e8c4731c575bd5f5db4c93ad8588f684dcbd0,15fa42c1a2cc0f2df3cfd8f668958688a4b7afb6,True,"background,methodology","This is a crucial problem since most of the entries of the attention matrices in Transformers’ models are very ∗Equal Contribution, Correspondence to kchoro@google.com.,…the critical observation that in several applications of the softmax kernel estimation, e.g. efficient softmax sampling or linear-attention Transformers (Choromanski et al., 2021b), small relative errors are a much more meaningful measure of the quality of the method than small absolute errors.,This definition captures the critical observation that in several applications of the softmax kernel estimation, e.g. efficient softmax sampling or linear-attention Transformers (Choromanski et al., 2021b), small relative errors are a much more meaningful measure of the quality of the method than small absolute errors.,An important application, where random feature map computations takes substantial time of the overall compute time are implicit-attention Transformers, such as Performers (Choromanski et al., 2021b).,The new application of random features for softmax kernel in Transformers proposed in (Choromanski et al., 2020; 2021b) led to fruitful research on the extensions and limitations of these methods.,In the former, HRFs are applied as a replacement of the default mechanism using positive random features in the class of implicit-attention architectures for vision processing called Implicit Attention Policies (or IAPs) (Choromanski et al., 2021a).,Otherwise the renormalizers computed to make attention matrices row-stochastic (standard attention normalization procedure in Transformers) would be estimated imprecisely, potentially even by negative values.,The above (common in most downstream use-cases) method for linearizing softmax/Gaussian kernels was recently shown to fail in some of the new impactful applications of scalable kernel methods such as implicit-attention Transformer architectures called Performers (Choromanski et al., 2021b).,We also demonstrated their robustness in a wide range of applications from softmax sampling to Transformers/attention training, also for downstream Robotics tasks.,The following result from (Choromanski et al., 2021b) shows that the mean squared error (MSE) of the trigonometric estimator is small for large softmax kernel values and large for small softmax kernel values, whereas an estimator applying positive random features behaves in the opposite way.,(3) Even though, very accurate in estimating small softmax kernel values (which turns out to be crucial in making RFs work for Transformers training), this mechanism is characterized by larger MSE for large kernel values.,…us to develop HRFs: (a) provide a unifying perspective, where trigonometric random features from Bochner’s Theorem and novel mechanisms proposed in (Choromanski et al., 2021b) are just special corollaries of the more general result from complex analysis, (b) integrate in the original way several…,We put implicit Performer’s attention into 17-layer Conformer-Transducer encoder (Gulati et al., 2020) and compared softmax kernel estimators in terms of word error rate (WER) metric, commonly used to evaluate speech models.,In several applications of softmax kernels (in particular Transformers, where attention matrices typically admit sparse combinatorial structure with relatively few but critical large entries and several close-to-zero ones or softmax sampling) the algorithm needs to process simultaneously very small and large kernel values.,The solution to this problem was presented in FAVOR+ mechanism (Choromanski et al., 2021b), where a new positive random feature map for unbiased softmax kernel estimation was applied:
φ++m (u) = 1√ 2m exp(−‖u‖ 2 2 ) ( exp(ω>1 u), ..., exp(ω > mu), exp(−ω>1 u), ..., exp(−ω>mu) )> .,We use Implicit Attention Policy (IAP) architecture (masking variant) described in Choromanski et al. (2021a) which uses Performer-based attention mechanism to process 32× 24 depth images from the 2 cameras.,Luo et al. (2021) observed that combining L2-normalization of queries and keys for variance reduction of softmax kernel estimation with FFT-based implementations of relative position encoding and FAVOR+ mechanism from Performers helps in training."
f02577b75226e60440a506ca79f40d8adfb533f3,4aa88c1406414cda3ce9cf76c8af0abaa8391760,True,"background,methodology","Next, we use VER to study the recently introduced (and more challenging) GeometricGoal rearrangement rearrangement tasks [Batra et al., 2020a] in Habitat 2.0 [Szot et al., 2021].,Our main application result is trained using the ReplicaCAD dataset [Szot et al., 2021], which is limited to only US apartments, and this may have negative societal impacts for deployed assistants.,This is more realistic and enables physics and rendering to be overlapped [Szot et al., 2021].,However, by enabling navigation and allowing the agent to learn how to (not) use it, we arrived upon emergent navigation and improved HAB performance.,Next, we evaluate VER on the recently introduced (and significantly more challenging) GeometricGoal rearrangement tasks [Batra et al., 2020a] in Habitat 2.0 [Szot et al., 2021].,We use the Home Assistant Benchmark (HAB) which consists of 3 scenarios of increasing difficulty: Tidy House, Prepare Groceries, and Set Table.,We use the Habitat simulator with the ReplicaCAD Dataset [Savva et al., 2019, Szot et al., 2021].,This result, and higher performance on HAB, highlights that it may not always be beneficial to remove ‘unneeded’ actions.,ReplicaCAD [Szot et al., 2021] – Creative Commons Attribution 4.0 International (CC BY 4.0) license.,We examine the performance of TP-SRL on the Home Assistant Benchmark (HAB) [Szot et al., 2021].,While these systems offer impressive performance, none currently support a benchmark like HAB (which combines physics and photo-realism) nor the flexibility of Habitat, AI2Thor [Kolve et al., 2017], or ThreeDWorld [Gan et al., 2020].,VER is designed with recurrent policies in mind because memory is key in long-range and partially observable tasks like HAB."
f02577b75226e60440a506ca79f40d8adfb533f3,efadee37d835d0352d7677da1dcd02ba2e6522b7,True,methodology,"AsyncOnRL methods provide high-throughput on-policy reinforcement learning [Espeholt et al., 2018, Petrenko et al., 2020].,VER is as fast as SampleFactory [Petrenko et al., 2020], the state-of-the-art AsyncOnRL, with the same sample efficiency.,This experience collection technique is similar to that of HTS-RL [Liu et al., 2020] (SyncOnRL) and SampleFactory [Petrenko et al., 2020] (AsyncOnRL).,On 1 GPU, VER is as fast as SampleFactory [Petrenko et al., 2020], the fastest single machine AsyncOnRL.,, 2020] (SyncOnRL) and SampleFactory [Petrenko et al., 2020] (AsyncOnRL)."
979810ca765695a481c37126103b8ba256ee2192,523acd658742fb9c978e3f7638c09d7ce78af719,True,"result,background,methodology","5x) from ViT-S to ViT-B, while keeping the data size fixed (HOI image collection [10]), does not increase performance and even hurts.,We build on our MVP pipeline [10] and freeze the image encoder throughout the policy learning, which prevents large pre-trained encoders from overfitting to a specific setting or task, and greatly reduces GPU memory footprint and training time.,The training recipe closely follows [16], with dataset specific settings from [10].,Note that [10] only uses the 700k HoI images, excluding the Ego4D and ImageNet.,This is consistent with the in-simulation results reported in [10].,In our recent work [10], we have shown that this recipe for self-supervised visual pre-training is effective for motor control in simulation.,We combine the Ego4D data with the ImageNet [11], as well as the Hand-object Interaction (HoI) data used in [10], which comprises of the egocentric Epic Kitchens [17] dataset, the YouTube 100 Days of Hands dataset [13], and the crowd-sourced Something-Something dataset [12].,While the MAE-trained ViT models yield improving performance in vision tasks as model sizes grow [9, 16, 46], previous work [10] does not show improvement from switching a ViT-Small model to the ViT-Base counterpart of 4x as many parameters.,Simple and free from dataset or task-specific augmentations [41], MAE is the state-of-the-art self-supervised framework in computer vision [42, 43, 44, 45], and has been demonstrated to work well for motor control tasks in simulation as well [10].,We also see that our smallest ViT-S model from [10] outperforms it as well (68."
3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3,cb3631f12b4465f4396380b61a651f0c74763480,True,background,"This section is adapted from Ma et al. (2022b).,Whereas Ma et al. (2022b) assumes access to the true state information and focuses on the offline GCRL setting using in-domain offline data with robot action labels, we extend the dual objective to enable out-of-domain, action-free pre-training from human videos.,Our work is also closely related to Ma et al. (2022b), which first introduced the dual offline GCRL objective based on Fenchel duality (Rockafellar, 1970; Nachum & Dai, 2020; Ma et al., 2022a).,Then, applying Proposition 4.2 of Ma et al. (2022b) to the inner optimization problem, we immediately obtain
max φ min V
Ep(g) [ (1− γ)Eµ0(o;g)[V (φ(o);φ(g))]
(D) + logEdD(φ(o),a;φ(g)) [ exp ( r(o, g) + γET (o′|o,a)[V (φ(o′);φ(g))]− V (φ(o), φ(g)) )]] (17)
Now, given our assumption that the…,Our particular dual objective also admits a novel implicit time contrastive learning interpretation, which simplifies VIP’s practical implementation by letting the value function be implicitly defined instead of a deep neural network as in Ma et al. (2022b)."
3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3,c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204,True,"methodology,background","We follow the training and evaluation protocol of (Nair et al., 2022) and consider 12 tasks combined from FrankaKitchen, MetaWorld (Yu et al., 2020), and Adroit (Rajeswaran et al., 2017), 3 camera views for each task, and 3 demonstration dataset sizes, and report the aggregate average maximum…,In this work, we choose the common choice of the negative L2 distance used in prior work Sermanet et al. (2018); Nair et al. (2022): V ∗(φ(o), φ(g)) := −‖φ(o)− φ(g)‖2.,The closest work to ours is R3M Nair et al. (2022), which is also pre-trained on the Ego4D dataset and attempts to capture temporal information in the videos by using time-contrastive learning (Sermanet et al., 2018); whereas VIP is fully self-supervised, R3M additionally requires video textual…,R O
] 7
M ar
2 02
visual imitation learning (IL) from demonstrations (Parisi et al., 2022; Nair et al., 2022).,, 2022) has emerged as an effective solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain representation learning because it does not require any intensive task-specific data collection or representation fine-tuning, and a single fixed representation can be used for a variety of unseen robotic domains and tasks (Parisi et al.,These choices are identical to a prior work (Nair et al., 2022); additionally, we use the exact same hyperparameters (e.g., batch size, optimizer, learning rate) as in Nair et al. (2022).,visual imitation learning (IL) from demonstrations (Parisi et al., 2022; Nair et al., 2022).,Our R3M-BC, though able to solve some of the simpler tasks, appears to perform relatively worse than the original R3M-BC in Nair et al. (2022) on their real-world tasks.,We train all policies using the same set of hyperparameters used for real-world BC training in Nair et al. (2022), and evaluate each method on 10 rollouts, covering the distribution of object’s initial placement in the dataset.,These choices are identical to a prior work (Nair et al., 2022); additionally, we use the exact same hyperparameters (e.,The closest comparison is R3M (Nair et al., 2022), which pre-trains on the same Ego4D dataset using a combination of time contrastive learning, L1 weight regularization, and language embedding consistency losses.,…emerged as an effective solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain representation learning because it does not…,We follow the training and evaluation protocol of (Nair et al., 2022) and consider 12 tasks combined from FrankaKitchen, MetaWorld (Yu et al."
3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3,523acd658742fb9c978e3f7638c09d7ce78af719,True,background,"Prior pre-trained representations for control demonstrate results in only visual reinforcement learning (RL) in simulation, assuming access to a well-shaped dense reward function (Shah & Kumar, 2021; Xiao et al., 2022), or ∗Corresponding author: jasonyma@seas.,…solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain representation learning because it does not require any…,, 2022) has emerged as an effective solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain representation learning because it does not require any intensive task-specific data collection or representation fine-tuning, and a single fixed representation can be used for a variety of unseen robotic domains and tasks (Parisi et al.,Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.,Prior pre-trained representations for control demonstrate results in only visual reinforcement learning (RL) in simulation, assuming access to a well-shaped dense reward function (Shah & Kumar, 2021; Xiao et al., 2022), or
∗Corresponding author: jasonyma@seas.upenn.edu."
3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3,9f2e34581ca03160e8fd8b770203a5e7c2a902d3,True,"methodology,background","We consider the problem setting of pre-training a frozen visual encoder for downstream control 328 tasks [11, 12, 4].,Furthermore, offline RL algorithms are significantly more difficult to implement and tune compared to BC (Kumar et al., 2021; Zhang & Jiang, 2021).,…on ImageNet, and CLIP Radford et al. (2021), covering a wide range of preexisting visual representations that have been
used for robotics control (Shah & Kumar, 2021; Parisi et al., 2022; Cui et al., 2022), though none has been tested in our three reward-based settings in which the reward also…,Prior pre-trained representations for control demonstrate results in only visual reinforcement learning (RL) in simulation, assuming access to a well-shaped dense reward function (Shah & Kumar, 2021; Xiao et al., 2022), or
∗Corresponding author: jasonyma@seas.upenn.edu.,We also consider a self-supervised ResNet50 network trained on ImageNet using Momentum Contrastive (MoCo), a supervised ResNet50 network trained on ImageNet, and CLIP Radford et al. (2021), covering a wide range of preexisting visual representations that have been
used for robotics control (Shah & Kumar, 2021; Parisi et al., 2022; Cui et al., 2022), though none has been tested in our three reward-based settings in which the reward also has to be produced by the representation.,Shah & Kumar (2021) demonstrates that pre-trained ResNet (He et al., 2016) representation on ImageNet (Deng et al., 2009) serves as effective visual backbone for simulated dexterous manipulation RL tasks.,Shah and Kumar [11] demonstrates that pre-trained 391 ResNet [18] representation on ImageNet [19] serves as effective visual backbone for simulated 392 dexterous manipulation RL tasks.,Inspired by pre-training successes in computer vision (CV) (He et al., 2020; 2022) and natural language processing (NLP) (Devlin et al., 2018; Radford et al., 2019; 2021), pre-training visual representations on out-of-domain natural and human data (Deng et al., 2009; Grauman et al., 2022) has emerged as an effective solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain representation learning because it does not require any intensive task-specific data collection or representation fine-tuning, and a single fixed representation can be used for a variety of unseen robotic domains and tasks (Parisi et al., 2022).,This intuition is theoretically proven (Kumar et al., 2022) and holds even if the offline data consists of solely expert demonstrations, though not validated on real-world tasks.,This is a known advantage of offline RL over BC (Kumar et al., 2022; Levine et al., 2020); however, we only observe this behavior in VIP-RWR and not R3M-RWR, indicating that this advantage of offline RL is only realized when the reward information is sufficiently informative.,…et al., 2009; Grauman et al., 2022) has emerged as an effective solution for acquiring a general visual representation for robotic manipulation (Shah & Kumar, 2021; Parisi et al., 2022; Nair et al., 2022; Xiao et al., 2022) This paradigm is favorable to the traditional approach of in-domain…"
da2fe6cd385194b0274d04d04ee72e8caf3854d4,65fc1f1c567801fee3788974e753cdbf934f07e9,True,methodology,"While methods such as VPT (Baker et al., 2022) can utilize internet-scale data through learning an inverse dynamics model to label unlabled videos, an inverse dynamics model itself does not support model-based planning or reinforcement learning to further improve learned policies beyond imitation learning.,While methods such as VPT (Baker et al., 2022) can utilize internet-scale data through learning an inverse dynamics model to label unlabled videos, an inverse
dynamics model itself does not support model-based planning or reinforcement learning to further improve learned policies beyond imitation learning.,While methods such as VPT (Baker et al., 2022) can utilize internet-scale data through learning an inverse dynamics model to label unlabled videos, an inverse
dynamics model itself does not support model-based planning or reinforcement learning to further improve learned policies beyond imitation…"
da2fe6cd385194b0274d04d04ee72e8caf3854d4,5922f437512158970c417f4413bface021df5f78,True,"result,background","Although substantial effort has been devoted to encoding different environments with universal tokens in a sequence modeling framework (Reed et al., 2022), it is unclear whether such an approach can preserve the rich knowledge embedded in pretrained vision and language models and leverage this knowledge to transfer to downstream reinforcement learning (RL) tasks.,Inspired by the large-scale pretraining success of vision and language domains, largescale sequence and image models have recently been applied to learning generalist decision making agents (Reed et al., 2022; Lee et al., 2022; Kumar et al., 2022).,First, we compare to existing work that uses goal-conditioned transformers to learn across multiple environments, where goals can be specified as episode returns (Lee et al., 2022), expert demonstrations (Reed et al., 2022), or text and images (Brohan et al., 2022).,Although substantial effort has been devoted to encoding different environments with universal tokens in a sequence modeling framework (Reed et al., 2022), it is unclear whether such an approach can preserve the rich knowledge embedded in pretrained vision and language models and leverage this…,, 2022), or require studious tokenization (Reed et al., 2022) that might seem unnatural in reinforcement learning settings where different environments have distinct state and actions spaces.,…under environments with the same state and action spaces (e.g., Atari games) (Lee et al., 2022; Kumar et al., 2022), or require studious tokenization (Reed et al., 2022) that might seem unnatural in reinforcement learning settings where different environments have distinct state and actions spaces.,Transformer BC (Reed et al., 2022; Lee et al., 2022).,, 2022), expert demonstrations (Reed et al., 2022), or text and images (Brohan et al."
da2fe6cd385194b0274d04d04ee72e8caf3854d4,69ee9b3a915951cc84b74599a3a2699a66d4004f,True,methodology,"While our environment setting is similar to that of (Shridhar et al., 2022), this method is not directly comparable to our approach, as CLIPort abstracts actions to the existing primitives of pick and place as opposed to using joint space of a robot.,We train our method using demonstrations across a set of 10 separate tasks from (Shridhar et al., 2022), and evaluate the ability of our approach to transfer to 3 different test tasks.,To measure multi-task learning and transfer, we use the suite of language guided manipulation tasks from (Shridhar et al., 2022)."
01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8,5c74b1021aebd0575c339ce4dfd47e183009a9c5,True,"methodology,background","We also experiment with the Mixture density networks (MDN) [8] and uniform quantization, as shown in previous works [25, 39].,(f) Implicit Behavioral Cloning (IBC): Instead of modeling the conditional distribution P (a | o), IBC models the joint probability distribution P (a, o) using energy-based models [25].,While IBC is our strongest baseline, in our experience it is also one that is quite easy to overfit to our datasets.,We experimentally evaluate BeT on five datasets ranging from simple diagnostic toy datasets to complex datasets that include simulated robotic pushing [25], sequential task solving in kitchen environments [34], and self-driving with visual observations in CARLA [21].,While this suffers from all the classic issues of training an EBM, like higher sample complexity and higher complexity in sampling, IBC models have been shown to have higher success in learning multi-modal and discontinuous actions.,While IBC is slower than explicit BC models because of their sampling requirements, they have been shown to learn well on multi-modal data, and outperform earlier work such as MLP-MDNs [8].,Finally, a class of algorithms, most notably [50, 25, 43, 58] do not directly learn a generative model but instead learn energy based models.,Since [25] is a BC model capable of multi-modality, we compare against it as a baseline in Sec.,Implicit Behavioral Cloning Implicit Behavioral Cloning (IBC) [25] takes a different approach in behavioral cloning, where instead of learning a model f(o) := a, we learn an energy based model E(o, a) where the intended action a at any observation is defined as arg minaE(o, a).,(d) Multi-modal block-pushing environment: For more complicated interaction data, we use the multi-modal block-pushing environment from Implicit Behavioral Cloning (IBC) [25], where an XArm robot needs to push two blocks into two squares in any order.,[25], we do not explicitly predict mode centers, which significantly improves our modeling capacity.,While more recent behavior generation models have sought to address this problem, they often require complex generative models [76], an exponential number of bins for actions [54], complicated training schemes [66], or time-consuming test-time optimization [25].,In terms of raw computation time to determine one action from the observations, in the Kitchen environment, BeT took 2.8 ms, while IBC took 52 ms and MLP, as the fastest point of comparison, took 0.5 ms.,In contrast, for the same task, our strongest baseline IBC takes about 14 hours.,Evaluation rollouts on the same environment take 1.65 seconds with BeT, as opposed to 17.70 seconds with IBC."
01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8,8d0eeb8aee3ce93c9c04f0662ee058e8eefee6bf,True,"methodology,background","[16, 54] use transformers mostly to summarize historical visual context, while [13] relies on their long-term extrapolation abilities to collect human-in-the-loop demonstrations more efficiently.,Current approach for behavior cloning from such datasets primarily focus on learning goal-conditioned policies, where each goal implies a single mode of behavior [35, 34, 52, 16].,The lack of labeled goal or reward labels in the second category implies that there is more multi-modality in the action distributions compared to action distributions of goal or reward conditioned datasets, which is the same reason a lot of work learning from multi-modal datasets try to learn a goal-conditioned model [35, 34, 52, 16].,Among those, [12, 39] applies them to Reinforcement Learning and Offline Reinforcement Learning, respectively, while [13, 16, 54] use them for imitation learning."
0ab3f612db15a5a986d731283ca52e08058c9c44,01bc9970cdceafe7a81fcd9acc95b82d6666c502,True,"background,methodology","In particular, symbolic state abstractions are often lossy [12, 13, 24], in that they discard information that may be important for decision-making, like the reachability of buttons or the relative stick grasp in Stick Button.,We are motivated by cases where it is easier to design [12, 13, 14, 15, 16] or learn [17, 18, 19, 20, 21] symbols than it is to design skills.,[13] propose a pipeline for learning operators and samplers for bilevel planning when given a set of parameterized policies1.,The Cover environment was introduced by [12, 13, 21].,See Figure 3, Algorithm 1, and [13] for an extended description.,Following [13], we learn two neural networks for each sampler.,[13], who assume parameterized policies are given.,3) for an elaborated discussion on the relationship between this work and [12, 13].,Following previous work [13, 18, 21, 44], we use a simple linear-time approach to learn symbolic operators.,In this line, other work has concentrated on learning operators [79, 80, 81, 82, 44, 12, 83], learning samplers [39, 84, 85, 86, 13, 87], or learning predicates [17, 88, 4, 89, 90, 91, 18, 19, 20, 92, 21]."
0ab3f612db15a5a986d731283ca52e08058c9c44,869cd60e0fe1c0ae5e93854f218bb33eaa45c1d4,True,"result,background","In particular, symbolic state abstractions are often lossy [12, 13, 24], in that they discard information that may be important for decision-making, like the reachability of buttons or the relative stick grasp in Stick Button.,We are motivated by cases where it is easier to design [12, 13, 14, 15, 16] or learn [17, 18, 19, 20, 21] symbols than it is to design skills.,In brief, [13] extends [12] by removing the assumption that samplers are given.,First, in [12, 13], the demonstration data is given in terms of high-level controllers: each transition corresponds to the execution of an entire controller, and the controller identity is known.,The Cover environment was introduced by [12, 13, 21].,Here we elaborate on the relationship between this work and our prior work [12, 13].,3) for an elaborated discussion on the relationship between this work and [12, 13].,However, previous work [12, 21] suggests that operator and sampler learning are robust to a limited degree to predicate ablation or addition; we expect the same for policy learning5.,• Notes: This extends the environment of [12, 13, 21] to make the robot move in two dimensions.,In this line, other work has concentrated on learning operators [79, 80, 81, 82, 44, 12, 83], learning samplers [39, 84, 85, 86, 13, 87], or learning predicates [17, 88, 4, 89, 90, 91, 18, 19, 20, 92, 21]."
0ab3f612db15a5a986d731283ca52e08058c9c44,424cf0cdf8015640ab36d80cd15f92476aac46b8,True,"background,methodology","We are motivated by cases where it is easier to design [12, 13, 14, 15, 16] or learn [17, 18, 19, 20, 21] symbols than it is to design skills.,Given 1000 demonstrations, after learning predicates (see [21] for a description of the approach), we run BPNS skill learning and planning, with the configuration identical to the main experiments.,The Cover environment was introduced by [12, 13, 21].,This lossiness can be mitigated to some degree by inventing new predicates [4, 21], but in robotics environments, there are often kinematic and geometric constraints that are hard to perfectly abstract away [1, 25, 26].,Following previous work [13, 18, 21, 44], we use a simple linear-time approach to learn symbolic operators.,However, previous work [12, 21] suggests that operator and sampler learning are robust to a limited degree to predicate ablation or addition; we expect the same for policy learning5.,• Notes: This extends the environment of [12, 13, 21] to make the robot move in two dimensions.,[21], which starts with a minimal set of goal predicates that are sufficient for describing the task goals, and then uses demonstrations to invent new predicates.,In this line, other work has concentrated on learning operators [79, 80, 81, 82, 44, 12, 83], learning samplers [39, 84, 85, 86, 13, 87], or learning predicates [17, 88, 4, 89, 90, 91, 18, 19, 20, 92, 21]."
01bc9970cdceafe7a81fcd9acc95b82d6666c502,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,"background,methodology","To plan with NSRTs, we borrow techniques from searchthen-sample task and motion planning (TAMP) [7], with symbolic AI planning in an outer loop serving as guidance,Task and motion planners (TAMP) [7] can plan effectively at long horizons, but they typically require hand-specified operators, action samplers, and lowlevel transition models.,For the latter, we hope to draw on TAMP techniques for stochastic and partially observed settings.,To plan with NSRTs, we borrow techniques from searchthen-sample task and motion planning (TAMP) [7], with symbolic AI planning in an outer loop serving as guidance for continuous planning with neural models in an inner loop.,This planning strategy falls under the broad class of search-then-sample TAMP techniques [7]."
869cd60e0fe1c0ae5e93854f218bb33eaa45c1d4,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,"methodology,background","optimization approach, with symbolic planning providing a dense sequence of subgoals for continuous optimization [1], [5].,Task and motion planning (TAMP) combines insights from AI planning and motion planning to address these challenges [1], [2], [3], [4], [5], [6].,, Which object should I grasp? and How should I grasp it?) [1].,in “search-then-sample” TAMP methods [1]."
742b195fb4c2868a4e60012c8e0bf7db43bb5650,826383e18568c9c37b5fc5dd7e2913352db22b47,True,"background,methodology","In this paper, we study Language-driven zero-shot object navigation (L-ZSON)—a more challenging but also more applicable version of object navigation [4,17,63,74,83] and ZSON [37, 44] tasks.,On a ROBOTHOR object subset, considered in prior work, the same CoW beats the leading method [37] by 15.,1 [37] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi.,[37] train on a subset of ROBOTHOR categories and evaluate on a held-out set.,LZSON encompasses ZSON, which specifies only the target category [37, 44].,• EmbCLIP-ZSON [37]: trains a model on eight ROBOTHOR categories, using CLIP language embeddings to specify the goal objects.,Recent work studies object navigation in zero-shot settings, where agents are evaluated on object categories that they are not explicitly trained on [37, 44].,in prior object navigation [4, 17] and ZSON [37, 44] tasks, which focus on high-level categories like “plant”."
8be4f7216f2124603b4477d61f5ddb577a640fe5,0fd064fd9a5e014484f531552e56342e522de21b,True,methodology,"Kim B, Lee K, Lim S, Kaelbling LP, Lozano-Perez T. 2020.,Sampling for continuous parameter values can itself be similarly guided, using techniques for optimistic global optimization (128, 107).,Interleaved Dornhege∗ (62, 63, 91) Gaschler∗ (92, 93, 94) Colledanchise∗ (95) Gravot∗ (96, 97) Stilman† (23, 98, 99) Plaku† (100) Kaelbling∗ (101, 102) Barry† (103, 30, 104) Garrett∗ (70, 71) Thomason∗ (105) Kim∗ (106, 107) Kingston† (108) Fernandez-Gonzalez∗ (109),Kim B, Kaelbling LP, Lozano-Perez T. 2018.,Kim B, Shimanuki L. 2019."
4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424,True,"background,methodology","…et al.,523 2021) that do not provide an oracle agent to answer524 question in natural language, researchers also needs525 to build a rule-based (Padmakumar et al., 2021) or526 neural-based (Roman et al., 2020) oracle.527
4.3 Data-centric Learning528
Compared with previously discussed works…,TEACh (Padmakumar et al., 2021) is a dataset that studies ob-,Both518 rule-based methods (Padmakumar et al., 2021) and519 neural-based methods (Roman et al., 2020; Nguyen520 et al., 2021) have been developed to build naviga-521 tion agents with dialog ability.,question in natural language, researchers also need to build a rule-based (Padmakumar et al., 2021) or neural-based (Roman et al.,Both rule-based methods (Padmakumar et al., 2021) and neural-based methods (Roman et al.,Meanwhile, for tasks (Thomason et al., 2019b; Padmakumar et al., 2021) that do not provide an oracle agent to answer,, 2019) TEACh (Padmakumar et al., 2021), Minecraft Collaborative Building (Narayan-Chen et al.,TEACh (Pad-269 makumar et al., 2021) is a dataset that studies object270 interaction and navigation with free-form dialog.271 The follower converses with the commander and272 interacts with the environment to complete various273
house tasks such as making coffee."
4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,cd14bb267b2badb0d4407b3327d162c2b4346b35,True,background,"In SOON (Zhu et al., 2021a), an agent190 receives a long complex coarse-to-fine instruction191 which gradually narrows down the search scope.192
Navigation+Object Interaction For some tasks, 193 the target object might be hidden (e.g., the spoon in 194 a drawer), or need to change status (e.g., a…,In SOON (Zhu et al., 2021a), an agent190 receives a long complex coarse-to-fine instruction191 which gradually narrows down the search scope.192
Navigation+Object Interaction For some tasks, 193 the target object might be hidden (e.g., the spoon in 194 a drawer), or need to change status (e.g., a sliced ap- 195 ple is requested but only a whole apple is available).,Visual Navigation (Zhu et al., 2021b) is a problem of navigating an agent from the current location to find the goal target.,Extracting these tokens and encod- 392 ing the object tokens and directions tokens are cru- 393 cial (Qi et al., 2020a; Zhu et al., 2021b)."
4f1d598f919aae55c3cbbc425ef1514a54e2b8cd,645bc7a5347a299a1e8aa965867bd097f6f4bddd,True,"background,methodology","Both518 rule-based methods (Padmakumar et al., 2021) and519 neural-based methods (Roman et al., 2020; Nguyen520 et al., 2021) have been developed to build naviga-521 tion agents with dialog ability.,, 2021) and neural-based methods (Roman et al., 2020; Nguyen et al., 2021a) have been developed to build navigation agents with dialog ability.,…agent to answer524 question in natural language, researchers also needs525 to build a rule-based (Padmakumar et al., 2021) or526 neural-based (Roman et al., 2020) oracle.527
4.3 Data-centric Learning528
Compared with previously discussed works that529 focus on building a better VLN agent…"
a58b3f2ab75fdbda082e684d027ab4f552b0b5d3,a2f6930b37febd2b88bbea7778f442ef3621b8a3,True,methodology,"We use STORM [7] as a planner which minimizes a Euclidean distance cost to generate trajectories from start to,We use STORM [7] as a planner which minimizes a Euclidean distance cost to generate trajectories from start to goal position.,We use STORM [7] as the planner which computes an action leveraging sampling based optimization to optimize over costs.,We interface our CL with an optimization based controller [7] to generate commands for the robot as shown in Algorithm 1."
4b516216d7d150a081fd74993bddf36b6b22c118,5c74b1021aebd0575c339ce4dfd47e183009a9c5,True,methodology,"The bimanual sweeping task [83, 14] requires two 7-DoF robot arms equipped with spatula-like end-effectors to sweep a pile of particles evenly into two bowls while avoiding dropping particles between the tips of the spatulas.,2% from the previous state-of-the-art [14].,For both BC and PC we parameterize log-likelihood using energy-based models, also known as an implicit loss, which is state-of-the-art for this task [14].,6 Experiments We now evaluate procedure cloning in larger scale settings on tasks including simulated robotic navigation [82] and manipulation [83, 14], and learning to play MinAtar [84] (a miniature version of Atari [85]).,All of our algorithm implementations use the implicit loss function described in [14] for this task."
5c74b1021aebd0575c339ce4dfd47e183009a9c5,30256fd41d471e8c6731c732e41ba865321ced7d,True,"background,methodology","Baselines Ours Explicit Implicit Explicit Implicit Method NearestBC CQL [26] S4RL [27] BC (MSE) BC (EBM) BC (MSE) BC (EBM) Neighbor (from CQL [26]) w/ RWR [28] w/ RWR [28] Uses data (o,a) (o,a) (o,a,r) (o,a,r) (o,a) (o,a) (o,a,r) (o,a,r) Domain Task Name,Baselines from [26] and [27] didn’t report standard deviations.,of both implicit and explicit policies significantly outperform the BC baselines reported on the benchmark, 107 and provide competitive results with state-of-the-art offline reinforcement learning results reported thus 108 far, including CQL [26] and S4RL [27].,Surprisingly, we find that our implementations106
of both implicit and explicit policies significantly outperform the BC baselines reported on the benchmark,107 and provide competitive results with state-of-the-art offline reinforcement learning results reported thus108 far, including CQL [26] and S4RL [27]."
183984d0426fd0702fbbe8bd890fe32058ecdfff,47049b3a3da29d003f7a44712f070c928e07efaa,True,"background,methodology","Mechanics representations and priors are ubiquitous in model-based techniques and thus have been able to demonstrate working solutions [5, 6, 7, 8].,• MIQP (Oracle): A model-based solution to each manipulation task, solved via CTO [6] and provided with all ground-truth parameters (such as object shape and desired object trajectory in SE(2)).,The main focus of the state-of-the-art is in planar manipulation tasks over sagittal settings, where friction and contact forces are the primary source of challenges [5, 27, 6, 28].,Datasets: We built four datasets of non-prehensile manipulation tasks in the sagittal3 plane with randomized trajectories in SE(2), all ground truth robot actions are found by solving Contact-Trajectory Optimization (CTO) [6]: • Training set of 20 tasks with randomly generated polygonal objects of 4 to 6 facets, including all mechanical parameters and robot actions for each task.,We train the first two stages with supervision from an expert contact trajectory optimizer [6] while the third stage is trained end-to-end over the desired object trajectory.,Once the task is setup, we pose a Mixed-Integer Quadratic Program (MIQP) that find the globally optimal robot finger placements and contact forces pc(t),λc(t) as:
MIQP: min p,Λ
T∑
t=0
λTc (t)Qλλc(t)+p̈ T c (t)Qpp̈c(t) (11)
subject to: Mr̈(t)+G(r)=J(r)TΛ(t), pc(t)∈Fc(t), λc(t)∈FCc(t), λe(t)∈FCe(t) (12) This problem is known as Contact-Trajectory Optimization (CTO), and we solve the MIQP using the formulation from [6].,Datasets: We built four datasets of non-prehensile manipulation tasks in the sagittal3 plane with randomized trajectories in SE(2), all ground truth robot actions are found by solving Contact-Trajectory Optimization (CTO) [6]:
• Training set of 20 tasks with randomly generated polygonal objects of 4 to 6 facets, including all mechanical parameters and robot actions for each task.,In contrast, the MDR and CVX result in lower training loss closer to the MIQP (Oracle) reference.,All these parameters, along with the solution to CTO are stored and used to train and validate our model.,This set of mechanical parameters [6] includes: (i) the Jacobian matrix J(r)(t), (ii) the location of external contacts pe(t), and (iii) parameters that implicitly encode contact modes Fc, FCc, andFCe.,Our vision is to perform training on completely synthetic data generated with MIQP [6], fine-tuned for robustness with the simulator, and execute a task specified with a real-world video.,While this is a limitation, it is a standard approach to evaluate planning and learning pipelines for manipulation problems (see [9, 28, 10, 26, 6, 8]).,Solving this problem without assuming known contact modes is often intractable and a significant body of research has focused on studying it [25, 33, 6].,This is akin to a pixel-to-actions style policy network [41, 3] and is trained to generate MIQP solutions with a loss function LNN = ||p∗c(t)−pc(t)||22.,We find optimal finger trajectories for each task using a Mixed-Integer Quadratic Program (MIQP) [6], for N = 2 point fingers.,The scope of these techniques is often focused in: (i) the planning/control of primitives [22, 23, 9], which can be combined sequentially to complete a task [7, 24], or (ii) in the optimization of general skills [25, 5, 6], which solve inverse rigid-body dynamics to solve a task.,and the robot having point-fingers (standard in robot manipulation literature [9, 10, 11, 6, 8]).,Inference time in a neural model is smaller than
MIQP (Oracle) [6], as our model involves simple operations and solving QP.,We first analyze the ability to infer finger trajectories from the ground truth data obtained via CTO."
fd399c7068512858b27535f75c8c31d2442dbaac,8d0eeb8aee3ce93c9c04f0662ee058e8eefee6bf,True,"background,methodology","To enable learning a potentially multi-modal policy that excels across many tasks, we adopt the same solution used in [7], [25], [33], which discretizes the action space into 256 independent bins along every dimension, and parameterize the policy using a mixture of discretized logistic distribution.,Following this definition, prior work [9], [7], [11], [20], [45] on one-shot imitation learning evaluate agents with a single task, as illustrated in Figure 1.,[11] applies the Model-Agnostic Meta-Learning algorithm (MAML) [10] to adapt policy model parameters for new tasks; TecNets [21] applies a hinge rank loss to learn explicit task embeddings; DAML [45] adds a domain-adaptation objective to MAML to use human demonstration videos; [7] improves policy network with Transformer architecture [41].,We adopt the nonlocal self-attention block in [42], and make it to a multi-head version as [7].,Task Setup DAML [45] T-OSIL [7] LSTM MLP MOSAIC (ours),First proposed in [9], the framework has been extended to different tasks and visual inputs [7], [11], [20], [45].,In addition to the behavioural cloning loss, we also utilize the inverse dynamics loss as in [7].,As a first step in our investigation, we study the performance of prior state-of-the-art methods [7], [45].,• T-OSIL [7]: Model architecture proposed by [7] which also uses non-local block [42] in self-attention module and is trained with end-effector point prediction as auxiliary loss.,• T-OSIL [7]: Model architecture proposed by [7] which"
96a1a24fb75635bd5a27b8e4034d0faef5f99ad5,449c5660d637741f7aa7ff42549c32b43c9968bf,True,"background,methodology","Other works have focused on directly modify task gradients [1, 2, 46, 17, 23, 32].,We trained on CIFAR-100 [20] and treated each of the 20 ‘coarse’ classes as one domain, thus creating a dataset with 20 tasks, where each task is a 5-class classification over the dataset’s ‘fine’ classes, following [34, 46].,Each task’s influence on the network parameters can be indirectly balanced by finding a suitable set of weightings which can be manually chosen, or learned through a heuristic [18, 26] — which we called weighting-based methods; or directly balanced by operating on taskspecific gradients [5, 46, 1, 2, 17, 23, 32] — which we called gradient-based methods.,5, when combined with recently proposed state-of-the-art gradient-based methods designed for multi-task learning: GradDrop [2], PCGrad [46] and CA-,5, when combined with recently proposed state-of-the-art gradient-based methods designed for multi-task learning: GradDrop [2], PCGrad [46] and CA-
Grad [23]."
395a4db5fef867d5bd352585aa00b97004994972,6b7d21cf7355e08d53cb2ad17c144002fca3f2c8,True,"background,methodology","The control agent remains largely unchanged from ARM [14].,Recently however, Q-attention and the ARM system [14] has been shown to bypass many flaws that come with reinforcement learning, most notably the large training burden and exploration difficulty with sparsely-rewarded and long-horizon tasks.,ARM [14] introduced several core concepts that facilitate the learning of vision-based robot manipulation tasks.,The Q-attention module [14] (discussed in Section 3.,Before training, we fill the replay buffer with demonstrations using keyframe discovery and demo augmentation [14].,learning from sparsely-rewarded and image-based tasks [14]: two properties that are particularly important for robot manipulation tasks.,ARM [14] uses Q-attention to reduce the image resolution to a next-best pose phase (by cropping 128×128 observations to 16×16), while the role of coarse-to-fine Q-attention is to discretise the otherwise large translation space.,The coarse-to-fine Q-attention shares the same motivation that was was laid out in ARM [14], i.,In addition to our method (C2F-ARM), we include the same baselines as in previous work: ARM [14], BC, SAC+AE [41], DAC [18] (an improved, off-policy version of GAIL [9]), SQIL [30], and DrQ [19].,Keyframe discovery and demo augmentation were another two important techniques introduced in ARM [14]."
1c3b3639a335fd0cfbe4a3ebab518561fdef660d,4f68e07c6c3173480053fd52391851d6f80d651b,True,"methodology,background","Learning representations for robotics tasks poses additional challenges, as perception data is conditioned on the motion policy and model dynamics [4].,This inflexible machine learning approach is ripe for innovation with the use of foundational models [4], in particular when,Beyond just language, we see a shift in machine learning architectures in multiple domains, as the dominant design paradigm changes from designing task-specific models towards the use of large foundational pre-trained models [4].,The use of foundational models is appealing because they are trained on broad datasets over a wide variety of downstream tasks, and therefore provide general skills which can be used directly or with minimal fine-tuning to new applications [4]."
36c95e3ef362742a5c1844257e8b79d3251a781e,ff0282b34d758a4aaad524ea554f6545852e3c68,True,"background,methodology","3Yes, that dataset title is “ACRONYM” [44].,Because we target physical robot applications, we may be able to tie language to sets of graspable points encoding gripper orientation and position [44] for each object.,To construct SNARE, we select a subset of 7,897 ACRONYM [44] object models from ShapeNetSem [45, 13].,The annotations are collected to complement the ACRONYM3 [44] grasping dataset and include language that targets both visual and tactile attributes of objects."
4c5d4601a3a19c31da6588d2a34adfb161f68c0e,8095bdd5861d1dbe43b77997bc0dbc2fd51acb93,True,"background,methodology","…the same spirit as recent work aimed to teach virtual or physical robots to follow instructions provided in natural language (Hermann et al., 2017; Lynch and Sermanet, 2020) but attempts to go beyond it by emphasising the interactive and language production capabilities of the agents we develop.,) Second, prior work has largely focused on comparatively constrained sets of behaviours, involving uncluttered environments with few objects to manipulate (Lynch and Sermanet, 2020; Hill et al., 2019a), or has studied navigation absent of environment manipulation altogether (Anderson et al.,Second, prior work has largely focused on comparatively constrained sets of behaviours, involving uncluttered environments with few objects to manipulate (Lynch and Sermanet, 2020; Hill et al., 2019a), or has studied navigation absent of environment manipulation altogether (Anderson et al., 2017).,Recent work in robotics demonstrated the possibility of conditioning simulated robotic manipulators with natural language instructions (Lynch and Sermanet, 2020).,Our research program shares much the same spirit as recent work aimed to teach virtual or physical robots to follow instructions provided in natural language (Hermann et al., 2017; Lynch and Sermanet, 2020) but attempts to go beyond it by emphasising the interactive and language production capabilities of the agents we develop."
89fba716d11de5b29ceb8d137f6b09f05be079fd,3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2,True,"result,methodology,background","9M parameter FCN [41] with residual connections [25], given that similar architectures have been used for FCN-based picking and placing [71], [68].,For several tasks in the benchmark, we propose to tackle them using novel goal-conditioned variants of Transporter Network [68] architectures.,We first describe the problem formulation, followed by background on Transporter Networks [68].,Transporter Networks [68] is a model architecture for manipulation that learns to rearrange objects by (i) attending to a local region of interest, then (ii) predicting its target spatial displacement by cross-correlating its dense deep visual features over the scene.,Our experiments also significantly extend results of using Transporter Networks for deformable manipulation tasks — while [68] demonstrated,For the first eight tasks listed, we benchmark with Transporter Networks [68] (“Transporter”) and two baselines that use ground-truth pose information instead of images as input.,Following [68], to handle multimodality of picking and placing distributions, the models output a mixture density [9] represented by a 26-D multivariate Gaussian."
bfba05093314e52317536b6cfc8b7fded8371e02,4e1e3bdf43c1556279e4c043dbdf5571a159fed9,True,"background,methodology","We implement the encoder and decoder as multi-layer feed-forward networks, with the ReLU activation as in prior work [7, 28].,Previous work on learning latent actions for assistive teleoperation [5, 28] learn a decoder Dec(s, z) : S × Z → A that maps user low-dimensional inputs z ∈ Z and current state s ∈ S to a high-dimensional action a ∈ A.,We believe that the ability to disambiguate with language, and the integration of language within the latent actions framework is a strong research contribution, and hope that future work looks to dynamic states – perhaps by leveraging visual latent actions [28] – and to adapting to new utterances and tasks dynamically [43, 52].,We collect demonstrations kinesthetically as in prior work [7, 28], recording joint states at a fixed frequency.,Instead, we adopt learned latent actions [5, 7, 8, 27, 28] a framework that uses conditional auto-encoders [29] to learn taskspecific latent “action” spaces from demonstrations."
bfba05093314e52317536b6cfc8b7fded8371e02,f17adb5a8ee6cf94ea7d08808fa1af3af8e22dbd,True,"background,methodology","Previous work on learning latent actions for assistive teleoperation [5, 28] learn a decoder Dec(s, z) : S × Z → A that maps user low-dimensional inputs z ∈ Z and current state s ∈ S to a high-dimensional action a ∈ A.,Shared autonomy approaches such as learned latent actions [5, 7, 8] however, build intuitive low-dimensional controllers for high-DoF robots via dimensionality reduction.,Paradigms for efficient human-robot collaboration that strike a balance between robot autonomy and human control such as shared autonomy [2, 3, 4, 5] present a promising path towards building such assistive systems.,Instead, we adopt learned latent actions [5, 7, 8, 27, 28] a framework that uses conditional auto-encoders [29] to learn taskspecific latent “action” spaces from demonstrations."
bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2,01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8,True,"result,methodology,background","Time-series diffusion transformer To reduce the oversmoothing effect in CNN models [49], we introduce a novel transformer-based DDPM which adopts the transformer architecture from minGPT [42] for action prediction.,We systematically evaluate Diffusion Policy on 12 tasks from 4 benchmarks [12, 15, 29, 42].,Diffusion Policy learns to approach the contact point equally likely from left or right, while LSTM-GMM [29] and IBC [12] exhibit bias toward one side and BET [42] cannot commit to one mode.,The baseline methods we evaluate, however, work best with velocity control (and this is reflected in the literature where most existing work reports using velocitycontrol action spaces [29, 42, 60, 13, 28, 27]).,Prior work attempts to address this challenge by exploring different action representations (Fig 1 a) – using mixtures of Gaussians [29], categorical representations of quantized actions [42], or by switching the the policy representation (Fig 1 b) – from explicit to implicit to better capture multi-modal distributions [12, 56].,We systematically evaluate Diffusion Policy across 12 tasks from 4 different benchmarks [12, 15, 29, 42] under the behavior cloning formulation.,3) Multimodal Block Pushing: adapted from BET [42], this task tests the policy’s ability to model multimodal action distributions by pushing two blocks into two squares in any order.,Similarly, BC-RNN and BET would have difficulty specifying the number of modes that exist in the action distribution (needed for GMM or kmeans steps).,In contrast, both LSTM-GMM [29] and IBC [12] are biased toward one mode, while BET [42] fails to commit to a single mode due to its lack of temporal action consistency.,However, suppose each action in the sequence is predicted as independent multimodal distributions (as done in BCRNN and BET).,This surprising result stands in contrast to the majority of recent behavior cloning work that generally relies on velocity control [29, 42, 60, 13, 28, 27].,We present the best-performing for each baseline method on each benchmark from all possible sources – our reproduced result (LSTM-GMM) or original number reported in the paper (BET, IBC).,The challenge of modeling multi-modal distribution in human demonstrations has been widely discussed in behavior cloning literature [12, 42, 29]."
bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2,5c74b1021aebd0575c339ce4dfd47e183009a9c5,True,"methodology,background","We systematically evaluate Diffusion Policy on 12 tasks from 4 benchmarks [12, 15, 29, 42].,Diffusion Policy learns to approach the contact point equally likely from left or right, while LSTM-GMM [29] and IBC [12] exhibit bias toward one side and BET [42] cannot commit to one mode.,Training energy-based policies often requires negative sampling to estimate an intractable normalization constant, which is known to cause training instability [11, 12].,In addition, we report the average of best-performing checkpoints for robomimic and Push-T tasks to be consistent with the evaluation methodology of their respective original papers [29, 12].,[12] evaluate every checkpoint and report results for the bestperforming checkpoint.,Prior work attempts to address this challenge by exploring different action representations (Fig 1 a) – using mixtures of Gaussians [29], categorical representations of quantized actions [42], or by switching the the policy representation (Fig 1 b) – from explicit to implicit to better capture multi-modal distributions [12, 56].,We systematically evaluate Diffusion Policy across 12 tasks from 4 different benchmarks [12, 15, 29, 42] under the behavior cloning formulation.,2) Push-T: adapted from IBC [12], requires pushing a Tshaped block (gray) to a fixed target (red) with a circular,In contrast, both LSTM-GMM [29] and IBC [12] are biased toward one mode, while BET [42] fails to commit to a single mode due to its lack of temporal action consistency.,The challenge of modeling multi-modal distribution in human demonstrations has been widely discussed in behavior cloning literature [12, 42, 29]."
22574ad98c141b45d26d66e8e7713bac8a7d6964,645bc7a5347a299a1e8aa965867bd097f6f4bddd,True,"methodology,background","For the sake of consistency with game-play mode notation introduced by Roman et al. (2020), we denote the role of asking questions that is intrinsic to the NAVIGATOR by QUESTIONER.,The QUESTIONER asks questions
every 4th time-step, which is a hard-coded heuristic by Roman et al. (2020) since their NAVIGATOR does not know when to ask questions.,recent Recursive Mental Model (RMM) (Roman et al., 2020) for CVDN attempts to address this by introducing a simulated dialogue game-play mode, where a trained navigator is fine-tuned jointly with a pre-trained guide and evaluated in this mode.,In the CVDN dataset, a human NAVIGATOR cooperates with a human GUIDE to find a goal region G with target object O. Roman et al. (2020) introduced the game-play mode, which is essentially an agent-agent replica of this dynamic dataset creation process wherein the two trained agents consume each…,However, the RMM navigator does not dynamically ask questions, instead relying on a data-driven heuristic of asking questions after every 4th navigation time-step.,The recent Recursive Mental Model (RMM) (Roman et al., 2020) for CVDN attempts to address this by introducing a simulated dialogue game-play mode, where a trained navigator is fine-tuned jointly with a pre-trained guide and evaluated in this mode.,We proposed generalizing the game-play regime introduced with RMM (Roman et al., 2020) to enable interactive fine-tuning and evaluation of VISITRONlike models with pre-trained guides.,Given VISITRON’s design and ability to identify when to engage in dialogue, we also propose a generalization of the game-play mode introduced by Roman et al. (2020) for jointly fine-tuning and evaluating VISITRON and future such models with pre-trained guides to help them easily adapt to their…"
15fa42c1a2cc0f2df3cfd8f668958688a4b7afb6,559d7a0a49a6b912338822226fa9448993a82e12,True,methodology,"Our experimental results on the Distracting Control Suite show that more fine-grained patches (lower patch size) with fewer selected patches (lower l) improves performance (Fig.,We then apply our method to a modified version of the DM control suite termed the Distracting Control Suite (Stone et al., 2021), where the background of the normal DM Control Suite’s observations are replaced with random images and backgrounds and viewed through random camera angles as shown in Fig.,In a series of experiments, we showed that IAP scales to higher-resolution images and emulate much finer-grain attention than what was previously possible, improving generalization in challenging visionbased RL tasks such as quadruped locomotion with obstacles and the recently introduced Distracting Control Suite.,We then apply our method to a modified version of the DM control suite termed the Distracting Control Suite (Stone et al., 2021), where the background of the normal DM Control Suite’s observations are replaced with random images and backgrounds and viewed through random camera angles as shown in…,We use the static setting on the medium difficulty benchmark found in (Stone et al., 2021)."
523acd658742fb9c978e3f7638c09d7ce78af719,0ee9b633a0914b51f1eec3ad434752aa58e10149,True,"background,methodology","We observe that MoCo-v3 can achieve good performance on one of the tasks and non-trivial on the other.,We opt to use the latest MoCo-v3 (Chen et al., 2021b) designed for ViT models.,…manipulation problems with multi-finger hands but rely on explicit state estimation (Handa et al., 2020; Huang et al., 2021), expert policies (Chen et al., 2021a), human demonstrations (Rajeswaran et al., 2018; Radosavovic et al., 2021; Qin et al., 2021), human priors (Mandikal & Grauman,…,In particular, we adopt PPO, ViT-Small visual encoder, and MoCo-v3 (Chen et al., 2021b) data augmentation recipe.,However, in contrast to MAE, it may be harder to adapt techniques like MoCo-v3 to in-the-wild images (see Figure 8).,MoCo-v3 comparisons."
523acd658742fb9c978e3f7638c09d7ce78af719,49142e3e381c0dc7fee0049ea41d2ef02c0340d7,True,methodology,"We leverage the recent NVIDIA IsaacGym simulator (Makoviychuk et al., 2021) to build our benchmark.,We leverage a GPU-based simulator for fast simulation (Makoviychuk et al., 2021), provide reward functions, baselines, and multiGPU implementation of learning algorithms from pixels.,We use a four-layer MLP with hidden layers of size [256, 128, 64] for all tasks, following (Makoviychuk et al., 2021)."
523acd658742fb9c978e3f7638c09d7ce78af719,e6486c08c54133ed92dad42dd41f277c047cd843,True,"background,methodology","Masked image autoencoding (Chen et al., 2020b; Bao et al., 2022; He et al., 2021) pursues a different direction by learning to recover masked pixels.,Yen-Chen et al. (2020) transfer image models trained on supervised vision tasks, e.g., edge detection and semantic segmentation, to affordance prediction models for object manipulation.,More recently, contrastive learning methods, e.g., (Hadsell et al., 2006; Oord et al., 2018; Wu et al., 2018; Henaff, 2020; He et al., 2020; Chen et al., 2020c; Jabri et al., 2020), have been popular.,Sax et al. (2018) and Chen et al. (2020a) show that representations learned from performing a set of mid-level vision tasks using label supervision benefits downstream navigation and manipulation tasks, respectively."
cb3631f12b4465f4396380b61a651f0c74763480,7f712d58084e32ddc1b0cd60932f8bc0a0916330,True,"methodology,result,background","This disentanglement is in sharp contrast to prior GCRL works [50, 6, 11, 3], which typically involve alternating updates to the critic Q-network and the policy network, a training procedure that has found to be unstable in the offline setting [23].,Existing offline GCRL algorithms [6, 50] adapt HER-based online GCRL algorithms to the offline setting by incorporating additional components conducive to offline training.,1 in [14] and Theorem 1 in [50], respectively); however, in both works, the lower bounds are loose due to constant terms that do not depend on the policy and hence do not vanish to zero.,[50] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang.,The regression-based methods are: (1) GCSL [14], which incorporates hindsight relabeling in conjunction with behavior cloning to clone actions that lead to a specified goal, and (2) WGCSL [50], which improves upon GCSL by incorporating discount factor and advantage weighting into the supervised policy learning update.,Both GCSL [14] and WGCSL [50] prove that their objectives are lower bounds of the true RL objective (Theorem 3.,Furthermore, it enables an algorithmic reduction of GoFAR to weighted regression [7], which allows us to obtain strong finite-sample statistical guarantee on GoFAR’s performance; prior regression-based GCRL approaches [14, 50] do not enjoy this reduction and obtain much weaker theoretical guarantees.,[50] improves upon [14] by incorporating discount-factor and advantage function weighting [40, 41], and shows improved performance in offline GCRL.,Offline GCRL [6, 50] is particularly promising because it enables learning general goal-reaching policies from purely offline interaction datasets without any environment interaction [29, 25], which can be expensive in the real-world.,Finally, WGCSL proves a policy improvement guarantee (Proposition 1 in [50]) under their exponentially weighted advantage; the improvement is not a strict equality, and consequently there is no convergence guarantee to the optimal policy."
cb3631f12b4465f4396380b61a651f0c74763480,6f812978ce061b6c650ffd70ad3cf7dc3eef6003,True,background,"[39] Soroush Nasiriany, Vitchyr H Pong, Ashvin Nair, Alexander Khazatsky, Glen Berseth, and Sergey Levine.,To see this, for any r(s; g), we can choose p(s; g) = e, corresponding to a softmax distribution with inputs r(s; g) [39].,We can in fact obtain a looser lower bound that does not require training a discriminator: −DKL(d(s; g)‖p(s; g)) ≥ E(s,g)∼dπ(s,g) [log p(s; g)]−Df (d(s, a; g)‖d(s, a; g)) (7) Here, the reward function is r(s; g) = log p(s; g), which encompasses standard GCRL reward functions [39] and is related to a probabilistic interpretation of GCRL; see Appendix E."
cb3631f12b4465f4396380b61a651f0c74763480,677b103eecc4d34e378502d60147456875e8741b,True,"methodology,result,background","This disentanglement is in sharp contrast to prior GCRL works [50, 6, 11, 3], which typically involve alternating updates to the critic Q-network and the policy network, a training procedure that has found to be unstable in the offline setting [23].,Existing offline GCRL algorithms [6, 50] adapt HER-based online GCRL algorithms to the offline setting by incorporating additional components conducive to offline training.,[6] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al.,As offline datasets contain diverse goals and become increasingly prevalent [9, 19, 6], policies learned this way can acquire a large set of useful primitives for downstream tasks [31].,The actor-critic methods are (1) DDPG [3], which adapts DDPG [30] to the goal-conditioned setting by incorporating hindsight relabeling, and (2) ActionableModel (AM) [6], which incorporates conservative Q-Learning [24] as well as goal-chaining on top of an actor-critic method; in this work, we use DDPG as the actor-critic method for AM.,[6] builds on actor-critic style GCRL algorithms [3] by adding conservative Q-learning [24] as well as goal-chaining, which expands the pool of candidate relabeling goals to the entire offline dataset.,Offline GCRL [6, 50] is particularly promising because it enables learning general goal-reaching policies from purely offline interaction datasets without any environment interaction [29, 25], which can be expensive in the real-world."
9f2e34581ca03160e8fd8b770203a5e7c2a902d3,5c9815f7d907b79b088cdf49665be9b8dc89e5e7,True,"methodology,background","A competing baseline FERM [60] makes good initial, but unstable, progress in a few tasks and often saturates in performance before exhausting our computational budget (40 hours/ task/ seed).,Unlike DAPG which is on-policy, FERM [60] is a closely related off-policy actor-critic methods combining learning from demonstrations with RL.,A competing baseline FERM 1 [60] is quite unstable in these tasks."
424cf0cdf8015640ab36d80cd15f92476aac46b8,5185759e89a8e52ee1184cbebba5917371673790,True,"methodology,background","(2021) consider learning both state and action abstractions for TAMP, like we do [16, 17, 69].,In the course of our research, we considered many options inspired by prior work, including per-operator prediction error [12, 34], bisimulation [17, 29], and inverse planning-based objectives [35, 36], but found them all to be divergent from Jreal, leading to poor performance (see the baselines in Section 6).,The lackluster performance of the bisimulation baseline is especially notable because of its prevalence in the literature [12, 13, 17, 29].,A baseline that learns abstractions by approximately optimizing the bisimulation criteria [37], as in prior work [17].,Inspired by prior work [16, 17, 29], we approach the predicate invention problem from a program synthesis perspective [30, 31, 32, 33].,A large body of work has emerged that studies learning abstractions for planning [12, 13, 14], but with a few notable exceptions [15, 16, 17], these efforts assume that the learned abstractions should be “all you need.,Recent efforts by Loula et al. (2019) and Curtis et al. (2021) consider learning both state and action abstractions for TAMP, like we do [16, 17, 69].,For this, we perform an intersection over all abstract states in each equivalence class [17, 20, 29]: PRE ← ⋂ τ=(s,·,·) δ −1 τ (s), where δ−1 τ (s) substitutes all occurrences of the objects in s with the parameters in PAR following an inversion of δτ , and discards any atoms involving objects that are not in the image of δτ .,• Following prior work [17], we prune out candidate predicates if they are equivalent to any previously enumerated predicate, in terms of all groundings that hold in every state in the dataset D."
424cf0cdf8015640ab36d80cd15f92476aac46b8,01bc9970cdceafe7a81fcd9acc95b82d6666c502,True,methodology,"To obtain data-efficient generalization over object identities, we learn relational, neuro-symbolic abstractions, where the symbolic components are predicates and operators, like those used in AI planning [11], and the neural components are samplers that refine the abstractions into actions that can be executed in the actual environment [18, 19, 20].,The expected abstract state sequence serves as a dense subgoal sequence for the REFINE procedure, in the sense that if there is ever a deviation from it, REFINE is able to resample and/or backtrack immediately (rather than, say, needing to roll out trajectories up to the end of the time horizon before testing the goal [20]).,For this, we perform an intersection over all abstract states in each equivalence class [17, 20, 29]: PRE ← ⋂ τ=(s,·,·) δ −1 τ (s), where δ−1 τ (s) substitutes all occurrences of the objects in s with the parameters in PAR following an inversion of δτ , and discards any atoms involving objects that are not in the image of δτ .,Our operator learning method is largely based on prior work [20, 27, 28].,Some of these efforts consider learning symbolic action abstractions [34, 63, 64] or refinement strategies [18, 20, 65, 66, 67, 68]; our operator and sampler learning methods take inspiration from these prior works."
424cf0cdf8015640ab36d80cd15f92476aac46b8,869cd60e0fe1c0ae5e93854f218bb33eaa45c1d4,True,"methodology,background","In the course of our research, we considered many options inspired by prior work, including per-operator prediction error [12, 34], bisimulation [17, 29], and inverse planning-based objectives [35, 36], but found them all to be divergent from Jreal, leading to poor performance (see the baselines in Section 6).,We were surprised by this result because the manually designed abstractions for PickPlace1D and Painting are not downward refinable [34].,For Painting, a learned “box lid open” predicate resolves the downward refinability issue discussed in prior work [34], where the position of the box lid (open or closed) was not modeled in the manual abstraction.,[34], but in that work, all state abstractions were manually defined (we use the same state abstractions for our Manual baseline below).,Some of these efforts consider learning symbolic action abstractions [34, 63, 64] or refinement strategies [18, 20, 65, 66, 67, 68]; our operator and sampler learning methods take inspiration from these prior works."
424cf0cdf8015640ab36d80cd15f92476aac46b8,8be4f7216f2124603b4477d61f5ddb577a640fe5,True,"methodology,background","Other efforts in this space include heuristic learning [70, 71] (sometimes for TAMP [72, 73, 74]), learning to estimate the feasibility of abstract plans [75, 76], and learning to generate reductions of planning models [77, 78].,To use the components of an abstraction — predicates Ψ, operators Ω, and samplers Σ — for efficient planning, we build on bilevel planning techniques [9, 10].,In the taxonomy of task and motion planners (TAMP), this approach is in the “search-then-sample” category [9, 10, 26].,Recent works have also considered learning abstractions for multi-level planning, like those in the task and motion planning (TAMP) [9, 61] and hierarchical planning [62] literature.,Recent efforts by Loula et al. (2019) and Curtis et al. (2021) consider learning both state and action abstractions for TAMP, like we do [16, 17, 69].,The problems have object-centric continuous states and hybrid discretecontinuous actions, as are common in robotics [9]."
cd14bb267b2badb0d4407b3327d162c2b4346b35,62516303058a1322450b58e4cd778ab873b5e531,True,"methodology,background","Goal-Oriented Semantic Exploration (SemExp) [126] tackles the object goal navigation task in realistic environments.,11: The performances of methods on the Habitat ObjectGoal navigation, including DD-PPO [89], Active Exploration [16], SemExp [17] and 6-Act Tether [267].,Consequently, more methods contribute on target-driven navigation tasks are proposed [13], [14], [15], [16], [17].,Goal-Oriented Semantic Exploration (SemExp) [17] tackles the object goal navigation task in realistic environments."
cd14bb267b2badb0d4407b3327d162c2b4346b35,98789b46f7be983300a0e93e1c53bab56b36efd1,True,"methodology,background","RoboTHOR* [39] 2020 300× 300 3 1200 3 3 7,Recently proposed interactive simulators [38], [39], [41] provide basic functions like open a door or move a chair, which enables researchers to build an interactive navigation agent.,Wait by the moose antlers hanging on
4 Simulator Year Use Dataset(s) Resolution Physics FPS Customizable Interactive Multi-agent MINOS [11] 2017 SUNCG, Matterport3D 84× 84 3 100 7 7 7 AI2-THOR* [40] 2017 - 300× 300 3 120 3 3 3 House3D [10] 2018 SUNCG 120× 90 7 600 3 7 7 CHALET [35] 2018 CHALET 800× 600 7 10 7 7 7 Matterport3D [12] 2018 Matterport3D 512× 512 7 1,000 7 7 7 Gibson [14] 2018 Gibson, Matterport3D, 2D-3D-S 512× 512 3 400 7 7 7 iGibson [41] 2018 Gibson 512× 512 3 400 3 3 3 Habitat [13] 2019 Matterport3D, Gibson, Replica 512× 512 3 10,000 7 7 7 RoboTHOR* [39] 2020 - 300× 300 3 1200 3 3 7
TABLE 2: Comparison of existing embodied simulators (*: the dataset that the simulator uses is not currently released).
the wall”.,Some work such as AI2-THOR [38], RoboTHOR [39] and CHALET [35] rely on the datasets that not currently released.,Some work such as AI2-THOR [40], RoboTHOR [39] and CHALET [35] rely on the datasets that not currently released.,AI2THOR [40], iGibson [41] and RoboTHOR [39] provide interactive environments to train such a skill.,AI2-THOR [38], iGibson [41] and RoboTHOR [39] provide interactive environments to train such a skill."
a2f6930b37febd2b88bbea7778f442ef3621b8a3,5f975172aa9088f24236ccc8fe4bfc01ce6fb9b9,True,"methodology,background","An open-source and highly optimized implementation of sampling-based joint-space MPC, which achieves a control rate of 125Hz on a single GPU, a speedup of 100x compared to existing MPPI based manipulation implementations [17].,A feedback control framework that directly integrates learned perception components into the control loop in the form of a learned self collision cost and a discrete collision checker between the robot links and raw environment pointcloud from [17].,5 Self Collision Avoidance Computing self-collision between the links of the robot for a large number of configurations can be computationally expensive [23, 17].,[17] that operates directly on raw pointcloud data and classifies if an robot link pointcloud pcl is in collision with the environment pointcloud pcenv given the robot link’s pose X l.,[17] learn a collision classifier and do online replanning in joint space leveraging an inverse kinematic function to get a goal joint configuration and find straight line paths in joint space to reach the goal while avoiding collisions.,Hence, similar to previous approaches [23, 17], we train a neural network that predicts the closest distance 1 between the links of the robot given a joint configuration (θ).,However, in the context of manipulation, sampling-based control in joint space has only been explored by optimizing in the joint position space without considering velocity and acceleration limits [17, 43, 44]."
a2f6930b37febd2b88bbea7778f442ef3621b8a3,6e05b33face941e235439ef75cadb56dad5c1746,True,methodology,"We provide results of ablation studies in simulation for different components of our framework such as cost terms, sampling strategy and policy parameterization in Appendix C, and a comparison to MOVEIT! and OSC [32] for the standard pose reaching problem in Appendix B.,Classic approaches to solving these tasks rely on operational-space control (OSC) [4, 5, 6], where the different task costs are formulated in their respective spaces and then projected into the joint space (i.e., the control space of the robot) via a Jacobian map.,Operation Space Controllers (OSC) are some of the fastest algorithms available for feedback control, with methods achieving control latency of 1-2 ms [6, 5, 32, 23].,We provide results of ablation studies in simulation for different components of our framework such as cost terms, sampling strategy and policy parameterization in Appendix C, and a comparison to MOVEIT! and OSC [32] for the standard pose reaching problem in Appendix B.2.,Standard planning and OSC approaches often fall short in optimizing for such behavior.,A key strength of feedback-based MPC over “plan and execute” and OSC approaches is its ability to simultaneously optimize complex cost functions over a long horizon while demonstrating reactive behavior.,However OSC methods rely heavily on a higher level planner for avoiding local minima (e.g.,obstacles).,OSC methods are inherently local as they only optimize for the next time step while ignoring future actions or states."
47049b3a3da29d003f7a44712f070c928e07efaa,b03666f63089671e0237e40d4f68500ec09cbb95,True,background,"In the middle, and most relevant to us, some works approximate specific constraints of the problem while retaining other relevant aspects of contact interaction [27, 17, 36].,Our long term vision is that the execution of these plans will be supported by a low level controller that uses tactile-feedback to enforce contact conditions, as initially explored in [17].,The term β is a lower-bound convex-approximation of the distance between each contact-force and the border of its friction cone, computed as in [2, 17], this term has to be linear in order for the cost-function to be convex.,This is particularly relevant under recent advances on localized tactile sensing [10, 17, 9], which could allow us to generate more dynamic motions under uncertainty."
4e1e3bdf43c1556279e4c043dbdf5571a159fed9,f17adb5a8ee6cf94ea7d08808fa1af3af8e22dbd,True,"methodology,background","Prior work tackles this by learning an embedding function that maps low-dimensional human inputs to high-dimensional robot actions (Losey et al., 2020; Jeon et al., 2020).,We build our approach atop a long-line of work around dimensionality reduction for assistive teleoperation (Ciocarlie and Allen, 2009; Jain and Argall, 2019) – including prior work on visually-guided manipulation and learned latent actions (Losey et al., 2020; Jeon et al., 2020; Li et al., 2020).,Prior work combined latent actions with shared autonomy for increased precision (Jeon et al., 2020), as well as personalized the mapping of the learned latent space to individual users (Li et al.,Prior work combined latent actions with shared autonomy for increased precision (Jeon et al., 2020), as well as personalized the mapping of the learned latent space to individual users (Li et al., 2020)."
7f712d58084e32ddc1b0cd60932f8bc0a0916330,48b1646ae5b5d2ec05f881a2a6f51489c5c0a9f4,True,background,"Besides, Actionable Model (AM) achieves higher final performance than HER and DDPG especially in expert dataset, which indicates the conservative estimation technique is useful for offline goal-conditioned RL.,Note that the most two effective and robust algorithms on both random and expert datasets are WGCSL and Actionable Models (AM), which are specifically designed for offline goal-conditioned RL. Comparing the two algorithms, WGCSL outperforms AM with a probability of 75% on the expert dataset and 62% on the random dataset.,In both expert and random dataset, HER outperforms DDPG, which demonstrates the benefits of goal relabeling for offline goal-conditioned RL.,Theoretically, the discounted weight for relabeling goals contributes to optimizing a tighter lower bound than GCSL. Based on the discounted weight, we show that additionally re-weighting with an exponential function over advantage value guarantees monotonic policy improvement for goal-conditioned RL.,Our work belongs to the scope of offline RL. Learning policies from static offline datasets is challenging for general off-policy RL algorithms due to error accumulation with distributional shift (Fujimoto et al., 2019; Kumar et al., 2019).,In this paper, we have proposed a novel algorithm, Weighted Goal-Conditioned Supervised Learning (WGCSL), to solve offline goal-conditioned RL problems with sparse rewards, by first revisiting GCSL and then generalizing it to offline goal-conditioned RL.,Eysenbach et al. (2020) and Li et al. (2020) further improved the goal sampling strategy from the perspective of inverse RL. Zhao et al. (2021) encouraged agents to control their states to reach goals by optimizing a mutual information objective without external rewards.,In this subsection, we make comparison with Actionable Models (AM) (Chebotar et al., 2021), which applies the conservative estimation idea similar to (Kumar et al., 2020) into offline goalconditioned RL.,WGCSL is strongly efficient in the offline goal-conditioned RL due to the following advantages:
(1) WGCSL learns a universal value function (Schaul et al., 2015) which is more generalizable than the value function learned in a single task, thus alleviating the overfitting problem in offline RL.
(2) WGCSL utilizes goal relabeling which contributes to augment huge amount of data ( ( T 2 ) times) for offline training and meanwhile alleviates the sparse reward issue.,(3) WGCSL leverages weighted supervised learning to reweight the action distribution in the dataset, avoiding the out-of-distribution action problem in offline RL.
(4) In the three weights, GEAW keeps the learned policy close to the offline dataset implicitly and BAW handles the multi-modality problem in offline goal-conditioned RL. WGCSL utilizes the three weights to learn a better policy with theoretical guarantees.,In this section, we will revisit GCSL and introduce the general weighting scheme which generalizes GCSL to offline goal-conditioned RL."
